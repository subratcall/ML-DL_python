{
  "cells": [
    {
      "metadata": {
        "_uuid": "24d6103c8023e95822a5243bc1aa4fd686399a58"
      },
      "cell_type": "markdown",
      "source": "# kaggle 우승작으로 배우는 머신러닝_1 \n\n코드 실습"
    },
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true
      },
      "cell_type": "code",
      "source": "\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nimport gc\nprint(os.listdir(\"../input\"))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "collapsed": true,
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": false
      },
      "cell_type": "markdown",
      "source": "# 간단한 설명\n\n고객의 다양한 재정적 의사결정을 돕기 위해 '맞춤형 추천' 제품을 제공하려는 은행. 고객의 과거 이력과 유사한 고객군의 데이터를 기반으로 '다음달에 해당 고객이 무슨 제품을 사용할지' 예측하는 것.\n\n자사의 금융제품을 사용하는 고객에게, 다른 금융제품을 소개해 만족도를 높이고 매출을 올리기 위한 것."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b49a765d19a396c065fd756b2546677b0b27d399"
      },
      "cell_type": "markdown",
      "source": "# 평가 척도\n\n'신규 구매 제품': 지난달까지는 보유하지 않은 금융제품 중 이번달에 신규로 구매하게 되는 제품을 말한다. 이미 보유한 제품을 지속적으로 사용하는 것은 제외, 지난달 보유 제품을 이번달에 해지하는 것도 제외한다.\n\n\"16-05-28(지난 달) 시점에 보유하지 않은 금융 제품 중 18-06-28(이번 달)에 구매할 것으로 예측되는 제품 상위 7개\"의 제출.\n\n### 평가 척도는 Mean Average Percision @ 7\n\n예컨대 예측 결과가 [1,0,0,1,1,1,0] 이라면, 첫 번째의 예측 정확도는 1/1, 네 번째 예측 정확도는 2/4, 그 이후로 3/5, 4/6이다.\n\n정확도의 합을 정답의 갯수만큼 나눈 숫자 = (1/1+2/4+3/5+4/6) / 4 = .69\n\n모든 예측 결과물의 average precision의 평균값을 말한다.\n\n예측 순서에 상당히 예민한 평가 척도. 정답을 앞쪽에 예측할수록 더 좋은 점수를 받을 수 있다는 뜻이다. 예컨대 7개 중 앞 4개를 제대로 예측했다면 Average precision은 1이 된다.\n\n반대로 4개 정답이 뒤에서부터 4개일 경우 (1/4+2/5+3/6+4/8) / 4 = .43\n"
    },
    {
      "metadata": {
        "_uuid": "688aeb3c9b7216a0d271ab275f0b844f943b43ec"
      },
      "cell_type": "markdown",
      "source": "# 주요 접근\n\nTabular 형태의 시계열 데이터가 제공된다. 'A라는 시간에 B라는 고객이 C라는 제품을 구매했다'는 정보가 담긴 데이터이며, B에 관련된 다양한 정보가 포함된다. 예컨대 나이, 직업유무, 결혼유무, 고객 등급, 거주지, 국적 등등. 또한 C에 관련된 제품 분류, 가격, 인기도, 특징 등등.\n\n모델이 예측해야 하는 target value 식별에 도움 되는 정보가 많을수록 좋은 데이터다.\n\n시계열 데이터를 다루는 경진대회는 '유의미한 파생변수 생성'을 위한 기술이 있다. \n1. 날짜/시간 정보가 포함된 데이터의 경우 '주중/주말 여부', '공휴일 여부', '아침/낮/밤', '계절구분', '학기/시험기간/방학'등 다양한 파생변수를 생성할 수 있다. \n2. 과거 데이터를 활용하는 lag 데이터를 파생 변수로도 사용할 수 있음. 몇 달 전에 고객이 구매한 제품 데이터를 이번 달 예측에 활용하는 식이다.\n\n식별력이 있는 파생변수는 모델마다, 문제마다 다르므로 시도해봐야 한다.\n\n### Tabular 형태의 시계열 데이터를 다루는 경진대회는 딥러닝 모델보다 트리 기반의 앙상블 모델이 더 효과가 좋다.\n\nscikit learn에서도 다양한 트리모델을 지원하지만, 케글 경진대회에서 많이 쓰인 xgboost와 lightgbm을 사용할 예정\n\n특히 xgboost는 가장 좋은 성능과 빠른 속도 덕분에 많이 쓰이는 모델. boosting tree는 간단히 요약하자면 \n* '하나의 트리 모델을 학습시킨 후, 해당 트리모델의 성능이 낮은 부분을 보완하는 다른 트리모델을 학습시키는 방식으로 수많은 트리 모델을 순차적으로 학습시키며 성능을 개선하는 모델'\n\n"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "48f486560782dee55597b90b1ec02fcd430542c0"
      },
      "cell_type": "code",
      "source": "trn = pd.read_csv('../input/train_ver2.csv')\ntrn.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "88262e2634df6081acd7a01ba4830d98e2eeb291"
      },
      "cell_type": "code",
      "source": "trn.shape\n# 데이터 크기는 약 1,300만 개. 고객마다 48개의 변수 존재",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true,
        "_uuid": "b846f4665380d45dd62b1e972cd072239d2919e4"
      },
      "cell_type": "code",
      "source": "for col in trn.columns:\n    print(trn[col].head(),trn[col].dtype)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "cd1780de055b5cded3558c5c5080a40b8e32b7da"
      },
      "cell_type": "markdown",
      "source": "fecha_dato : datetime 변수가 아니라 object 형식\nage : 마찬가지로 정수 변수가 아니라 object. 변형 필요\nrenta: 가구 총수입을 의미하는 변수. 결측치가 존재. 변환 필요\n\n## EDA 첫 단계에서 모든 걸 이해하려고 접근할 필요 없다\n\n단계적으로 데이터에 익숙해지는 과정 필요. 간단한 시각화 정도?\n- 경진대회 데이터가 어떻게 생겼는지\n- 어떤 변수들이 존재하는지\n- data type 확인하고 전처리 수행할 변수 찾기\n\n정도 느낌으로도 충분하다"
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true,
        "_uuid": "4ab6e64fa1a2ed521755ec397e9ceebb7ee171d6"
      },
      "cell_type": "code",
      "source": "trn.info()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "96b2b6712dcd1b54ca1eb96aa951cea8eebbcc31"
      },
      "cell_type": "markdown",
      "source": "1~24까지는 고객 관련 변수, 그 다음 변수들은 '금융 제품 변수'\n\n머신러닝 학습을 위해서는 모든 변수가 int / float로 바뀌어야 한다. 그  외의 타입은 인수로 받지 못함. 따라서 '어떤 변수'를 변환해야 하는지 확인할 수 있다.\n\nfecha_dato : 날짜\n\nncodpers : 고객 고유식별번호\n\nind_empleado : 고용 지표. a = active, b = ex employed, f = filial, n = not employeed, p = passive\n\npais_residencia: 거주지\n\nfecha_alta : 고객이 은행과 첫 계약체결한 날짜\n\nind_nuevo: 신규 고객지표( 6개월 이내 신규고객이면 1)\n\nantiguedad : 거래 누적기간(월)\n\nindrel: 고객 등급( 1: 1등급, 99: 해당 달에 고객 1등급이 해제되는 1등급 고객)\n\nult_fec_cli_1t : 1등급 고객으로의 마지막 날짜\n\nindrel_1mes: 월초 기준 고객등급 (1: 1등급, 2: co-owner, P: potential, 3: former primary, 4: former co-owenr)\n\ntiprel_1mes: 월초 기준 고객관계유형 (a: active, i: inactive, p: former customer, r: potential)\n\nindresi: 거주지표 (고객의 거주국가와 은행 위치국가 동일여부. s: yes, n: no)\n\nindext: 외국인 지표 (태어난 국가와 은행이 위치한 국가 동일 여부)\n\nconyuemp: 배우자 지표. (1:은행 직원을 배우자로 둔 고객)\n\ncanal_entrada: 고객 유입채널\n\nindfall : 고객 사망여부\n\ntipodom : 주소유형(1: primary address)\n\ncod_prov: 지방 코드 (주소 기반)\n\nnomprov : 지방 이름\n\nind_actividad_cliente : 활발성 지표 (1: active, 2: inactive)\n\nrenta : 가구 총수입\n\nsegmento: 분류 (1 : vip, 2: 개인, 3: 대졸)\n"
    },
    {
      "metadata": {
        "_uuid": "2ca1d990cf65800a85c735090c1a230766b4950f"
      },
      "cell_type": "markdown",
      "source": "## 수치형 / 범주형 변수 확인\n\n24개의 고객 관련 변수 확인하기,"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e81734fbe534f8130b971a8a14fb561178f2cd02"
      },
      "cell_type": "code",
      "source": "num_col = [col for col in trn.columns[:24] if trn[col].dtype in ['int64','float64']]\ntrn[num_col].describe()\n# 수치형 변수는 ncodpers, ind_nuevo, indrel, tipodom, cod_prov, ind_actividad_cliente, renta\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "fbfcef39a8ab11c1e82f89bbf30e8e7b7d61895d"
      },
      "cell_type": "code",
      "source": "cat_col = [col for col in trn.columns[:24] if trn[col].dtype in ['O']]\ntrn[cat_col].describe()\n# count : shape에 출력된 값보다 작다면 결측치가 있다는 뜻.\n# unique: 고유값이 몇 개 있는지\n# top : 가장 빈출 빈도 높은 데이터\n# freq : 최빈 데이터의 빈도수. count 대비 최빈값이 얼마인지를 확인하면, 대략적인 분포를 가늠할 수 있다. 예컨대 ind_empleado는 N이 전체의 99% 차지함. (편중 정도)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "67eeb78b6681ce805c2173ec76f08d21ce2662dc",
        "scrolled": true
      },
      "cell_type": "code",
      "source": "# 고유값 직접 출력해보기\nfor col in cat_col:\n    uniq = np.unique(trn[col].astype(str))\n    print('-'*50)\n    print(\"# col {}, n uniq {}, uniq {}\".format(col, len(uniq), uniq))\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "d6958c1d99ecbc0e1fe9ee99fd642ae5ede2db82"
      },
      "cell_type": "markdown",
      "source": "이렇게 출력되는 데이터 타입을 보면서, 눈에 띄는 특징은 노트에 별도로 기록하는 것을 권장한다. '변수 아이디어'로 해당 변수를 유의미하게 활용할 수 있는 feature engineering idea를 기록하는 것도 좋다.\n\n자기만의 아이디어를 코드로 구현하고 실험으로 검증하는 것이 머신러닝 학습의 지름길이다.\n\n## 시각화로 데이터 살펴보기"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2943a060f35a09c42dab612018c3128c11d4cb2c"
      },
      "cell_type": "code",
      "source": "import matplotlib\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true,
        "_uuid": "450e39787995376531b0c3f220b80497dad30e9a"
      },
      "cell_type": "code",
      "source": "# 각 변수의 histogram 그리기. 고객 고유식별번호와 총수입인 ncodpers, renta는 고유값이 너무 많아 시각화에 시간이 오래 걸리므로 제외\nskip_col= ['ncodpers','renta']\n\nfor idx, col in enumerate(trn.columns):\n    if col in skip_col:\n        continue\n    \n    print('-' * 50)\n    print('col : ', col)\n    \n    # 그래프 크기 설정\n    f, ax = plt.subplots(figsize=(20,15))\n    # seaborn 사용한 막대그래프 생성 \n    sns.countplot(x = col, data = trn, alpha = .5)\n    plt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "12da73697f8e42c42890c97b21dcd96942e33f95"
      },
      "cell_type": "markdown",
      "source": "fecha_dato : 첫 6개월은 고객 데이터 수가 같으며, 7월 28일 이후부터 매달 고객이 증가한다.\n\nindrel_1mes : 1, 1.0, 1.0이 분리되어 나타나 있다. 데이터 타입 통일이 필요함\n\nage, antiguedad: 수치형 변수인 나이가 중간에 끊긴 모습. 아마 object 형태로 저장되어 있어서인 것으로 보인다. \n\nfecha_alta : 95년부터 16년까지 폭넓은 값의 데이터. 95년 부근 고객과 16년 부근이 높은 빈도인 것으로 보아 장기고객 & 신규고객 비율이 둘 다 높을 것.\n\n**시각화해서 보이는 정보도 노트에 꼼꼼히 적어두는 것을 추천한다.**\n\n"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "698794981279970f235c8e9d8da5a5e257ac73c3"
      },
      "cell_type": "code",
      "source": "# 시계열성을 포함한 시각화 기법으로 24개의 제품 변수를 자세히 들여다보기.\n# 각 제품은 고객이 보유하면 1, 보유하지 않을 경우 0이다.\n# 각 제품 변수를 다 더한 값은 제품 보유 수준을 의미한다.\n\n# 월별 제품변수의 합을 누적 막대그래프 형태로 시각화하기. 누적 그래프를 쓰는 이유는 '서로 다른 제품의 차이를 함께 시각화하기 위해'\n\n# 날짜 데이터를 기준으로 분석하기 위해 날짜 데이터 추출\nmonths = trn['fecha_dato'].unique().tolist()\nlabel_cols = trn.columns[24:].tolist()\n\nlabel_over_time=[]\n\nfor i in range(len(label_cols)):\n    # 매월, 각 제품의 총합을 groupby.agg(sum) 으로 계산 후 label_sum에 저장한다.\n    label_sum = trn.groupby(['fecha_dato'])[label_cols[i]].agg(sum)\n    label_over_time.append(label_sum.tolist())\n    \nlabel_sum_over_time =[]\nfor i in range(len(label_cols)):\n    # 누적 막대그래프의 시각화를 위해 n번째 제품의 총합을 1~n번째 제품의 총합으로 바꾼다\n    label_sum_over_time.append(np.asarray(label_over_time[i:]).sum(axis=0))\n    \ncolor_list = ['#F5B7B1','#D2B4DE','#AED6F1','#A2D9CE','#ABEBC6','#F9E79F','#F5CBA7','#CCD1D1']\n# 그림 크기를 정의한다\nf, ax = plt.subplots(figsize=(30,15))\nfor i in range(len(label_cols)):\n    # 24개 제품 historgram\n    # x축은 월 데이터, y는 누적 총합.\n    sns.barplot(x = months, y = label_sum_over_time[i],color = color_list[i%8], alpha=.7)\n    \n    \nplt.legend([plt.Rectangle((0,0),1,1,fc=color_list[i%8], edgecolor = 'none') for i in range(len(label_cols))], label_cols, loc=1, ncol = 2, prop={'size':16})",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b6ec4b62c5ab3c6c672eb4a8c2c8dc41cea8beac"
      },
      "cell_type": "markdown",
      "source": "상품의 총합은 증가하고 있다. (고객의 숫자가 꾸준히 늘어나고 있기 때문인 것으로 보인다고 분석)\n\n절대값 대신 상대값으로 시각화를 시도한다면"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "30066ac33228b760a18d1f7e9823a81d2b5ceb48"
      },
      "cell_type": "code",
      "source": "# label_sum_over_time의 값을 퍼센트 단위로 변환하기. 월마다 최댓값으로 나누고 100을 곱한다.\nlabel_sum_percent = (label_sum_over_time / (1.*np.asarray(label_sum_over_time).max(axis=0))) * 100\n\n# 앞선 코드와 동일한, 시각화 실행 코드\nf, ax = plt.subplots(figsize=(30, 15))\nfor i in range(len(label_cols)):\n    sns.barplot(x=months, y=label_sum_percent[i], color = color_list[i%8], alpha=0.7)\n    \nplt.legend([plt.Rectangle((0,0),1,1,fc=color_list[i%8], edgecolor = 'none') for i in range(len(label_cols))], \\\n           label_cols, loc=1, ncol = 2, prop={'size':16})",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b2b849473917472e63d12fd1946c4a3378ee8199"
      },
      "cell_type": "markdown",
      "source": "딱히 눈에 띄는 변화패턴이 있지는 않다.\n\n## 데이터 분석의 목적을 잃지 않기.\n\n컴퓨터 스크린에 뿌려진 데이터를 분석하다 보면 분석의 진짜 목적과 방향성을 잃어버리기 쉽다. 아무 생각 없이, 무의미한 분석방법을 실행하고자 어렵고 복잡한 코딩에 시간낭비하는 경우가 허다함.\n\n아직 머신러닝 알고리즘 학습도 안 한 상태에서 '무엇을 시각화할지, 무엇을 분석할지' 정답을 찾기는 어렵다.\n\n필자의 경험상, 예측을 위해 제공되는 변수(고객 변수)보다는 실제로 예측해야 하는 변수(금융 제품 변수)를 올바르게 분석해 이해도를 높이는 게 머신러닝 모델 구축에 도움이 됐다.\n\nEDA를 통해 얻고자 하는 것\n1. 데이터 기초 통계 & 시각화로 데이터 직접 들여다보기\n2. 변수 아이디어 찾아내기\n3. 예측 변수 24개의 특징 찾아내기\n\n\n### 경진대회에서 예측해야 하는 건 '고객이 신규로 구매할 제품'이다. 보유 여부가 0에서 1로 전환되는 경우가 중요하지, 얼마나 오래 보유하느냐 등등은 중요하지 않다\n\n앞선 누적 막대그래프는 '제품의 총 보유량'일 뿐, '신규 구매'의 월별 추이는 나타나지 않는다.\n\n하지만 데이터에는 '신규 구매'라는 정보가 주어지지 않았으므로, '신규 구매'정보를 별도로 추출하는 과정이 필요하다.\n"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "29e9787cdb8fefddb961dafec4ba7e2f80616c55"
      },
      "cell_type": "code",
      "source": "gc.collect()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b88619d750bd2d21f2a37b1fb613ea61f160d216"
      },
      "cell_type": "code",
      "source": "# 제품 변수를 prods에 list형태로 저장한다\nprods = trn.columns[24:].tolist()\n\n# 날짜를 숫자로 변환하는 함수이다. 2015-01-28은 1, 2016-06-28은 18로 변환된다\ndef date_to_int(str_date):\n    Y, M, D = [int(a) for a in str_date.strip().split(\"-\")]\n    int_date = (int(Y) - 2015) * 12 + int(M)\n    return int_date\n\n\n# 날짜를 숫자로 변환하여 int_date에 저장한다\ntrn['int_date'] = trn['fecha_dato'].map(date_to_int).astype(np.int8)\n#trn['int_date+1'] = trn['int_date']+1\n# 데이터를 복사하고, int_date 날짜에 1을 더하여 lag를 생성한다. 변수명에 _prev를 추가한다.\n# trn_lag = trn.copy()\n# trn_lag['int_date'] += 1\n# trn_lag.columns = [col + '_prev' if col not in ['ncodpers', 'int_date'] else col for col in trn.columns]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "970505970248a0d325d69e9bff0f38e6d22b0e93"
      },
      "cell_type": "code",
      "source": "# # 원본 데이터와 lag 데이터를 ncodper와 int_date 기준으로 합친다. Lag 데이터의 int_date는 1 밀려있기 때문에, 저번달의 제품 정보가 삽입된다.\n# # 메모리 초과로 여기서부터 죽는다\n#df_trn = trn.merge(trn_lag, on=['ncodpers','int_date'], how='left')\n# # 메모리 효율을 위해 불필요한 변수를 메모리에서 제거한다\n# del trn, trn_lag",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "60e9c6d8b9618c9a8ba5c4aa72dbf59abc960475"
      },
      "cell_type": "code",
      "source": "\n\n# # 저번달의 제품 정보가 존재하지 않을 경우를 대비하여 0으로 대체한다.\n# for prod in prods:\n#     prev = prod + '_prev'\n#     df_trn[prev].fillna(0, inplace=True)\n\n# # 원본 데이터에서의 제품 보유 여부 – lag데이터에서의 제품 보유 여부를 비교하여 신규 구매 변수 padd를 구한다\n# for prod in prods:\n#     padd = prod + '_add'\n#     prev = prod + '_prev'\n#     df_trn[padd] = ((df_trn[prod] == 1) & (df_trn[prev] == 0)).astype(np.int8)\n\n# # 신규 구매 변수만을 추출하여 labels에 저장한다.\n# add_cols = [prod + '_add' for prod in prods]\n# labels = df_trn[add_cols].copy()\n# labels.columns = prods\n# #labels.to_csv('../input/labels.csv', index=False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "bef8cc27ed553dcbb4bd38c2258f61062a55d0c7"
      },
      "cell_type": "code",
      "source": "# # 코드 1-12. 신규 구매 누적 막대 그래프를 시각화하기\n\n# #labels = pd.read_csv('../input/labels.csv').astype(int)\n# fecha_dato = trn['fecha_dato']\n\n# labels['date'] = fecha_dato.fecha_dato\n# months = np.unique(fecha_dato.fecha_dato).tolist()\n# label_cols = labels.columns.tolist()[:24]\n\n# label_over_time = []\n# for i in range(len(label_cols)):\n#     label_over_time.append(labels.groupby(['date'])[label_cols[i]].agg('sum').tolist())\n    \n# label_sum_over_time = []\n# for i in range(len(label_cols)):\n#     label_sum_over_time.append(np.asarray(label_over_time[i:]).sum(axis=0))\n    \n# color_list = ['#F5B7B1','#D2B4DE','#AED6F1','#A2D9CE','#ABEBC6','#F9E79F','#F5CBA7','#CCD1D1']\n\n# f, ax = plt.subplots(figsize=(30, 15))\n# for i in range(len(label_cols)):\n#     sns.barplot(x=months, y=label_sum_over_time[i], color = color_list[i%8], alpha=0.7)\n    \n# plt.legend([plt.Rectangle((0,0),1,1,fc=color_list[i%8], edgecolor = 'none') for i in range(len(label_cols))], label_cols, loc=1, ncol = 2, prop={'size':16})",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a5ec935b692e41bf6a11d3c9782d82e8797e2ba4"
      },
      "cell_type": "markdown",
      "source": "첫 달에 신규 구매 숫자가 압도적으로 많은 이유는, 데이터 첫 달에는 모든 보유제품이 신규 구매로 인식되기 때문이다. 다른 달의 신규 구매횟수를 자세히 보려면, 상대값 기준으로 다시 시각화한다."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b83aa69f985d5a54385f4bbe308acd7f8b4db7ef"
      },
      "cell_type": "code",
      "source": "# # 코드 1-13. 신규 구매 누적 막대 그래프를 상대값으로 시각화하기\n\n# label_sum_percent = (label_sum_over_time / (1.*np.asarray(label_sum_over_time).max(axis=0))) * 100\n\n# f, ax = plt.subplots(figsize=(30, 15))\n# for i in range(len(label_cols)):\n#     sns.barplot(x=months, y=label_sum_percent[i], color = color_list[i%8], alpha=0.7)\n    \n# plt.legend([plt.Rectangle((0,0),1,1,fc=color_list[i%8], edgecolor = 'none') for i in range(len(label_cols))], \\\n#            label_cols, loc=1, ncol = 2, prop={'size':16})",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "cd72819b820678a1d67098feb5466f5621a89891"
      },
      "cell_type": "markdown",
      "source": "1. ind_cco_fin_ult1 (당좌 예금)은 8월 여름에 가장 높은 값, 겨울에는 축소되는 계절 추이 (누적그래프 맨 위)\n2. ind_deco_fin_ult1 (단기 예금)은 150628 시기에 특이하게 높은 값, 나머지는 값이 매우 낮다 (위에서 다섯 번째)\n3. 급여, 연금 (ind_nomina_ult1, ind_nom_pens_ult1)은 당좌예금과 반대로 8월 여름에 가장 낮으며 160228 겨울에 가장 높은 값\n4. 신규 구매빈도가 가장 높은 상위 5개 금융제품은 당좌예금, 신용카드, 급여, 연금, 직불카드이다 (ind_cco_fin,ult1, ind_tjcr_fin_ult1, ind_nomina_ult1, ind_nom_pens_ult1, ind_recibo_fin_ult1)\n\n즉 데이터가 seasonality를 보인다. 이 경우 훈련 데이터를 몇 월로 지정하는가에 따라 머신러닝 모델 결과물이 많이 달라질 수 있다는 걸 의미함. 계절 변동성을 모델링하려면 '하나의 일반적인 모델을 구축할 건지, 계절에 따라 다수 모델을 구축해 혼합하여 쓸 건지' 결정해야 한다."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c3e89b6762a42a7a70f8db2d4e76c0f889c0a150"
      },
      "cell_type": "markdown",
      "source": "## train/test dataset 설명\n\n1년 6개월치의 월별 고객데이터. 첫 1년 5개월은 훈련 데이터.\n1. age, antiguedad, indrel_1mes 등의 수치 변수가 object. 정제작업이 필요하다\n2. 대부분의 고객변수에 결측치가 있다. 수치형 / 범주형 변수의 결측치는 기존 변수에 없는 값 (0 or -1)으로 흔히 대체한다. 날짜 변수는 어느 날짜로 대체할지 고민이 필요함\n3. 두 개의 고유값을 갖는 이진변수가 많다. 메모리 효율 극대화를 위해 int64 0, 1로 변환해야.\n4. 고객 등급, 고객 관계유형 등 변수의 각 값이 무엇을 의미하는지 구체적인 설명이 부족하다. \n5. 예측하려는 게 '신규 구매'이므로, 제공된 데이터에서 '신규 구매'여부를 별도로 추출해야 한다. 평가 기준도 '신규 구매' 기준으로 진행되어야 한다.\n6. 신규 구매 데이터는 계절성을 띄고 있다. 단일 모델을 쓸지, 계절별로 다른 모델을 쓸지 결정해야 한다. 다수의 모델을 서로 다른 계절 기반으로 학습하는 것도 방법이다.\n\n"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1c15ba831d5dc383267c0a0e77d476ee1ff5ab82"
      },
      "cell_type": "markdown",
      "source": "# Baseline 모델\n\n## 전처리\n\n1. 제품 변수의 결측값을 0으로 대체한다. 제품 보유 여부 정보가 없으면 보유하고 있지 않다고 가정한다.\n2. 훈련 / 테스트 데이터를 통합한다. 테스트 데이터에 없는 24개 제품변수는 0으로 채운다.\n3. 범주형 / 수치형 데이터 전처리. 범주형은 .factorize()로 라벨 인코딩. object 타입일 경우 unique()로 특이값을 대체하거나 제거하고 정수형 데이터로 변환한다\n4. 모델 학습에 사용할 변수는 features 항목에 미리 저장한다"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "cec856b265a540a79d6114697cb1b1523a94d7d8"
      },
      "cell_type": "code",
      "source": "import pandas as pd\nimport numpy as np\nimport xgboost as xgb\n\nnp.random.seed(2018)\n\n# 데이터를 불러온다.\ntrn = pd.read_csv('../input/train_ver2.csv')\ntst = pd.read_csv('../input/test_ver2.csv')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "182aa414035bceb48022b443dec0c57e20f4ca23"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "66f73d6fde3054f186cee49e22ae05af7c2d0604"
      },
      "cell_type": "code",
      "source": "## 데이터 전처리 ##\n\n# 제품 변수를 별도로 저장해 놓는다.\nprods = trn.columns[24:].tolist()\n\n# 제품 변수 결측값을 미리 0으로 대체한다.\ntrn[prods] = trn[prods].fillna(0.0).astype(np.int8)\n\n# 24개 제품 중 하나도 보유하지 않는 고객 데이터를 제거한다.\nno_product = trn[prods].sum(axis=1) == 0\ntrn = trn[~no_product]\n\n# 훈련 데이터와 테스트 데이터를 통합한다. 테스트 데이터에 없는 제품 변수는 0으로 채운다.\nfor col in trn.columns[24:]:\n    tst[col] = 0\ndf = pd.concat([trn, tst], axis=0)\n\n# 학습에 사용할 변수를 담는 list이다.\nfeatures = []\n\n# 범주형 변수를 .factorize() 함수를 통해 label encoding한다.\ncategorical_cols = ['ind_empleado', 'pais_residencia', 'sexo', 'tiprel_1mes', 'indresi', 'indext', 'conyuemp', 'canal_entrada', 'indfall', 'tipodom', 'nomprov', 'segmento']\nfor col in categorical_cols:\n    df[col], _ = df[col].factorize(na_sentinel=-99)\nfeatures += categorical_cols\n\n# 수치형 변수의 특이값과 결측값을 -99로 대체하고, 정수형으로 변환한다.\ndf['age'].replace(' NA', -99, inplace=True)\ndf['age'] = df['age'].astype(np.int8)\n\ndf['antiguedad'].replace('     NA', -99, inplace=True)\ndf['antiguedad'] = df['antiguedad'].astype(np.int8)\n\ndf['renta'].replace('         NA', -99, inplace=True)\ndf['renta'].fillna(-99, inplace=True)\ndf['renta'] = df['renta'].astype(float).astype(np.int8)\n\ndf['indrel_1mes'].replace('P', 5, inplace=True)\ndf['indrel_1mes'].fillna(-99, inplace=True)\ndf['indrel_1mes'] = df['indrel_1mes'].astype(float).astype(np.int8)\n\n# 학습에 사용할 수치형 변수를 features에 추가한다.\nfeatures += ['age','antiguedad','renta','ind_nuevo','indrel','indrel_1mes','ind_actividad_cliente']\n\n# (피쳐 엔지니어링) 두 날짜 변수에서 연도와 월 정보를 추출한다.\ndf['fecha_alta_month'] = df['fecha_alta'].map(lambda x: 0.0 if x.__class__ is float else float(x.split('-')[1])).astype(np.int8)\ndf['fecha_alta_year'] = df['fecha_alta'].map(lambda x: 0.0 if x.__class__ is float else float(x.split('-')[0])).astype(np.int16)\nfeatures += ['fecha_alta_month', 'fecha_alta_year']\n\ndf['ult_fec_cli_1t_month'] = df['ult_fec_cli_1t'].map(lambda x: 0.0 if x.__class__ is float else float(x.split('-')[1])).astype(np.int8)\ndf['ult_fec_cli_1t_year'] = df['ult_fec_cli_1t'].map(lambda x: 0.0 if x.__class__ is float else float(x.split('-')[0])).astype(np.int16)\nfeatures += ['ult_fec_cli_1t_month', 'ult_fec_cli_1t_year']\n\n# 그 외 변수의 결측값은 모두 -99로 대체한다.\ndf.fillna(-99, inplace=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a7ab6819ec954c837bdc110598d451185de30346"
      },
      "cell_type": "markdown",
      "source": "# Feature Engineering\n\n머신러닝 모델 학습에 사용할 파생변수를 생성한다. baseline 모델에서는 24개 고객 변수, 4개의 날짜 기반 파생변수, 24개의 lag_1 변수를 사용했다.\n\n고객이 첫 계약을 맺은 날짜인 fecha_alta, 고객이 마지막으로 1등급이었던 날짜 ult_fec_cli_1t 변수에서 각각 연도와 월 정보를 추출한다. ex) 두 개의 날짜변수의 차이값을 파생변수로 생성하거나, 졸업식 또는 방학 같은 특별한 날짜까지의 거리를 수치형 변수로 생성할 수도.\n\n결측값은 임시로 -99로 대체한다. sklearn에서는 결측값을 입력값으로 받지 않고 실행에러를 반환하지만, xgboost는 결측값도 정상적인 입력값으로 받는다. 데이터가 결측되었다는 것도 하나의 정보로 인식하고 모델학습에 활용하지만, 이번 장에서는 결측값을 -99로 설정한다.\n\n시계열 데이터는 고객의 과거데이터를 기반으로 다양한 파생변수를 만들 수 있다. 예컨대 고객 나이가 최근 3개월간 변동이 있었는지(3개월 안에 생일을 맞이했는지)를 이진변수로 생성하거나, 한 달 전에 구매한 제품변수를 사용하거나, 최근 6개월 평균월급을 계산할 수 있다.\n\n이 경진대회의 경우 n개월 전에 금융제품을 보유하고 있었는지를 나타내는 lag변수가 좋은 파생변수로 작용했다. 24개의 금융제품 변수를 1개월 전, 2개월 전, 3개월 전 보유여부를 현재 고객 데이터로 활용하는 것.\n\nbaseline 모델에서는 1개월 전 정보만을 사용했다. lag_5까지 직접 구현해서 성능개선을 확인하는 걸 추천한다.\n"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e35c75f9e5e98d95d370dabef81dbc9689dddb89"
      },
      "cell_type": "code",
      "source": "# (피쳐 엔지니어링) lag-1 데이터를 생성한다.\n# 코드 2-12와 유사한 코드 흐름이다.\n\n# 날짜를 숫자로 변환하는 함수이다. 2015-01-28은 1, 2016-06-28은 18로 변환된다\ndef date_to_int(str_date):\n    Y, M, D = [int(a) for a in str_date.strip().split(\"-\")] \n    int_date = (int(Y) - 2015) * 12 + int(M)\n    return int_date\n\n# 날짜를 숫자로 변환하여 int_date에 저장한다\ndf['int_date'] = df['fecha_dato'].map(date_to_int).astype(np.int8)\n\n# 데이터를 복사하고, int_date 날짜에 1을 더하여 lag를 생성한다. 변수명에 _prev를 추가한다.\ndf_lag = df.copy()\ndf_lag.columns = [col + '_prev' if col not in ['ncodpers', 'int_date'] else col for col in df.columns ]\ndf_lag['int_date'] += 1\n\n# 원본 데이터와 lag 데이터를 ncodper와 int_date 기준으로 합친다. Lag 데이터의 int_date는 1 밀려 있기 때문에, 저번 달의 제품 정보가 삽입된다.\ndf_trn = df.merge(df_lag, on=['ncodpers','int_date'], how='left')\n\n# 메모리 효율을 위해 불필요한 변수를 메모리에서 제거한다\ndel df, df_lag\n\n# 저번 달의 제품 정보가 존재하지 않을 경우를 대비하여 0으로 대체한다.\nfor prod in prods:\n    prev = prod + '_prev'\n    df_trn[prev].fillna(0, inplace=True)\ndf_trn.fillna(-99, inplace=True)\n\n# lag-1 변수를 추가한다.\nfeatures += [feature + '_prev' for feature in features]\nfeatures += [prod + '_prev' for prod in prods]\n\n###\n### Baseline 모델 이후, 다양한 피쳐 엔지니어링을 여기에 추가한다.\n###\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "887b60db1a3a169178adf5383222e6c6b6718a5f"
      },
      "cell_type": "markdown",
      "source": "# 모델 학습\n\n[교차 검증]\n\n경진대회에서 좋은 성적을 거두기 위해 가장 중요한 것. 경진대회 진행 중에는 참가자가 하루 최대 5개의 예측 결과물을 제출할 수 있다.\n\n이런 시계열 데이터는 validation용 데이터를 가장 마지막 시간데이터로 두는 게 일반적. 예컨대 150128 ~ 160628 훈련 데이터라면 validation 데이터는 160528 한 달치, 나머지를 train 데이터로 구분한다.\n\n이 baseline 모델에서는 간소화를 위해 160128 ~ 160428 4개월치 데이터를 훈련 데이터로, 160528 데이터를 validation으로 사용한다."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9bbd4b0a6c7c67a2fc7957f2dcf44d5fcdd3fb32"
      },
      "cell_type": "code",
      "source": "\n## 모델 학습\n# 학습을 위하여 데이터를 훈련, 테스트용으로 분리한다.\n# 학습에는 2016-01-28 ~ 2016-04-28 데이터만 사용하고, 검증에는 2016-05-28 데이터를 사용한다.\nuse_dates = ['2016-01-28', '2016-02-28', '2016-03-28', '2016-04-28', '2016-05-28']\ntrn = df_trn[df_trn['fecha_dato'].isin(use_dates)]\ntst = df_trn[df_trn['fecha_dato'] == '2016-06-28']\ndel df_trn\n\n# 훈련 데이터에서 신규 구매 건수만 추출한다.\nX = []\nY = []\nfor i, prod in enumerate(prods):\n    prev = prod + '_prev'\n    prX = trn[(trn[prod] == 1) & (trn[prev] == 0)]\n    prY = np.zeros(prX.shape[0], dtype=np.int8) + i\n    X.append(prX)\n    Y.append(prY)\nXY = pd.concat(X)\nY = np.hstack(Y)\nXY['y'] = Y\n\n# 훈련, 검증 데이터로 분리한다. \nvld_date = '2016-05-28'\nXY_trn = XY[XY['fecha_dato'] != vld_date]\nXY_vld = XY[XY['fecha_dato'] == vld_date]\n\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "8c81148554f0973a0420421283929935b675ff01"
      },
      "cell_type": "markdown",
      "source": "# 모델\n\nxgboost 모델 사용. 대부분의 케글 상위입상자가 사용하는 모델.\n\n파라미터 설명\n* max_depth: 트리모델의 최대 깊이를 말한다. 값이 높을수록 더 복잡한 트리모델을 생성, overfit의 원인이 될 수 있다\n* eta: 딥러닝의 lr과 같은 개념. 값이 너무 높으면 학습이 잘 안 될 수 있으며, 값이 너무 낮으면 학습속도가 느리다.\n* colsample_bytree: 트리 생성시 훈련 데이터에서 변수를 샘플링해 주는 비율. 모든 트리는 전체 변수의 일부만을 학습하여 서로의 약점을 보완해주는 것. 보통 .6 ~ .9 값을 사용한다.\n* colsample_bylevel: 트리의 레벨별로 훈련 데이터의 변수를 샘플링해주는 비율이다. .6 ~ .9 사이값 사용\n\n모델의 최적 파라미터를 찾는 것도 중요하지만, 시간 대비 투자효율을 생각한다면 feature engineering에 더 많은 시간을 할애하는 걸 권장한다."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5c702cfadd32b4c6c1688bcd524c3e04922bc7dc"
      },
      "cell_type": "code",
      "source": "\n# XGBoost 모델 parameter를 설정한다.\nparam = {\n    'booster': 'gbtree',\n    'max_depth': 8,\n    'nthread': 4,\n    'num_class': len(prods),\n    'objective': 'multi:softprob',\n    'silent': 1,\n    'eval_metric': 'mlogloss',\n    'eta': 0.1,\n    'min_child_weight': 10,\n    'colsample_bytree': 0.8,\n    'colsample_bylevel': 0.9,\n    'seed': 2018,\n    }\n\n# 훈련, 검증 데이터를 XGBoost 형태로 변환한다.\nX_trn = XY_trn.as_matrix(columns=features)\nY_trn = XY_trn.as_matrix(columns=['y'])\ndtrn = xgb.DMatrix(X_trn, label=Y_trn, feature_names=features)\n\nX_vld = XY_vld.as_matrix(columns=features)\nY_vld = XY_vld.as_matrix(columns=['y'])\ndvld = xgb.DMatrix(X_vld, label=Y_vld, feature_names=features)\n\n# XGBoost 모델을 훈련 데이터로 학습한다!\nwatch_list = [(dtrn, 'train'), (dvld, 'eval')]\nmodel = xgb.train(param, dtrn, num_boost_round=1000, evals=watch_list, early_stopping_rounds=20)\n\n# 학습한 모델을 저장한다.\nimport pickle\npickle.dump(model, open(\"../model/xgb.baseline.pkl\", \"wb\"))\nbest_ntree_limit = model.best_ntree_limit\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "9c9d849f26fa13826e6a476d45f3923afc0369a5"
      },
      "cell_type": "markdown",
      "source": "# Cross Validation\n\n경진대회의 평가 척도를 사용하는 게 매우 중요하다. 이게 개선되어야 유의미하니까.\n\nmap@7 척도는 최고 점수가 데이터에 따라 변동할 수 있다. baseline 모델의 경우 검증데이터에서 얻을 수 있는 최고점수는 .04점대. (실제 정답값을 넣었을 때의 결과라고 함)\n\n점수가 1보다 낮은 이유는, 검증 데이터의 모든 고객이 신규 구매를 하지 않았기 때문. 예컨대 100명 중 10명이 신규 구매를 했다면, 10명을 정확히 예측해도 10%의 map@7점수를 받게 된다."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9e42fce4a0d1f98d7e0de90eb3b7998136f323ff"
      },
      "cell_type": "code",
      "source": "\n# MAP@7 평가 척도를 위한 준비작업이다.\n# 고객 식별 번호를 추출한다.\nvld = trn[trn['fecha_dato'] == vld_date]\nncodpers_vld = vld.as_matrix(columns=['ncodpers'])\n# 검증 데이터에서 신규 구매를 구한다.\nfor prod in prods:\n    prev = prod + '_prev'\n    padd = prod + '_add'\n    vld[padd] = vld[prod] - vld[prev]    \nadd_vld = vld.as_matrix(columns=[prod + '_add' for prod in prods])\nadd_vld_list = [list() for i in range(len(ncodpers_vld))]\n\n# 고객별 신규 구매 정답 값을 add_vld_list에 저장하고, 총 count를 count_vld에 저장한다.\ncount_vld = 0\nfor ncodper in range(len(ncodpers_vld)):\n    for prod in range(len(prods)):\n        if add_vld[ncodper, prod] > 0:\n            add_vld_list[ncodper].append(prod)\n            count_vld += 1\n\n# 검증 데이터에서 얻을 수 있는 MAP@7 최고점을 미리 구한다. (0.042663)\nprint(mapk(add_vld_list, add_vld_list, 7, 0.0))\n\n# 검증 데이터에 대한 예측 값을 구한다.\nX_vld = vld.as_matrix(columns=features)\nY_vld = vld.as_matrix(columns=['y'])\ndvld = xgb.DMatrix(X_vld, label=Y_vld, feature_names=features)\npreds_vld = model.predict(dvld, ntree_limit=best_ntree_limit)\n\n# 저번 달에 보유한 제품은 신규 구매가 불가하기 때문에, 확률값에서 미리 1을 빼준다\npreds_vld = preds_vld - vld.as_matrix(columns=[prod + '_prev' for prod in prods])\n\n# 검증 데이터 예측 상위 7개를 추출한다.\nresult_vld = []\nfor ncodper, pred in zip(ncodpers_vld, preds_vld):\n    y_prods = [(y,p,ip) for y,p,ip in zip(pred, prods, range(len(prods)))]\n    y_prods = sorted(y_prods, key=lambda a: a[0], reverse=True)[:7]\n    result_vld.append([ip for y,p,ip in y_prods])\n    \n# 검증 데이터에서의 MAP@7 점수를 구한다. (0.036466)\nprint(mapk(add_vld_list, result_vld, 7, 0.0))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "817630a59c13df848e31dcf7ac8e6741429dc8bb"
      },
      "cell_type": "markdown",
      "source": "Baseline 모델은 검증 데이터에서 map@7 .036 정도 나온다.\n\n검증 데이터 최고점수가 .04인 걸 감안하면 정확도는 .036 / .004 = 약 85% 정도.\n\n# 테스트 데이터 예측 및 케글 업로드\n\n테스트 데이터에 더 좋은 성능을 내기 위해서는 훈련+검증 데이터 합친 전체 데이터에 xgboost 모델을 다시 학습한다. 파라미터는 '교차검증'을 통해 찾아낸 최적의 파라미터를 사용하되, xgboost 모델에 사용되는 트리의 개수는 늘어난 검증 데이터만큼 증가한다.\n\n학습한 모델의 변수 중요도는 get_fscore()로 확인할 수 있다.\n\n케글 제출을 위해 제출파일을 생성한다. "
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1c184e826b2df9168cbc328b9042540913c861e4"
      },
      "cell_type": "code",
      "source": "import numpy as np\n\ndef apk(actual, predicted, k=7, default=0.0):\n    # MAP@7 이므로, 최대 7개만 사용한다\n    if len(predicted) > k:\n        predicted = predicted[:k]\n\n    score = 0.0\n    num_hits = 0.0\n\n    for i, p in enumerate(predicted):\n        # 점수를 부여하는 조건은 다음과 같다 :\n        # 예측값이 정답에 있고 (‘p in actual’)\n        # 예측값이 중복이 아니면 (‘p not in predicted[:i]’) \n        if p in actual and p not in predicted[:i]:\n            num_hits += 1.0\n            score += num_hits / (i+1.0)\n\n    # 정답값이 공백일 경우, 무조건 0.0점을 반환한다\n    if not actual:\n        return default\n\n    # 정답의 개수(len(actual))로 average precision을 구한다\n    return score / min(len(actual), k)\n\ndef mapk(actual, predicted, k=7, default=0.0):\n    # list of list인 정답값(actual)과 예측값(predicted)에서 고객별 Average Precision을 구하고, np.mean()을 통해 평균을 계산한다\n    return np.mean([apk(a, p, k, default) for a, p in zip(actual, predicted)]) \n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "92c8cb4c7ad70e9a6678f9cba223c488acabd73a"
      },
      "cell_type": "code",
      "source": "# XGBoost 모델을 전체 훈련 데이터로 재학습한다!\nX_all = XY.as_matrix(columns=features)\nY_all = XY.as_matrix(columns=['y'])\ndall = xgb.DMatrix(X_all, label=Y_all, feature_names=features)\nwatch_list = [(dall, 'train')]\n# 트리 개수를 늘어난 데이터 양만큼 비례해서 증가한다.\nbest_ntree_limit = int(best_ntree_limit * (len(XY_trn) + len(XY_vld)) / len(XY_trn))\n# XGBoost 모델 재학습!\nmodel = xgb.train(param, dall, num_boost_round=best_ntree_limit, evals=watch_list)\n\n# 변수 중요도를 출력해본다. 예상하던 변수가 상위로 올라와 있는가?\nprint(\"Feature importance:\")\nfor kv in sorted([(k,v) for k,v in model.get_fscore().items()], key=lambda kv: kv[1], reverse=True):\n    print(kv)\n\n# 캐글 제출을 위하여 테스트 데이터에 대한 예측 값을 구한다.\nX_tst = tst.as_matrix(columns=features)\ndtst = xgb.DMatrix(X_tst, feature_names=features)\npreds_tst = model.predict(dtst, ntree_limit=best_ntree_limit)\nncodpers_tst = tst.as_matrix(columns=['ncodpers'])\npreds_tst = preds_tst - tst.as_matrix(columns=[prod + '_prev' for prod in prods])\n\n# 제출 파일을 생성한다.\nsubmit_file = open('../model/xgb.baseline.2015-06-28', 'w')\nsubmit_file.write('ncodpers,added_products\\n')\nfor ncodper, pred in zip(ncodpers_tst, preds_tst):\n    y_prods = [(y,p,ip) for y,p,ip in zip(pred, prods, range(len(prods)))]\n    y_prods = sorted(y_prods, key=lambda a: a[0], reverse=True)[:7]\n    y_prods = [p for y,p,ip in y_prods]\n    submit_file.write('{},{}\\n'.format(int(ncodper), ' '.join(y_prods)))\n\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "5ff30aa688d3d1fd654522cab320dcdd83b7cbca"
      },
      "cell_type": "markdown",
      "source": "케글에서는 public score = 테스트 데이터의 70% 에 대한 평가점수를, private score는 나머지 30%에 대한 평가점수를 의미한다. 비율은 경진대회마다 다름\n\n경진대회가 현재 진행형인 경우는 참가자에게 public score만 공개한다. 이 값을 기준으로 자신의 머신러닝 파이프라인 성능을 확인할 수 있다. 끝나는 날에는 private score공개, 최종순위는 private score 기준으로 정해진다.\n\n참고로 이 baseline 모델은 전체 참가팀 기준으로 1787팀 중 1023등 정도라고.\n\n이제 이 모델의 성능을 개선하는 일만 남은 셈이다. 모델 튜닝보다는 feature engineering에 더 많은 시간을 투자하는 것을 권하며, 데이터를 심도있게 고민하고 다양한 아이디어를 구현, 실험하는 것이 중요하다."
    },
    {
      "metadata": {
        "_uuid": "3bbb8ae4f215c452729e93f9f18f73a3ecace2d9"
      },
      "cell_type": "markdown",
      "source": "# 8등을 기록한 팀의 코드 분석\n저작권: yaxinus\n\n[참고사항]\n\n- 데이터 전처리: 큰 효과를 본 건 없지만, 고객 데이터에서 날짜 관련 결측값을 채우려 시도했으며, age / antiguedad 변수를 수정했다. lag_5 이상의 데이터를 만들기 위해 14년 데이터를 생성하려 했지만 의미 없었다\n- feature Engineering: 평범한 엔지니어링. lag_5, lag데이터에 대한 기초 통계값(최소, 최대, 표준편차)이 주된 엔지니어링이었다. 5 이상의 lag데이터로 성능 개선을 보지 못했다.\n- 기본 모델 및 모델 통합: 다양한 변수, 가중치와 모델 설정값을 기준으로 mlogloss 기준으로 학습된 lightgbm, xgboost 기본 모델을 다수 만들었다. 초기 예측값을 기준으로 확률값을 계산하는 알고리즘을 사용해, 초기 예측값 대비 피어슨 상관관계가 가장 낮은 모델 결과물을 통합했다. 단순히 다수의 모델 결과물을 통합하는 것보다 이 편이 조금 더 좋은 성능개선을 보였다.\n\n"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1728a5be2a3f82c5732bcd4aa78c4fef1e7c7040"
      },
      "cell_type": "code",
      "source": "# 훈련 데이터와 테스트 데이터를 하나의 데이터로 통합하는 코드이다.\ndef clean_data(fi, fo, header, suffix):\n    \n    # fi : 훈련/테스트 데이터를 읽어오는 file iterator\n    # fo : 통합되는 데이터가 write되는 경로\n    # header : 데이터에 header 줄을 추가할 것인지를 결정하는 boolean\n    # suffix : 훈련 데이터에는 48개의 변수가 있고, 테스트 데이터에는 24개의 변수만 있다. suffix로 부족한 테스트 데이터 24개분을 공백으로 채운다.\n\n    # csv의 첫줄, 즉 header를 읽어온다\n    head = fi.readline().strip(\"\\n\").split(\",\")\n    head = [h.strip('\"') for h in head]\n\n    # ‘nomprov’ 변수의 위치를 ip에 저장한다\n    for i, h in enumerate(head):\n        if h == \"nomprov\":\n            ip = i\n\n    # header가 True 일 경우에는, 저장할 파일의 header를 write한다\n    if header:\n        fo.write(\"%s\\n\" % \",\".join(head))\n\n    # n은 읽어온 변수의 개수를 의미한다 (훈련 데이터 : 48, 테스트 데이터 : 24)\n    n = len(head)\n    for line in fi:\n        # 파일의 내용을 한줄 씩 읽어와서, 줄바꿈(\\n)과 ‘,’으로 분리한다\n        fields = line.strip(\"\\n\").split(\",\")\n\n        # ‘nomprov’변수에 ‘,’을 포함하는 데이터가 존재한다. ‘,’으로 분리된 데이터를 다시 조합한다\n        if len(fields) > n:\n            prov = fields[ip] + fields[ip+1]\n            del fields[ip]\n            fields[ip] = prov\n\n        # 데이터 개수가 n개와 동일한지 확인하고, 파일에 write한다. 테스트 데이터의 경우, suffix는24개의 공백이다\n        assert len(fields) == n\n        fields = [field.strip() for field in fields]\n        fo.write(\"%s%s\\n\" % (\",\".join(fields), suffix))\n\n# 하나의 데이터로 통합하는 코드를 실행한다. 먼저 훈련 데이터를 write하고, 그 다음으로 테스트 데이터를 write한다. 이제부터 하나의 dataframe만을 다루며 데이터 전처리를 진행한다.\nwith open(\"../input/8th.clean.all.csv\", \"w\") as f:\n    clean_data(open(\"../input/train_ver2.csv\"), f, True, \"\")\n    comma24 = \"\".join([\",\" for i in range(24)])\n    clean_data(open(\"../input/test_ver2.csv\"), f, False, comma24)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "ef0d6b51eb9a13de5ea21dd532bc9dbc4fc54244"
      },
      "cell_type": "markdown",
      "source": "8등팀의 main.py 파일에서 \n1. 데이터 전처리\n2. feature engineering\n3. 모델 학습\n4. test data예측 / 케글 업로드\n\n일련의 머신러닝 파이프라인 과정을 모두 수행했다.\n\nmain.py가 메인이며, 관련 라이브러리와 머신러닝 함수가 포함된 게 engine.py, 자주 사용되는 함수가 포함된 utils.py로 함께 import한다.\n\nmain.py에서 데이터 전처리와 feature engineering은 make_data() 함수가, 모델 학습 및 케글 제출용 파일은 train_predict()에서 담당한다."
    },
    {
      "metadata": {
        "_uuid": "357901b2c52115dff34c3d94c5e63eaf336025b2"
      },
      "cell_type": "markdown",
      "source": "# 데이터 전처리 + feature engineering\n\nmake_data() 함수에서 동시에 처리한다,. 48개 변수 각각에 맞춤화된 전처리 / 엔지니어링 제공.\n"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "488bc30c3fbe9a9f285ae2421312319d627525c5"
      },
      "cell_type": "code",
      "source": "import math    \nimport io    \n\n# 파일 압축 용도\nimport gzip    \nimport pickle    \nimport zlib    \n\n# 데이터, 배열을 다루기 위한 기본 라이브러리\nimport pandas as pd \nimport numpy as np\n\n# 범주형 데이터를 수치형으로 변환하기 위한 전처리 도구\nfrom sklearn.preprocessing import LabelEncoder\n\nimport engines\nfrom utils import *\nnp.random.seed(2016)\ntransformers = {}",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "50a821f3e738cfea612eb7c60d092526bca6282f"
      },
      "cell_type": "code",
      "source": "def load_data():\n    # “데이터 준비”에서 통합한 데이터를 읽어온다\n    fname = \"../input/8th.clean.all.csv\"\n    train_df = pd.read_csv(fname, dtype=dtypes)\n\n    # products는 util.py에서 정의한 24개의 금융 제품이름이다\n    # 결측값을 0.0으로 대체하고, 정수형으로 변환한다\n    for prod in products:\n        train_df[prod] = train_df[prod].fillna(0.0).astype(np.int8)\n\n    # 48개의 변수마다 전처리/피처 엔지니어링을 적용한다\n    train_df, features = apply_transforms(train_df)\n\n    prev_dfs = []\n    prod_features = None\n\n    use_features = frozenset([1,2])\n    # 1 ~ 5까지의 step에 대하여 make_prev_df()를 통해 lag-n 데이터를 생성한다\n    for step in range(1,6):\n        prev1_train_df, prod1_features = make_prev_df(train_df, step)\n        # 생성한 lag 데이터는 prev_dfs 리스트에 저장한다\n        prev_dfs.append(prev1_train_df)\n        # features에는 lag-1,2만 추가한다\n        if step in use_features:\n            features += prod1_features\n        # prod_features에는 lag-1의 변수명만 저장한다\n        if step == 1:\n            prod_features = prod1_features\n\n    return train_df, prev_dfs, features, prod_features\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "a33273b5fc098303f92130982c64a15d2a727ac7"
      },
      "cell_type": "markdown",
      "source": "24개 금융변수 결측치 대체, 정수형으로 변환한 후 apply_function() 사용해 데이터 전처리 / feature engineering 수행.\n\napply_funcion() 설명하기 전, 이 함수에 포함된 4개 도구를 먼저 요약하면\n\n1. label_encoder(df, features, name) = df에서 범주형 변수 name을 LabelEncoder()를 써서 수치형으로 변환한다. 사전에 정의한 dict()인 transformers에 label encoding 수행한 변수명을 기록함\n\n\n== df에 동일한 변수를 label encoding할 때는 기존의 LabelEncoder()를 재활용한다. 단 실제 코드에서는 재활용된 적이 한 번도 없다.\n"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1f7a3177a8ba6812c166980b29af6b09452a9da8"
      },
      "cell_type": "code",
      "source": "def label_encode(df, features, name):\n    # 데이터 프레임 df의 변수 name의 값을 모두 string으로 변환한다\n    df[name] = df[name].astype('str')\n    # 이미, label_encode 했던 변수일 경우, transformer[name]에 있는 LabelEncoder()를 재활용한다\n    if name in transformers:\n        df[name] = transformers[name].transform(df[name])\n    # 처음 보는 변수일 경우, transformer에 LabelEncoder()를 저장하고, .fit_transform() 함수로 label encoding을 수행한다\n    else: # train\n        transformers[name] = LabelEncoder()\n        df[name] = transformers[name].fit_transform(df[name])\n    # label encoding한 변수는 features 리스트에 추가한다\n    features.append(name)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "1335891e396b18c0294a0ee69b63be0e20214934"
      },
      "cell_type": "markdown",
      "source": "2. encode_top(s, count=100, dtype=np.int8) 함수는 Series에서 빈도가 높은 100개 고유값을 순위로 대체하고, 그 외 빈도가 낮은 값을 0으로 변환한 새로운 Series를 반환한다. 데이터 전체가 아닌, 고빈도 데이터 정보를 추출하기 위해 사용함."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "69a437c2b7142d07b66cea592a87a772422e5fc9"
      },
      "cell_type": "code",
      "source": "def encode_top(s, count=100, dtype=np.int8):\n    # 모든 고유값에 대한 빈도를 계산한다\n    uniqs, freqs = np.unique(s, return_counts=True)\n    # 빈도 Top 100을 추출한다\n    top = sorted(zip(uniqs,freqs), key=lambda vk: vk[1], reverse = True)[:count]\n    # { 기존 데이터 : 순위 } 를 나타내는 dict()를 생성한다\n    top_map = {uf[0]: l+1 for uf, l in zip(top, range(len(top)))}\n    # 고빈도 100개의 데이터는 순위로 대체하고, 그 외는 0으로 대체한다\n    return s.map(lambda x: top_map.get(x, 0)).astype(dtype)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "69df30733add0cc7ea7e382aa6e57a2b4f957bbf"
      },
      "cell_type": "markdown",
      "source": "3. data_to_float(str_date) 함수는 입력으로 들어오는 날짜 데이터를 숫자로 변환하는 함수. 입력값이 결측치인 경우 np.nan을 반환하지만, 입력값이 문자열인 형태의 날짜 데이터일 경우 (연도 * 12 + 월) 계산으로 날짜 데이터를 소수로 환산해 반환한다. (날짜 데이터를 단순 월 단위 수치형 데이터로 변경한 것). date_to_int(str_date)는 월 단위 수치형으로 변환된 데이터를 1~18 사이의 값으로 변환한다.\n\nutils.py에 있다."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "624b6d405334cb9462346e5d2985e65a9c4f215a"
      },
      "cell_type": "code",
      "source": "def date_to_float(str_date):\n    if str_date.__class__ is float and math.isnan(str_date) or str_date == \"\":\n        return np.nan\n    Y, M, D = [int(a) for a in str_date.strip().split(\"-\")]\n    float_date = float(Y) * 12 + float(M)\n    return float_date\n\n# 날짜 데이터를 월 단위 숫자로 변환하되 1~18 사이로 제한하는 함수\ndef date_to_int(str_date):\n    Y, M, D = [int(a) for a in str_date.strip().split(\"-\")] # \"2016-05-28\"\n    int_date = (int(Y) - 2015) * 12 + int(M)\n    assert 1 <= int_date <= 12 + 6\n    return int_date",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "acf91281b8b6e4aeacdeb756a04ed1ba5c22898f"
      },
      "cell_type": "markdown",
      "source": "4. custom_one_hot(df, features, name, names) 함수는 범주형 변수를 입력받아, 변수 안에 존재하는 고유값을 새로운 이진 변수로 생성하는 one hot encoding을 수행한다.\n\n**범주형 데이터를 하나의 열에서 label encoding하는 것보다 표현력이 높아지지만 고유값의 숫자만큼 데이터의 열이 늘어나기 때문에, 고유값이 적은 데이터에서 선호되는 엔지니어링 기법**\n\nsklearn의 onehotencoding이나 pd.get_dummies 로도 만들 수 있으나 여기서는 직접 구현했다고 함\n"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "84095a89fe5636ee3cd594377a7f4182af2d2c33"
      },
      "cell_type": "code",
      "source": "def custom_one_hot(df, features, name, names, dtype=np.int8, check=False):\n    for n, val in names.items():\n        # 신규 변수명을 “변수명_숫자”로 지정한다\n        new_name = \"%s_%s\" % (name, n)\n        # 기존 변수에서 해당 고유값을 가지면 1, 그 외는 0인 이진 변수를 생성한다\n        df[new_name] = df[name].map(lambda x: 1 if x == val else 0).astype(dtype)\n        features.append(new_name)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "f149efd0fd25cf8f810bc2444c88915652b023ee"
      },
      "cell_type": "markdown",
      "source": "### apply_transform\n\n4개의 도구함수를 이용해 총 48개의 변수에 대한 데이터 전처리와 엔지니어링 수행. 주된 전처리 내용과 엔지니어링 내용은\n\n- 결측값 대체: 데이터 내에 존재하는 결측치를 0 똔느 1로 대체한다\n- 범주형 데이터 label encoding: 범주형으로 표현되는 데이터를 sklearn.preprocessing의 LabelEncoder 도구를 써서 수치형으로 변환했다\n- 고빈도 top 100를 빈도 순위로 변환: 특정 변수에서 빈도가 높은 값을 순위로 변환하여 고빈도 데이터에 대한 선형관계를 추출했다\n- 수치형 변수 Log transformation: log transformationdms 데이터 내 대소관계를 유지하면서 포함된 값들의 차이를 줄여주는 역할\n- 날짜 데이터에서 '연,월' 추출: 문자열 데이터에서 연도와 월 추출\n- 날짜 데이터 간의 차이값으로 파생변수 생성: 2개의 날짜 데이터 차이값을 통해 상대적인 거리 변수를 생성한다.\n- one hot encoding 변수 생성: 범주형 데이터의 표현력을 높이기 위해, 모든 고유값을 새로운 이진 변수로 생성한다.\n"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "436183e8536e6c0bdce0e68d46b40eec3fa04356"
      },
      "cell_type": "code",
      "source": "def apply_transforms(train_df):\n    # 학습에 사용할 변수를 저장할 features 리스트를 생성한다\n    features = []\n\n    # 두 변수를 label_encode() 한다\n    label_encode(train_df, features, \"canal_entrada\")\n    label_encode(train_df, features, \"pais_residencia\")\n\n    # age의 결측값을 0.0으로 대체하고, 모든 값을 정수로 변환한다.\n    train_df[\"age\"] = train_df[\"age\"].fillna(0.0).astype(np.int16)\n    features.append(\"age\")\n\n    # renta의 결측값을 1.0으로 대체하고, log를 씌워 분포를 변형한다\n    train_df[\"renta\"].fillna(1.0, inplace=True)\n    train_df[\"renta\"] = train_df[\"renta\"].map(math.log)\n    features.append(\"renta\")\n\n    # 고빈도 100개의 순위를 추출한다\n    train_df[\"renta_top\"] = encode_top(train_df[\"renta\"])\n    features.append(\"renta_top\")\n\n    # 결측값 혹은 음수를 0으로 대체하고, 나머지 값은 +1.0 은 한 후에, 정수로 변환한다\n    train_df[\"antiguedad\"] = train_df[\"antiguedad\"].map(lambda x: 0.0 if x < 0 or math.isnan(x) else x+1.0).astype(np.int16)\n    features.append(\"antiguedad\")\n\n    # 결측값을 0.0으로 대체하고, 정수로 변환한다\n    train_df[\"tipodom\"] = train_df[\"tipodom\"].fillna(0.0).astype(np.int8)\n    features.append(\"tipodom\")\n    train_df[\"cod_prov\"] = train_df[\"cod_prov\"].fillna(0.0).astype(np.int8)\n    features.append(\"cod_prov\")\n\n    # fecha_dato에서 월/년도를 추출하여 정수값으로 변환한다\n    train_df[\"fecha_dato_month\"] = train_df[\"fecha_dato\"].map(lambda x: int(x.split(\"-\")[1])).astype(np.int8)\n    features.append(\"fecha_dato_month\")\n    train_df[\"fecha_dato_year\"] = train_df[\"fecha_dato\"].map(lambda x: float(x.split(\"-\")[0])).astype(np.int16)\n    features.append(\"fecha_dato_year\")\n\n    # 결측값을 0.0으로 대체하고, fecha_alta에서 월/년도를 추출하여 정수값으로 변환한다\n    # x.__class__는 결측값일 경우 float를 반환하기 때문에, 결측값 탐지용으로 사용하고 있다\n    train_df[\"fecha_alta_month\"] = train_df[\"fecha_alta\"].map(lambda x: 0.0 if x.__class__ is float else float(x.split(\"-\")[1])).astype(np.int8)\n    features.append(\"fecha_alta_month\")\n    train_df[\"fecha_alta_year\"] = train_df[\"fecha_alta\"].map(lambda x: 0.0 if x.__class__ is float else float(x.split(\"-\")[0])).astype(np.int16)\n    features.append(\"fecha_alta_year\")\n\n    # 날짜 데이터를 월 기준 수치형 변수로 변환한다\n    train_df[\"fecha_dato_float\"] = train_df[\"fecha_dato\"].map(date_to_float)\n    train_df[\"fecha_alta_float\"] = train_df[\"fecha_alta\"].map(date_to_float)\n\n    # fecha_dato 와 fecha_alto의 월 기준 수치형 변수의 차이값을 파생 변수로 생성한다\n    train_df[\"dato_minus_alta\"] = train_df[\"fecha_dato_float\"] - train_df[\"fecha_alta_float\"]\n    features.append(\"dato_minus_alta\")\n\n    # 날짜 데이터를 월 기준 수치형 변수로 변환한다 (1 ~ 18 사이 값으로 제한)\n    train_df[\"int_date\"] = train_df[\"fecha_dato\"].map(date_to_int).astype(np.int8)\n\n    # 자체 개발한 one-hot-encoding을 수행한다\n    custom_one_hot(train_df, features, \"indresi\", {\"n\":\"N\"})\n    custom_one_hot(train_df, features, \"indext\", {\"s\":\"S\"})\n    custom_one_hot(train_df, features, \"conyuemp\", {\"n\":\"N\"})\n    custom_one_hot(train_df, features, \"sexo\", {\"h\":\"H\", \"v\":\"V\"})\n    custom_one_hot(train_df, features, \"ind_empleado\", {\"a\":\"A\", \"b\":\"B\", \"f\":\"F\", \"n\":\"N\"})\n    custom_one_hot(train_df, features, \"ind_nuevo\", {\"new\":1})\n    custom_one_hot(train_df, features, \"segmento\", {\"top\":\"01 - TOP\", \"particulares\":\"02 - PARTICULARES\", \"universitario\":\"03 - UNIVERSITARIO\"})\n    custom_one_hot(train_df, features, \"indfall\", {\"s\":\"S\"})\n    custom_one_hot(train_df, features, \"tiprel_1mes\", {\"a\":\"A\", \"i\":\"I\", \"p\":\"P\", \"r\":\"R\"}, check=True)\n    custom_one_hot(train_df, features, \"indrel\", {\"1\":1, \"99\":99})\n\n    # 결측값을 0.0으로 대체하고, 그 외는 +1.0을 더하고, 정수로 변환한다\n    train_df[\"ind_actividad_cliente\"] = train_df[\"ind_actividad_cliente\"].map(lambda x: 0.0 if math.isnan(x) else x+1.0).astype(np.int8)\n    features.append(\"ind_actividad_cliente\")\n\n    # 결측값을 0.0으로 대체하고, “P”를 5로 대체하고, 정수로 변환한다\n    train_df[\"indrel_1mes\"] = train_df[\"indrel_1mes\"].map(lambda x: 5.0 if x == \"P\" else x).astype(float).fillna(0.0).astype(np.int8)\n    features.append(\"indrel_1mes\")\n    \n    # 데이터 전처리/피쳐 엔지니어링이 1차적으로 완료된 데이터 프레임 train_df와 학습에 사용할 변수 리스트 features를 tuple 형태로 반환한다\n    return train_df, tuple(features)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "509d28f2dd7d44a82fc26d3d3908d5ecc3587109"
      },
      "cell_type": "markdown",
      "source": "# 금융 변수의 lag 데이터 생성\n\n시계열 문제에서 많이 사용되는 파생변수의 하나, 해당 변수의 n단위 이전 값을 lag_n 데이터라고 칭한다. 시계열 경진대회에서는 lag_n 데이터가 유의미한 성능개선을 보이는 경우가 종종 있다.\n\n2개의 도구함수\n1. make_prev_df(train_df, step) = 24개 금융변수의 Lag 데이터를 직접 생성. 날짜 데이터를 1~18 사이의 정수로 반환한 int_date 변수를 써서 24개 금융변수 값을 step 개월 이동시켜 lag 변수를 만든다"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "205e88ea0bb46f9fd4cf7701feeeaef20f5c4cb0"
      },
      "cell_type": "code",
      "source": "def make_prev_df(train_df, step):\n    # 새로운 데이터 프레임에 ncodpers를 추가하고, int_date를 step만큼 이동시킨 값을 넣는다\n    prev_df = pd.DataFrame()\n    prev_df[\"ncodpers\"] = train_df[\"ncodpers\"]\n    prev_df[\"int_date\"] = train_df[\"int_date\"].map(lambda x: x+step).astype(np.int8)\n\n    # “변수명_prev1” 형태의 lag 변수를 생성한다\n    prod_features = [\"%s_prev%s\" % (prod, step) for prod in products]\n    for prod, prev in zip(products, prod_features):\n        prev_df[prev] = train_df[prod]\n\n    return prev_df, tuple(prod_features)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "d1f760b5181bdc1e296afcfd932914f5e6f4f527"
      },
      "cell_type": "markdown",
      "source": "2. join_with_prev(df, prev_df, how) 함수는 기존 train_df에 lag 데이터 조인하는 함수"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d6f5a562e6c548c938a197566440d9bbd416ad72"
      },
      "cell_type": "code",
      "source": "def join_with_prev(df, prev_df, how):\n    # pandas merge 함수를 통해 join\n    df = df.merge(prev_df, on=[\"ncodpers\", \"int_date\"], how=how)\n    # 24개 금융 변수를 소수형으로 변환한다\n    for f in set(prev_df.columns.values.tolist()) - set([\"ncodpers\", \"int_date\"]):\n        df[f] = df[f].astype(np.float16)\n    return df",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "802739a31bc0c36fbe43ef2c659fc6a29513dd81"
      },
      "cell_type": "markdown",
      "source": "최대 lag_5까지의 변수를 생성하고 train_df에 조인한다. 이 경진대회에서 가장 중요한 변수가 Lag_5였던 만큼, 이 과정이 있고 없고가 성능에 큰 영향을 준다"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6c5424b59707041bd605387e0c2d2b4c9f554250"
      },
      "cell_type": "code",
      "source": "def load_data():\n    # “데이터 준비”에서 통합한 데이터를 읽어온다\n    fname = \"../input/8th.clean.all.csv\"\n    train_df = pd.read_csv(fname, dtype=dtypes)\n\n    # products는 util.py에서 정의한 24개의 금융 제품이름이다\n    # 결측값을 0.0으로 대체하고, 정수형으로 변환한다\n    for prod in products:\n        train_df[prod] = train_df[prod].fillna(0.0).astype(np.int8)\n\n    # 48개의 변수마다 전처리/피처 엔지니어링을 적용한다\n    train_df, features = apply_transforms(train_df)\n\n    prev_dfs = []\n    prod_features = None\n\n    use_features = frozenset([1,2])\n    # 1 ~ 5까지의 step에 대하여 make_prev_df()를 통해 lag-n 데이터를 생성한다\n    for step in range(1,6):\n        prev1_train_df, prod1_features = make_prev_df(train_df, step)\n        # 생성한 lag 데이터는 prev_dfs 리스트에 저장한다\n        prev_dfs.append(prev1_train_df)\n        # features에는 lag-1,2만 추가한다\n        if step in use_features:\n            features += prod1_features\n        # prod_features에는 lag-1의 변수명만 저장한다\n        if step == 1:\n            prod_features = prod1_features\n\n    return train_df, prev_dfs, features, prod_features\n\n# 특이점은, lag_5 변수를 생성하면서도 features에는 lag 1,2내용만 추가하고, prod_features에는 lag_1 변수명만 저장했다",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "565b1775aee76d54659837b9a96962d2d4f3db84"
      },
      "cell_type": "markdown",
      "source": "make_data()의\n\n``    for i, prev_df in enumerate(prev_dfs):\n        with Timer(\"join train with prev%s\" % (i+1)):\n            how = \"inner\" if i == 0 else \"left\"\n            train_df = join_with_prev(train_df, prev_df, how=how)``\n            \n   여기가 lag데이터 통합하는 부분. lag 1만 inner join, 나머지는 left join\n  "
    },
    {
      "metadata": {
        "_uuid": "62c724abf196c97f124e43f2f167fbd47305c50f"
      },
      "cell_type": "markdown",
      "source": "또한 lag 구간별로 평균 / 표준편차 / 최댓값 / 최솟값을 구해 데이터에 추가한다.\n\n```\n# 24개의 금융 변수에 대해서 for loop을 돈다\n    for prod in products:\n        # [1~3], [1~5], [2~5] 의 3개 구간에 대해서 표준편차를 구한다\n        for begin, end in [(1,3),(1,5),(2,5)]:\n            prods = [\"%s_prev%s\" % (prod, i) for i in range(begin,end+1)]\n            mp_df = train_df.as_matrix(columns=prods)\n            stdf = \"%s_std_%s_%s\" % (prod,begin,end)\n\n            # np.nanstd로 표준편차를 구하고, features에 신규 파생 변수 이름을 추가한다\n            train_df[stdf] = np.nanstd(mp_df, axis=1)\n            features += (stdf,)\n\n        # [2~3], [2~5] 의 2개 구간에 대해서 최소값/최대값을 구한다\n        for begin, end in [(2,3),(2,5)]:\n            prods = [\"%s_prev%s\" % (prod, i) for i in range(begin,end+1)]\n            mp_df = train_df.as_matrix(columns=prods)\n\n            minf = \"%s_min_%s_%s\"%(prod,begin,end)\n            train_df[minf] = np.nanmin(mp_df, axis=1).astype(np.int8)\n\n            maxf = \"%s_max_%s_%s\"%(prod,begin,end)\n            train_df[maxf] = np.nanmax(mp_df, axis=1).astype(np.int8)\n\n            features += (minf,maxf,)\n```\n\nlag의 기초통계 변수를 명시적으로 변수화해서, 학습 모델이 lag 변수에 숨겨진 패턴을 더 찾기 쉽게 하도록 돕는 셈.\n\n그리고 마지막으로 train_df에서 학습에 필요한 변수만을 빼내는 코드가\n\n``` \n# 고객 고유 식별 번호(ncodpers), 정수로 표현한 날짜(int_date), 실제 날짜(fecha_dato), 24개의 금융 변수(products)와 학습에 사용하기 위해 전처리/피쳐 엔지니어링한 변수(features)가 주요 변수이다.\n    leave_columns = [\"ncodpers\", \"int_date\", \"fecha_dato\"] + list(products) + list(features)\n    # 중복값이 없는지 확인한다\n    assert len(leave_columns) == len(set(leave_columns))\n    # train_df에서 주요 변수만을 추출한다\n    train_df = train_df[leave_columns]\n\n    return train_df, features, prod_features\n    ```\n   \n   \n   가 된다.\n   \n \n \n "
    },
    {
      "metadata": {
        "_uuid": "754f07dd1c7e41c2966961b62b32802630f16311"
      },
      "cell_type": "markdown",
      "source": "features는 학습에 사용하기 위한 변수를 기록한 튜플값. 결측값 대체, log transform, label encoding, one hot encoding, lag 데이터 등 변수명이 포함되어 있다\n\nprdo_features는 lag_1 데이터 변수명을 저장한 튜플\n\n```\nif __name__ == \"__main__\":\n    all_df, features, prod_features = make_data()\n    \n    # 피쳐 엔지니어링이 완료된 데이터를 저장한다\n    train_df.to_pickle(\"../input/8th.feature_engineer.all.pkl\")\n    pickle.dump((features, prod_features), open(\"../input/8th.feature_engineer.cv_meta.pkl\", \"wb\"))\n\n    train_predict(all_df, features, prod_features, \"2016-05-28\", cv=True)\n    train_predict(all_df, features, prod_features, \"2016-06-28\", cv=False)\n``` \n\nmain 함수 마지막 부분. 데이터를 압축파일 형태로 저장한다."
    },
    {
      "metadata": {
        "_uuid": "9886dc0abf931bbe8700d3df4a44d66accd2a85a"
      },
      "cell_type": "markdown",
      "source": "# 모델 학습\n\ntrain_predict() 함수는 머신러닝 모델학습을 위한 준비과정을 마친 후, lightgbm과 xgboost 모델을 학습한다.\n\n첫 번째 train_predict() 함수는 훈련 데이터의 최신 날짜인 160528을 테스트 데이터와 같이 사용, 그 외 데이터는 훈련과 검증 데이터 8:2 비율로 분리한다. \n\n- 검증 데이터를 기반으로 모델의 최적 파라미터를 결정하고,\n- 모든 훈련 데이터를 머신러닝 모델에 다시 학습시킨 다음,\n- 테스트 데이터에 대한 예측값을 만들어낸다.\n이 때 테스트 데이터의 정답값을 기반으로 경진대회 평가척도 값을 확인해서, 케글에 업로드하기 전 모델의 성능을 자체적으로 검증한다.\n\n훈련 데이터 안에서 훈련/검증/테스트를 분리하는 이뉴는 최대한 객관적인 모델 성능을 얻기 위해서.\n\n두 번째는 실제 훈련데이터 전부 써서, 케글에 제출할 예측 결과물을 만들어낸다.\n\n\ntrain_predict()는 머신러닝 학습 직전에 준비과정을 수행한다. 제품 보유여부를 의미하는 금융변수에서 신규 구매정보를 추출함.\n\n탐색적 데이터 분석, baseline 모델에서도 지적됐듯, 제품보유와 신규구매의 차이를 인지하고 신규구매 정보를 추출하는 작업을 수행한다. 신규 구매가 존재하는 데이터를 유효한 데이터로 인식.\n\n이 과정에서 1명 고객이 같은 날 2개 이상의 금융제품을 신규구매할 경우... '구매활동량'이 높은 고객 데이터가 필요 이상으로 많아져, 신규구매가 적거나 평범한 고객의 정확도가 낮아질 수 있다. xgboost와 ligthgbm은 이런 데이터 분포를 고려해 '각 고객 데이터의 가중치를 부여하는 weight변수를 지원한다'. 신규 구매건수가 많은 고객의 경우 weight을 낮게 배정하여 올바른 데이터분포를 유지하는 것."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d414f05e04fb6aa908e02b5a7ded835f02c3815a"
      },
      "cell_type": "code",
      "source": "def train_predict(all_df, features, prod_features, str_date, cv):\n    # all_df : 통합 데이터\n    # features : 학습에 사용할 변수\n    # prod_features : 24개 금융 변수\n    # str_date : 예측 결과물을 산출하는 날짜. 2016-05-28일 경우, 훈련 데이터의 일부이며 정답을 알고 있기에 교차 검증을 의미하고, 2016-06-28일 경우, 캐글에 업로드하기 위한 테스트 데이터 예측 결과물을 생성한다\n    # cv : 교차 검증 실행 여부\n\n    # str_date로 예측 결과물을 산출하는 날짜를 지정한다\n    test_date = date_to_int(str_date)\n    # 훈련 데이터는 test_date 이전의 모든 데이터를 사용한다\n    train_df = all_df[all_df.int_date < test_date]\n    # 테스트 데이터를 통합 데이터에서 분리한다\n    test_df = pd.DataFrame(all_df[all_df.int_date == test_date])\n\n    # 신규 구매 고객만을 훈련 데이터로 추출한다\n    X = []\n    Y = []\n    for i, prod in enumerate(products):\n        prev = prod + \"_prev1\"\n        # 신규 구매 고객을 prX에 저장한다\n        prX = train_df[(train_df[prod] == 1) & (train_df[prev] == 0)]\n        # prY에는 신규 구매에 대한 label 값을 저장한다\n        prY = np.zeros(prX.shape[0], dtype=np.int8) + i\n        X.append(prX)\n        Y.append(prY)\n\n    XY = pd.concat(X)\n    Y = np.hstack(Y)\n    # XY는 신규 구매 데이터만 포함한다\n    XY[\"y\"] = Y\n\n    # 메모리에서 변수 삭제\n    del train_df\n    del all_df\n\n    # 데이터별 가중치를 계산하기 위해서 새로운 변수 (ncodpers + fecha_dato)를 생성한다\n    XY[\"ncodepers_fecha_dato\"] = XY[\"ncodpers\"].astype(str) + XY[\"fecha_dato\"]\n    uniqs, counts = np.unique(XY[\"ncodepers_fecha_dato\"], return_counts=True)\n    # 자연 상수(e)를 통해서, count가 높은 데이터에 낮은 가중치를 준다\n    weights = np.exp(1/counts - 1)\n\n    # 가중치를 XY 데이터에 추가한다\n    wdf = pd.DataFrame()\n    wdf[\"ncodepers_fecha_dato\"] = uniqs\n    wdf[\"counts\"] = counts\n    wdf[\"weight\"] = weights\n    XY = XY.merge(wdf, on=\"ncodepers_fecha_dato\")\n####################################################################################\n    # 교차 검증을 위하여 XY를 훈련:검증 (8:2)로 분리한다\n    mask = np.random.rand(len(XY)) < 0.8\n    XY_train = XY[mask]\n    XY_validate = XY[~mask]\n\n    # 테스트 데이터에서 가중치는 모두 1이다\n    test_df[\"weight\"] = np.ones(len(test_df), dtype=np.int8)\n\n    # 테스트 데이터에서 “신규 구매” 정답값을 추출한다. \n    test_df[\"y\"] = test_df[\"ncodpers\"]\n    Y_prev = test_df.as_matrix(columns=prod_features)\n    for prod in products:\n        prev = prod + \"_prev1\"\n        padd = prod + \"_add\"\n        # 신규 구매 여부를 구한다\n        test_df[padd] = test_df[prod] - test_df[prev]\n\n    test_add_mat = test_df.as_matrix(columns=[prod + \"_add\" for prod in products])\n    C = test_df.as_matrix(columns=[\"ncodpers\"])\n    test_add_list = [list() for i in range(len(C))]\n    # 평가 척도 MAP@7 계산을 위하여, 고객별 신규 구매 정답값을 test_add_list에 기록한다\n    count = 0\n    for c in range(len(C)):\n        for p in range(len(products)):\n            if test_add_mat[c,p] > 0:\n                test_add_list[c].append(p)\n                count += 1\n    \n    # 교차 검증에서, 테스트 데이터로 분리된 데이터가 얻을 수 있는 최대 MAP@7 값을 계산한다. \n    if cv:\n        max_map7 = mapk(test_add_list, test_add_list, 7, 0.0)\n        map7coef = float(len(test_add_list)) / float(sum([int(bool(a)) for a in test_add_list]))\n        print(\"Max MAP@7\", str_date, max_map7, max_map7*map7coef)\n\n    # LightGBM 모델 학습 후, 예측 결과물을 저장한다\n    Y_test_lgbm = engines.lightgbm(XY_train, XY_validate, test_df, features, XY_all = XY, restore = (str_date == \"2016-06-28\"))\n    test_add_list_lightgbm = make_submission(io.BytesIO() if cv else gzip.open(\"tmp/%s.lightgbm.csv.gz\" % str_date, \"wb\"), Y_test_lgbm - Y_prev, C)\n\n    # 교차 검증일 경우, LightGBM 모델의 테스트 데이터 MAP@7 평가 척도를 출력한다\n    if cv:\n        map7lightgbm = mapk(test_add_list, test_add_list_lightgbm, 7, 0.0)\n        print(\"LightGBMlib MAP@7\", str_date, map7lightgbm, map7lightgbm*map7coef)\n\n    # XGBoost 모델 학습 후, 예측 결과물을 저장한다\n    Y_test_xgb = engines.xgboost(XY_train, XY_validate, test_df, features, XY_all = XY, restore = (str_date == \"2016-06-28\"))\n    test_add_list_xgboost = make_submission(io.BytesIO() if cv else gzip.open(\"tmp/%s.xgboost.csv.gz\" % str_date, \"wb\"), Y_test_xgb - Y_prev, C)\n\n    # 교차 검증일 경우, XGBoost 모델의 테스트 데이터 MAP@7 평가 척도를 출력한다\n    if cv:\n        map7xgboost = mapk(test_add_list, test_add_list_xgboost, 7, 0.0)\n        print(\"XGBoost MAP@7\", str_date, map7xgboost, map7xgboost*map7coef)\n\n    # 곱셈 후, 제곱근을 구하는 방식으로 앙상블을 수행한다\n    Y_test = np.sqrt(np.multiply(Y_test_xgb, Y_test_lgbm))\n    # 앙상블 결과물을 저장하고, 테스트 데이터에 대한 MAP@7 를 출력한다\n    test_add_list_xl = make_submission(io.BytesIO() if cv else gzip.open(\"tmp/%s.xgboost-lightgbm.csv.gz\" % str_date, \"wb\"), Y_test - Y_prev, C)\n\n    # 정답값인 test_add_list와 앙상블 모델의 예측값을 mapk 함수에 넣어, 평가 척도 점수를 확인한다\n    if cv:\n        map7xl = mapk(test_add_list, test_add_list_xl, 7, 0.0)\n        print(\"XGBoost+LightGBM MAP@7\", str_date, map7xl, map7xl*map7coef)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "23fe81696777866b371ebfe49bdfb5864da007db"
      },
      "cell_type": "markdown",
      "source": "여기서는 lightgbm, xgboost 각각 하나의 모델을 학습하고, 각 모델의 결과물을 앙상블한 최종 결과물을 생성한다.\n\n실제로는 수많은 모델을 학습해 그 결과물을 케글에 제출하였을 것이다.\n\n다수의 모델 결과물을 생성하고 앙상블을 수행하는 건 경진대회에서 일반적인 전략이다. 가장 일반적인 앙상블은 각 모델의 결과물 산술평균을 계산하는 방법임. 단 이 코드에서는 기하 평균으로 앙상블을 계산함"
    },
    {
      "metadata": {
        "_uuid": "3842dc75b78a759f91054132475de0b9d208c5c2"
      },
      "cell_type": "markdown",
      "source": "### 모델 학습\n\n모델 학습은 engines.py 파일이 담당한다. lightgbm, xgboost 두 함수의 입력값은 동일하며, 함수의 기본 흐름은\n\n1. 교차 검증에 사용되는 훈련 데이터 XY_train, 검증 데이터 XY_validate를 기반으로 최적의 파라미터를 찾는다. 모델 학습 시에는 features 변수를 사용한다.\n2. 전체 훈련 데이터(XY_all)에 최적의 파라미터를 기반으로 모델 학습을 한 후, test_df을 넣고 예측 결과물을 출력한다\n\n### 예측 결과물은 make_submission() 함수로 케글 제출용 파일로 저장된다.\n\n\ncf. baseline 모델의 xgboost 파라미터와 8등 팀의 xgboost모델 파라미터는 동일하다. feature engineering 차이가 낳은 결과임."
    },
    {
      "metadata": {
        "_uuid": "92e284e63dac11b77a30bbba483cbc21155d82a4"
      },
      "cell_type": "markdown",
      "source": "요약\n\n1. 범주형 변수는 자체 구현한 label encoder onehotencoder로 수치형 변수로 변환\n2. 빈도값 기준 상위 100개 데이터를 별도로 순위 표현하는 encoder_top()을 통해 파생변수 생성\n3. 변수 범위가 큰 renta 변수는 log 취해서 정규화\n4. 날짜 변수간의 차이값을 파생변수로 활용\n5. lag_5 변수 활용 - 구간별 표준편차, 최솟값, 최댓값을 구해 다양한 엔지니어링을 구사함.\n6. 같은 고객이 두 개 이상의 제품을 구매했을 때 생기는 빈도수 차이를 조절하기 위해 weight 값을 사용함\n\n"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7096b39d0e19a6aaa21aefabb6479e3c8b19eac2"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}