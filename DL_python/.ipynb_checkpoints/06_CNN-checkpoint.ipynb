{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "# transform -> preprocess before fed to the network\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((.5,),(.5,))])\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download = True, transform = transform)\n",
    "training_loader = torch.utils.data.DataLoader(dataset = train_dataset, batch_size = 100, shuffle=True)\n",
    "\n",
    "\n",
    "def image_convert(tensor):\n",
    "    # 원래 이미지로 돌려놓기 위한 함수\n",
    "    \n",
    "    image = tensor.clone().detach().numpy() # create new copy + detach\n",
    "    # [color channel, width, height] 값으로 구분됨. 엠니스트의 경우 1 * 28 * 28\n",
    "    \n",
    "    image = image.transpose(1,2,0) # 28*28*1로 변경\n",
    "    image = image * np.array((.5,.5,.5)) + np.array((.5,.5,.5)) #정규화한 작업을 풀어주는 거 같은데...\n",
    "\n",
    "    # 학습을 위해 0~1 값을 -1~1로 변경했으므로, 이걸 되돌려 준다\n",
    "    image = image.clip(0,1) # min을 0, max를 1로 변경해주는 것\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 20, 5, 1)\n",
    "        # Conv2d(input_channel 개수, output_channel 개수(feature map 개수), kernal size(3 by 3, 5 by 5), stride)\n",
    "        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n",
    "        self.fc1 = nn.Linear(4*4*50, 500)\n",
    "        self.fc2 = nn.Linear(500, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2,2) # 2 by 2 kernal을 사용하겠단 뜻.\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2,2) # 2 by 2 kernal을 사용하겠단 뜻.\n",
    "        x = x.view(x.size(0),-1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LeNet(\n",
       "  (conv1): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (conv2): Conv2d(20, 50, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (fc1): Linear(in_features=800, out_features=500, bias=True)\n",
       "  (fc2): Linear(in_features=500, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LeNet()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = .01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'device' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-756897842b4b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mrunning_correct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtraining_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'device' is not defined"
     ]
    }
   ],
   "source": [
    "epochs = 12\n",
    "\n",
    "losses = []\n",
    "correct_history = []\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    running_correct = 0\n",
    "    for images, labels in training_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs,labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        _, pred = torch.max(outputs,1) # 튜플이 나올 때 첫번째 값은 image 연산결과고, 두 번째가 label값이 될 것.\n",
    "        # max(output,1)에서 1은 dim으로, 1이기 때문에 각 row별 최대값을 구함.\n",
    "\n",
    "#  tensor([[-1.2360, -0.2942, -0.1222,  0.8475],\n",
    "#         [ 1.1949, -1.1127, -2.2379, -0.6702],\n",
    "#         [ 1.5717, -0.9207,  0.1297, -1.8768],\n",
    "#         [-0.6172,  1.0036, -0.6060, -0.2432]])\n",
    "# torch.max(a, 1)\n",
    "# (tensor([ 0.8475,  1.1949,  1.5717,  1.0036]), tensor([ 3,  0,  0,  1]))\n",
    "\n",
    "        running_correct +=torch.sum(pred == labels.data)\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    else:\n",
    "        epoch_loss = running_loss / len(training_loader)\n",
    "        epoch_acc = running_correct.float() / len(training_loader)\n",
    "        losses.append(epoch_loss)\n",
    "        correct_history.append(epoch_acc)\n",
    "        print('training loss: {:4f}, Accuracy: {:4f}'.format(epoch_loss, epoch_acc))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 15\n",
    "running_loss_history = []\n",
    "running_corrects_history = []\n",
    "val_running_loss_history = []\n",
    "val_running_corrects_history = []\n",
    "\n",
    "for e in range(epochs):\n",
    "\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0.0\n",
    "    val_running_loss = 0.0\n",
    "    val_running_corrects = 0.0\n",
    "\n",
    "    for inputs, labels in training_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        running_loss += loss.item()\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "    else:\n",
    "    with torch.no_grad():\n",
    "      for val_inputs, val_labels in validation_loader:\n",
    "        val_inputs = val_inputs.to(device)\n",
    "        val_labels = val_labels.to(device)\n",
    "        val_outputs = model(val_inputs)\n",
    "        val_loss = criterion(val_outputs, val_labels)\n",
    "\n",
    "        _, val_preds = torch.max(val_outputs, 1)\n",
    "        val_running_loss += val_loss.item()\n",
    "        val_running_corrects += torch.sum(val_preds == val_labels.data)\n",
    "\n",
    "    epoch_loss = running_loss/len(training_loader)\n",
    "    epoch_acc = running_corrects.float()/ len(training_loader)\n",
    "    running_loss_history.append(epoch_loss)\n",
    "    running_corrects_history.append(epoch_acc)\n",
    "\n",
    "    val_epoch_loss = val_running_loss/len(validation_loader)\n",
    "    val_epoch_acc = val_running_corrects.float()/ len(validation_loader)\n",
    "    val_running_loss_history.append(val_epoch_loss)\n",
    "    val_running_corrects_history.append(val_epoch_acc)\n",
    "    print('epoch :', (e+1))\n",
    "    print('training loss: {:.4f}, acc {:.4f} '.format(epoch_loss, epoch_acc.item()))\n",
    "    print('validation loss: {:.4f}, validation acc {:.4f} '.format(val_epoch_loss, val_epoch_acc.item()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
