{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchtext.datasets import Multi30k\n",
    "from torchtext.data import Field, BucketIterator\n",
    "\n",
    "import spacy\n",
    "\n",
    "import random\n",
    "import math\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 2222\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_de = spacy.load('de')\n",
    "spacy_en = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_en(text):\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
    "def process_de(text):\n",
    "    return [tok.text for tok in spacy_de.tokenizer(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Source = Field(tokenize=process_de, init_token='<sos>', eos_token='<eos>', lower=True)\n",
    "Target = Field(tokenize=process_en, init_token='<sos>', eos_token='<eos>', lower=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, valid_data, test_data = Multi30k.splits(exts=('.de', '.en'), fields=(Source, Target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29000, 1014, 1000)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data),len(valid_data),len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Source.build_vocab(train_data, min_freq=2)\n",
    "Target.build_vocab(train_data, min_freq=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device.type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), batch_size=BATCH_SIZE, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.emb_dim = emb_dim\n",
    "        self.enc_hid_dim = enc_hid_dim\n",
    "        self.dec_hid_dim = dec_hid_dim\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        self.rnn = nn.GRU(emb_dim, enc_hid_dim, bidirectional=True)\n",
    "        self.fc = nn.Linear(enc_hid_dim * 2, dec_hid_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, src):\n",
    "        embedded = self.dropout(self.embedding(src))#[sent len, batch size]\n",
    "        outputs, hidden = self.rnn(embedded)#[sent len, batch size, emb dim]\n",
    "        #outputs -> [sent len, batch size, hid dim * n directions]\n",
    "        #hidden -> [n layers * n directions, batch size, hid dim]\n",
    "        hidden = torch.tanh(self.fc(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)))\n",
    "        #hidden -> [batch size, dec hid dim]\n",
    "        return outputs, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, enc_hid_dim, dec_hid_dim):\n",
    "        super().__init__()\n",
    "        self.enc_hid_dim = enc_hid_dim\n",
    "        self.dec_hid_dim = dec_hid_dim\n",
    "        self.attn = nn.Linear((enc_hid_dim * 2) + dec_hid_dim, dec_hid_dim)\n",
    "        self.vec = nn.Parameter(torch.rand(dec_hid_dim))\n",
    "    \n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        #hidden -> [batch size, dec hid dim]\n",
    "        #encoder_outputs -> [src sent len, batch size, enc hid dim * 2]\n",
    "        batch_size = encoder_outputs.shape[1]\n",
    "        src_len = encoder_outputs.shape[0]\n",
    "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "        #hidden -> [batch size, src sent len, dec hid dim]\n",
    "        #encoder_outputs -> [batch size, src sent len, enc hid dim * 2]\n",
    "        association = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))\n",
    "        #association -> [batch size, src sent len, dec hid dim]\n",
    "        association = association.permute(0, 2, 1)\n",
    "        #association -> [batch size, dec hid dim, src sent len]\n",
    "        #vec -> [dec hid dim]\n",
    "        vec = self.vec.repeat(batch_size, 1).unsqueeze(1)\n",
    "        #vec -> [batch size, 1, dec hid dim]\n",
    "        attention = torch.bmm(vec, association).squeeze(1)\n",
    "        #attention-> [batch size, src len]\n",
    "        return F.softmax(attention, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout, attention):\n",
    "        super().__init__()\n",
    "        self.emb_dim = emb_dim\n",
    "        self.enc_hid_dim = enc_hid_dim\n",
    "        self.dec_hid_dim = dec_hid_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.attention = attention\n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        self.rnn = nn.GRU((enc_hid_dim * 2) + emb_dim, dec_hid_dim)\n",
    "        self.out = nn.Linear((enc_hid_dim * 2) + dec_hid_dim + emb_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        #input -> [batch size]\n",
    "        #hidden -> [batch size, dec hid dim]\n",
    "        #encoder_outputs -> [src sent len, batch size, enc hid dim * 2]\n",
    "        input = input.unsqueeze(0)\n",
    "        #input -> [1, batch size]\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        #embedded -> [1, batch size, emb dim]\n",
    "        a = self.attention(hidden, encoder_outputs)\n",
    "        #a -> [batch size, src len]\n",
    "        a = a.unsqueeze(1)\n",
    "        #a -> [batch size, 1, src len]\n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "        #encoder_outputs -> [batch size, src sent len, enc hid dim * 2]\n",
    "        weighted = torch.bmm(a, encoder_outputs)\n",
    "        #weighted -> [batch size, 1, enc hid dim * 2]\n",
    "        weighted = weighted.permute(1, 0, 2)\n",
    "        #weighted -> [1, batch size, enc hid dim * 2]\n",
    "        rnn_input = torch.cat((embedded, weighted), dim=2)\n",
    "        #rnn_input -> [1, batch size, (enc hid dim * 2) + emb dim]\n",
    "        output, hidden = self.rnn(rnn_input, hidden.unsqueeze(0))\n",
    "        #output -> [sent len, batch size, dec hid dim * n directions]\n",
    "        #hidden -> [n layers * n directions, batch size, dec hid dim]\n",
    "        #output -> [1, batch size, dec hid dim]\n",
    "        #hidden -> [1, batch size, dec hid dim]\n",
    "        embedded = embedded.squeeze(0)\n",
    "        output = output.squeeze(0)\n",
    "        weighted = weighted.squeeze(0)\n",
    "        output = self.out(torch.cat((output, weighted, embedded), dim=1))\n",
    "        #output -> [batch size, output dim]\n",
    "        return output, hidden.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        \n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "        #src->[sent len, batch size]\n",
    "        #trg->[sent len, batch size]\n",
    "        \n",
    "        batch_size = trg.shape[1]\n",
    "        max_len = trg.shape[0]\n",
    "        target_voc_size = self.decoder.output_dim\n",
    "        outputs = torch.zeros(max_len, batch_size, target_voc_size).to(self.device)\n",
    "        encoder_outputs, hidden = self.encoder(src)\n",
    "        input = trg[0,:]\n",
    "        for t in range(1, max_len):\n",
    "            output, hidden = self.decoder(input, hidden, encoder_outputs)\n",
    "            outputs[t] = output\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            top1 = output.max(1)[1]\n",
    "            input = (trg[t] if teacher_force else top1)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(Source.vocab)\n",
    "OUTPUT_DIM = len(Target.vocab)\n",
    "ENC_EMB_DIM = 256\n",
    "DEC_EMB_DIM = 256\n",
    "ENC_HID_DIM = 512\n",
    "DEC_HID_DIM = 512\n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.5\n",
    "\n",
    "attn = Attention(ENC_HID_DIM, DEC_HID_DIM)\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn)\n",
    "\n",
    "model = Seq2Seq(enc, dec, device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(7853, 256)\n",
       "    (rnn): GRU(256, 512, bidirectional=True)\n",
       "    (fc): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (dropout): Dropout(p=0.5)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (attention): Attention(\n",
       "      (attn): Linear(in_features=1536, out_features=512, bias=True)\n",
       "    )\n",
       "    (embedding): Embedding(5893, 256)\n",
       "    (rnn): GRU(1280, 512)\n",
       "    (out): Linear(in_features=1792, out_features=5893, bias=True)\n",
       "    (dropout): Dropout(p=0.5)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_idx = Target.vocab.stoi['<pad>']\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for i, batch in enumerate(iterator):\n",
    "        source = batch.src\n",
    "        target = batch.trg#[sent len, batch size]\n",
    "        optimizer.zero_grad()\n",
    "        output = model(source, target)#[sent len, batch size, output dim]\n",
    "        loss = criterion(output[1:].view(-1, output.shape[2]), target[1:].view(-1))\n",
    "        #trg->[(sent len - 1) * batch size]\n",
    "        #output->[(sent len - 1) * batch size, output dim]\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(iterator):\n",
    "            source = batch.src\n",
    "            target = batch.trg\n",
    "            output = model(source, target, 0)\n",
    "            loss = criterion(output[1:].view(-1, output.shape[2]), target[1:].view(-1))\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
	    "| Epoch: 001 | Train Loss: 3.786 | Train PPL:  44.060 | Val. Loss: 3.562 | Val. PPL:  35.239 |\n",
            "| Epoch: 002 | Train Loss: 2.999 | Train PPL:  20.072 | Val. Loss: 3.309 | Val. PPL:  27.357 |\n",
            "| Epoch: 003 | Train Loss: 2.615 | Train PPL:  13.666 | Val. Loss: 3.317 | Val. PPL:  27.577 |\n",
            "| Epoch: 004 | Train Loss: 2.353 | Train PPL:  10.520 | Val. Loss: 3.257 | Val. PPL:  25.983 |\n",
            "| Epoch: 005 | Train Loss: 2.148 | Train PPL:   8.569 | Val. Loss: 3.324 | Val. PPL:  27.763 |\n",
            "| Epoch: 006 | Train Loss: 2.006 | Train PPL:   7.435 | Val. Loss: 3.254 | Val. PPL:  25.894 |\n",
            "| Epoch: 007 | Train Loss: 1.901 | Train PPL:   6.693 | Val. Loss: 3.299 | Val. PPL:  27.089 |\n",
            "| Epoch: 008 | Train Loss: 1.811 | Train PPL:   6.119 | Val. Loss: 3.333 | Val. PPL:  28.022 |\n",
            "| Epoch: 009 | Train Loss: 1.749 | Train PPL:   5.748 | Val. Loss: 3.355 | Val. PPL:  28.657 |\n",
            "| Epoch: 010 | Train Loss: 1.672 | Train PPL:   5.324 | Val. Loss: 3.425 | Val. PPL:  30.725 |\n"
     ]
    }
   ],
   "source": [
    "DIR = 'models'\n",
    "MODEL_DIR = os.path.join(DIR, 'seq2seq_model.pt')\n",
    "\n",
    "N_EPOCHS = 10\n",
    "CLIP = 10\n",
    "\n",
    "best_loss = float('inf')\n",
    "\n",
    "if not os.path.isdir(f'{DIR}'):\n",
    "    os.makedirs(f'{DIR}')\n",
    "    \n",
    "for epoch in range(N_EPOCHS):\n",
    "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
    "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    if valid_loss < best_loss:\n",
    "        best_loss = valid_loss\n",
    "        torch.save(model.state_dict(), MODEL_DIR)\n",
    "    print(f'| Epoch: {epoch+1:03} | Train Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f} | Val. Loss: {valid_loss:.3f} | Val. PPL: {math.exp(valid_loss):7.3f} |')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Test Loss: 3.332 | Test PPL:  28.003 |\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(MODEL_DIR))\n",
    "\n",
    "test_loss = evaluate(model, test_iterator, criterion)\n",
    "\n",
    "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
