{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchtext.datasets import Multi30k\n",
    "from torchtext.data import Field, BucketIterator\n",
    "\n",
    "import spacy\n",
    "\n",
    "import random\n",
    "import math\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 2222\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_de = spacy.load('de')\n",
    "spacy_en = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_en(text):\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
    "def process_de(text):\n",
    "    return [tok.text for tok in spacy_de.tokenizer(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Source = Field(tokenize=process_de, init_token='<sos>', eos_token='<eos>', lower=True)\n",
    "Target = Field(tokenize=process_en, init_token='<sos>', eos_token='<eos>', lower=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, valid_data, test_data = Multi30k.splits(exts=('.de', '.en'), fields=(Source, Target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29000, 1014, 1000)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data),len(valid_data),len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Source.build_vocab(train_data, min_freq=2)\n",
    "Target.build_vocab(train_data, min_freq=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device.type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), batch_size=BATCH_SIZE, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hid_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.emb_dim = emb_dim\n",
    "        self.hid_dim = hid_dim\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        self.rnn = nn.GRU(emb_dim, hid_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, src):\n",
    "        embedded = self.dropout(self.embedding(src))#[sent len, batch size]\n",
    "        outputs, hidden = self.rnn(embedded)#[sent len, batch size, emb dim]\n",
    "        #outputs -> [sent len, batch size, hid dim * n directions]\n",
    "        #hidden -> [n layers * n directions, batch size, hid dim]\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hid_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.emb_dim = emb_dim\n",
    "        self.hid_dim = hid_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        self.rnn = nn.GRU(emb_dim + hid_dim, hid_dim)\n",
    "        self.out = nn.Linear(emb_dim + hid_dim*2, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, input, hidden, context):\n",
    "        #input -> [batch size]\n",
    "        #hidden -> [n layers * n directions, batch size, hid dim]\n",
    "        #context -> [n layers * n directions, batch size, hid dim]\n",
    "        \n",
    "        #hidden -> [1, batch size, hid dim]\n",
    "        #context -> [1, batch size, hid dim]\n",
    "        \n",
    "        input = input.unsqueeze(0)\n",
    "        #input -> [1, batch size]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        #embedded -> [1, batch size, emb dim]\n",
    "        \n",
    "        emb_con = torch.cat((embedded, context), dim=2)\n",
    "        #emb_con -> [1, batch size, emb dim + hid dim]\n",
    "        \n",
    "        output, hidden = self.rnn(emb_con, hidden)\n",
    "        #output -> [sent len, batch size, hid dim * n directions]\n",
    "        #hidden -> [n layers * n directions, batch size, hid dim]\n",
    "        #output -> [1, batch size, hid dim]\n",
    "        #hidden -> [1, batch size, hid dim]\n",
    "        \n",
    "        output = torch.cat((embedded.squeeze(0), hidden.squeeze(0), context.squeeze(0)), dim=1)\n",
    "        #output -> [batch size, emb dim + hid dim * 2]\n",
    "        \n",
    "        prediction = self.out(output)\n",
    "        \n",
    "        return prediction,hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        \n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "        #src->[sent len, batch size]\n",
    "        #trg->[sent len, batch size]\n",
    "        \n",
    "        batch_size = trg.shape[1]\n",
    "        max_len = trg.shape[0]\n",
    "        target_voc_size = self.decoder.output_dim\n",
    "        outputs = torch.zeros(max_len, batch_size, target_voc_size).to(self.device)\n",
    "        context = self.encoder(src)\n",
    "        hidden = context\n",
    "        input = trg[0,:]\n",
    "        for t in range(1, max_len):\n",
    "            output, hidden = self.decoder(input, hidden, context)\n",
    "            outputs[t] = output\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            top1 = output.max(1)[1]\n",
    "            input = (trg[t] if teacher_force else top1)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(Source.vocab)\n",
    "OUTPUT_DIM = len(Target.vocab)\n",
    "ENC_EMB_DIM = 256\n",
    "DEC_EMB_DIM = 256\n",
    "HID_DIM = 512\n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.5\n",
    "\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, ENC_DROPOUT)\n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, DEC_DROPOUT)\n",
    "\n",
    "model = Seq2Seq(enc, dec, device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(7853, 256)\n",
       "    (rnn): GRU(256, 512)\n",
       "    (dropout): Dropout(p=0.5)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (embedding): Embedding(5893, 256)\n",
       "    (rnn): GRU(768, 512)\n",
       "    (out): Linear(in_features=1280, out_features=5893, bias=True)\n",
       "    (dropout): Dropout(p=0.5)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_idx = Target.vocab.stoi['<pad>']\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for i, batch in enumerate(iterator):\n",
    "        source = batch.src\n",
    "        target = batch.trg#[sent len, batch size]\n",
    "        optimizer.zero_grad()\n",
    "        output = model(source, target)#[sent len, batch size, output dim]\n",
    "        loss = criterion(output[1:].view(-1, output.shape[2]), target[1:].view(-1))\n",
    "        #trg->[(sent len - 1) * batch size]\n",
    "        #output->[(sent len - 1) * batch size, output dim]\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(iterator):\n",
    "            source = batch.src\n",
    "            target = batch.trg\n",
    "            output = model(source, target, 0)\n",
    "            loss = criterion(output[1:].view(-1, output.shape[2]), target[1:].view(-1))\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch: 001 | Train Loss: 4.641 | Train PPL: 103.700 | Val. Loss: 4.439 | Val. PPL:  84.687 |\n",
      "| Epoch: 002 | Train Loss: 3.666 | Train PPL:  39.111 | Val. Loss: 3.914 | Val. PPL:  50.118 |\n",
      "| Epoch: 003 | Train Loss: 3.204 | Train PPL:  24.630 | Val. Loss: 3.718 | Val. PPL:  41.171 |\n",
      "| Epoch: 004 | Train Loss: 2.877 | Train PPL:  17.765 | Val. Loss: 3.605 | Val. PPL:  36.765 |\n",
      "| Epoch: 005 | Train Loss: 2.672 | Train PPL:  14.471 | Val. Loss: 3.554 | Val. PPL:  34.937 |\n",
      "| Epoch: 006 | Train Loss: 2.459 | Train PPL:  11.693 | Val. Loss: 3.614 | Val. PPL:  37.126 |\n",
      "| Epoch: 007 | Train Loss: 2.296 | Train PPL:   9.937 | Val. Loss: 3.581 | Val. PPL:  35.921 |\n",
      "| Epoch: 008 | Train Loss: 2.195 | Train PPL:   8.984 | Val. Loss: 3.649 | Val. PPL:  38.444 |\n",
      "| Epoch: 009 | Train Loss: 2.091 | Train PPL:   8.089 | Val. Loss: 3.642 | Val. PPL:  38.174 |\n",
      "| Epoch: 010 | Train Loss: 2.000 | Train PPL:   7.387 | Val. Loss: 3.679 | Val. PPL:  39.615 |\n"
     ]
    }
   ],
   "source": [
    "DIR = 'models'\n",
    "MODEL_DIR = os.path.join(DIR, 'seq2seq_model.pt')\n",
    "\n",
    "N_EPOCHS = 10\n",
    "CLIP = 10\n",
    "\n",
    "best_loss = float('inf')\n",
    "\n",
    "if not os.path.isdir(f'{DIR}'):\n",
    "    os.makedirs(f'{DIR}')\n",
    "    \n",
    "for epoch in range(N_EPOCHS):\n",
    "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
    "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    if valid_loss < best_loss:\n",
    "        best_loss = valid_loss\n",
    "        torch.save(model.state_dict(), MODEL_DIR)\n",
    "    print(f'| Epoch: {epoch+1:03} | Train Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f} | Val. Loss: {valid_loss:.3f} | Val. PPL: {math.exp(valid_loss):7.3f} |')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Test Loss: 3.525 | Test PPL:  33.950 |\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(MODEL_DIR))\n",
    "\n",
    "test_loss = evaluate(model, test_iterator, criterion)\n",
    "\n",
    "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
