{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word embedding의 의미??\n",
    "\n",
    "learned representation of words를 의미한다.\n",
    "\n",
    "- similar meanings => similar representation 전제에서 출발.\n",
    "\n",
    "Dense, low dimensional representations 형태로, NN에 적용시켜 학습하기 편하다. (원 핫 인코딩에 비해서)\n",
    "\n",
    "(특정 word가 나타났는지 여부 확인 + 해당 단어의 의미를 encapsulate할 수 있는 거라고 보면 된다)\n",
    "\n",
    "예컨대 20000개 단어를 one_hot 하면 input은 20000개일 거고, 대부분의 값은 0일 것이다. 이거 대신 사용하려는 것.\n",
    "\n",
    "ex) this is awful movie / this is terrible movie. 두 개의 단어는 형용사만 제외하면 동일한 의미이다.\n",
    "\n",
    "이 경우는 contextual operation을 할 수 있다. by performing vector operations on these learned representations.\n",
    "\n",
    "King - men + women = Queen...\n",
    "\n",
    "\n",
    "워드 임베딩 알고리즘의 종류\n",
    "- embedding layer -> Neural network의 한 레이어를 아예 embedding으로 할애하는 것\n",
    "- word2vec -> predicts words in similar context. \n",
    "- glove -> similar context의 word count를 바탕으로 한 알고리즘\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    reviewsFile = open(\"./Hands-on-NLP-with-PyTorch-master/data/reviews.txt\",'r')\n",
    "    reviews = list(map(lambda x:x[:-1],reviewsFile.readlines()))\n",
    "    reviewsFile.close()\n",
    "\n",
    "    labelsFile = open(\"./Hands-on-NLP-with-PyTorch-master/data/labels.txt\",'r')\n",
    "    labels = list(map(lambda x:x[:-1],labelsFile.readlines()))\n",
    "    labelsFile.close()\n",
    "    \n",
    "    return reviews,labels\n",
    "\n",
    "reviews, labels = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(\"\\w+\\'?\\w+|\\w+\")\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "exceptionStopWords = {\n",
    "    'again',\n",
    "    'against',\n",
    "    'ain',\n",
    "    'almost',\n",
    "    'among',\n",
    "    'amongst',\n",
    "    'amount',\n",
    "    'anyhow',\n",
    "    'anyway',\n",
    "    'aren',\n",
    "    \"aren't\",\n",
    "    'below',\n",
    "    'bottom',\n",
    "    'but',\n",
    "    'cannot',\n",
    "    'couldn',\n",
    "    \"couldn't\",\n",
    "    'didn',\n",
    "    \"didn't\",\n",
    "    'doesn',\n",
    "    \"doesn't\",\n",
    "    'don',\n",
    "    \"don't\",\n",
    "    'done',\n",
    "    'down',\n",
    "    'except',\n",
    "    'few',\n",
    "    'hadn',\n",
    "    \"hadn't\",\n",
    "    'hasn',\n",
    "    \"hasn't\",\n",
    "    'haven',\n",
    "    \"haven't\",\n",
    "    'however',\n",
    "    'isn',\n",
    "    \"isn't\",\n",
    "    'least',\n",
    "    'mightn',\n",
    "    \"mightn't\",\n",
    "    'move',\n",
    "    'much',\n",
    "    'must',\n",
    "    'mustn',\n",
    "    \"mustn't\",\n",
    "    'needn',\n",
    "    \"needn't\",\n",
    "    'neither',\n",
    "    'never',\n",
    "    'nevertheless',\n",
    "    'no',\n",
    "    'nobody',\n",
    "    'none',\n",
    "    'noone',\n",
    "    'nor',\n",
    "    'not',\n",
    "    'nothing',\n",
    "    'should',\n",
    "    \"should've\",\n",
    "    'shouldn',\n",
    "    \"shouldn't\",\n",
    "    'too',\n",
    "    'top',\n",
    "    'up',\n",
    "    'wasn',\n",
    "    \"wasn't\",\n",
    "    'well',\n",
    "    'weren',\n",
    "    \"weren't\",\n",
    "    'won',\n",
    "    \"won't\",\n",
    "    'wouldn',\n",
    "    \"wouldn't\",\n",
    "}\n",
    "\n",
    "stop_words = set(stop_words).union(STOP_WORDS)\n",
    "\n",
    "final_stop_words = stop_words-exceptionStopWords\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"en\",disable=['parser', 'tagger', 'ner'])\n",
    "\n",
    "def make_token(review):\n",
    "    return tokenizer.tokenize(str(review))\n",
    "\n",
    "def remove_stopwords(review):\n",
    "    return [token for token in review if token not in final_stop_words]\n",
    "\n",
    "def lemmatization(review):\n",
    "    lemma_result = []\n",
    "    \n",
    "    for words in review:\n",
    "        doc = nlp(words)\n",
    "        for token in doc:\n",
    "            lemma_result.append(token.lemma_)\n",
    "    return lemma_result\n",
    "\n",
    "def pipeline(review):\n",
    "    review = make_token(review)\n",
    "    review = remove_stopwords(review)\n",
    "    return lemmatization(review)\n",
    "\n",
    "# %%time\n",
    "reviews = list(map(lambda review: pipeline(review),reviews))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['bromwell',\n",
       "  'high',\n",
       "  'cartoon',\n",
       "  'comedy',\n",
       "  'run',\n",
       "  'time',\n",
       "  'program',\n",
       "  'school',\n",
       "  'life',\n",
       "  'teacher',\n",
       "  'year',\n",
       "  'teach',\n",
       "  'profession',\n",
       "  'lead',\n",
       "  'believe',\n",
       "  'bromwell',\n",
       "  'high',\n",
       "  'satire',\n",
       "  'much',\n",
       "  'close',\n",
       "  'reality',\n",
       "  'teacher',\n",
       "  'scramble',\n",
       "  'survive',\n",
       "  'financially',\n",
       "  'insightful',\n",
       "  'student',\n",
       "  'right',\n",
       "  'pathetic',\n",
       "  'teacher',\n",
       "  'pomp',\n",
       "  'pettiness',\n",
       "  'situation',\n",
       "  'remind',\n",
       "  'school',\n",
       "  'know',\n",
       "  'student',\n",
       "  'see',\n",
       "  'episode',\n",
       "  'student',\n",
       "  'repeatedly',\n",
       "  'try',\n",
       "  'burn',\n",
       "  'down',\n",
       "  'school',\n",
       "  'immediately',\n",
       "  'recall',\n",
       "  'high',\n",
       "  'classic',\n",
       "  'line',\n",
       "  'inspector',\n",
       "  'sack',\n",
       "  'teacher',\n",
       "  'student',\n",
       "  'welcome',\n",
       "  'bromwell',\n",
       "  'high',\n",
       "  'expect',\n",
       "  'adult',\n",
       "  'age',\n",
       "  'think',\n",
       "  'bromwell',\n",
       "  'high',\n",
       "  'far',\n",
       "  'fetch',\n",
       "  'pity',\n",
       "  'isn'],\n",
       " ['story',\n",
       "  'man',\n",
       "  'unnatural',\n",
       "  'feeling',\n",
       "  'pig',\n",
       "  'start',\n",
       "  'open',\n",
       "  'scene',\n",
       "  'terrific',\n",
       "  'example',\n",
       "  'absurd',\n",
       "  'comedy',\n",
       "  'formal',\n",
       "  'orchestra',\n",
       "  'audience',\n",
       "  'turn',\n",
       "  'insane',\n",
       "  'violent',\n",
       "  'mob',\n",
       "  'crazy',\n",
       "  'chantings',\n",
       "  'singer',\n",
       "  'unfortunately',\n",
       "  'stay',\n",
       "  'absurd',\n",
       "  'time',\n",
       "  'no',\n",
       "  'general',\n",
       "  'narrative',\n",
       "  'eventually',\n",
       "  'make',\n",
       "  'too',\n",
       "  'putt',\n",
       "  'era',\n",
       "  'should',\n",
       "  'turn',\n",
       "  'cryptic',\n",
       "  'dialogue',\n",
       "  'shakespeare',\n",
       "  'easy',\n",
       "  'grader',\n",
       "  'technical',\n",
       "  'level',\n",
       "  'well',\n",
       "  'think',\n",
       "  'good',\n",
       "  'cinematography',\n",
       "  'future',\n",
       "  'great',\n",
       "  'vilmos',\n",
       "  'zsigmond',\n",
       "  'future',\n",
       "  'star',\n",
       "  'sally',\n",
       "  'kirkland',\n",
       "  'frederic',\n",
       "  'forrest',\n",
       "  'see',\n",
       "  'briefly']]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews[:2]\n",
    "# base form이며, stopwords는 제거된 상태."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 100\n",
    "# 이 값은 보통 vocab size & text couples에 따라 다르다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(reviews, size = embedding_dim, window = 3, min_count = 3, workers = 4)\n",
    "# window = N of words that the embedding has to consider for a given words.\n",
    "# min_count는 3. 최소 3번은 등장한 단어부터 model에 포함된다는 소리\n",
    "# train을 마치면, model을 삭제하는 걸 추천한다고 함. 메모리 소모가 많기 때문이라고.\n",
    "# embedding에 필요한 모든 정보를 가지고 있는 model.wv를 쓴다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = model.wv\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28165"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_vectors.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "word embedding이 찾아 준, similar words 조합을 찾아보자\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('decent', 0.7098484039306641),\n",
       " ('alright', 0.670569121837616),\n",
       " ('darn', 0.6590393781661987),\n",
       " ('okay', 0.6483892202377319),\n",
       " ('great', 0.6436715126037598)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors.similar_by_word(word='good', topn=5)\n",
    "# 상위 5개 조합. 비슷해 보이는 의미라고 유추 가능하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('terrible', 0.7132728099822998),\n",
       " ('horrible', 0.7132444381713867),\n",
       " ('suck', 0.6856053471565247),\n",
       " ('lousy', 0.679572343826294),\n",
       " ('awful', 0.6717656850814819)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors.similar_by_word(word='bad', topn=5)\n",
    "# 상위 5개 조합. 비슷해 보이는 의미라고 유추 가능하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('terrible', 0.7132728099822998),\n",
       " ('horrible', 0.7132444381713867),\n",
       " ('suck', 0.6856053471565247),\n",
       " ('lousy', 0.679572343826294),\n",
       " ('awful', 0.6717656850814819)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors.most_similar(positive='bad', topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5713808953482875"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors.similarity('good','bad')\n",
    "# 두 단어가 similar context에서 등장할 경우 높고, 아닐 경우 낮은 값이 나온다\n",
    "# 다시말해 this is a good movie / this is a bad movie와 같이 유사한 형태로 많이 등장하는 단어라는 뜻이고"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.29746172755519396"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors.similarity('good', 'be')\n",
    "# when they don't appear in simliar context.\n",
    "# this is a good movie와 같은 문장 맥락에서 be라는 단어는 잘 안 쓰였다는 의미다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('depress', 0.7849045991897583),\n",
       " ('cry', 0.7138973474502563),\n",
       " ('heartwarming', 0.6890783905982971),\n",
       " ('happy', 0.679260790348053),\n",
       " ('scary', 0.6663157939910889)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors.similar_by_word('sad',topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('solid', 0.3796919882297516),\n",
       " ('fine', 0.3740949034690857),\n",
       " ('nicely', 0.3674663305282593),\n",
       " ('support', 0.3629467189311981),\n",
       " ('splendid', 0.36054667830467224)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# contextual relationship btwn words\n",
    "# king - men + women = queen 같은\n",
    "word_vectors.most_similar(negative = ['bad'], positive=['decent'],topn=5)\n",
    "# positive = needs to be added, negative= needs to be substracted\n",
    "# contextual relationship의 workout을 이런 식으로 확인할 수 있다고 함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretrained Embedding models\n",
    "\n",
    "- word embedding = statistic method to learn standalone word embeddings from a text corpus. 2 Different types of models.\n",
    "\n",
    "2 dif types {\n",
    "    continuous bag of words : current word is predicted from its surroundings.\n",
    "    continuous skip gram model : surrounding words are predicted from a current words.\n",
    "}\n",
    "두 개의 모델을 바탕으로 만들어지는 게 word embedding\n",
    "\n",
    "pretrained 모델을 쓸 수 있다고 함.\n",
    "\n",
    "``from gensim.models import KeyedVectors``\n",
    "\n",
    "``model = KeyedVectors.load_word2vec_format('data/GoogleGoogleNews-vectors-negative300.bin/', binary=True)``\n",
    "\n",
    "구글 뉴스데이터를 바탕으로 한 값들. 특정 디렉토리에 다운로드해 저장한 다음 필요할 때 꺼내쓰는 게 일반적이다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
