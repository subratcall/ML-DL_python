{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN (Pytorch 첫걸음)\n",
    "\n",
    "일반적인 신경망과 다르게, 내부 상태를 저장하고 있다. 특정 시점 t의 입력값 x(t)와 이전 시점의 내부상태 h(t-1)을 입력하면 새로운 내부상태 h(t)를 출력.\n",
    "\n",
    "일반 신경망보다 train이 어렵다. 오랜 시간 쌓인 이력을 사용한다는 건 그만큼 layer가 deep하다는 것. gradient vanishing이나 경사 분실 문제 발생 가능.\n",
    "\n",
    "\n",
    "따라서 RNN의 layer를 단순 선형(누적형태) 대신 정교한 처리 모듈로 변경한 LSTM이나 GRU 등의 RNN 모듈도 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 텍스트 데이터의 수치화\n",
    "\n",
    "세 단계로 구성\n",
    "1. 정규화 & 토큰화\n",
    "2. Dictionary 구축\n",
    "3. 수치로 변환\n",
    "\n",
    "1. 문장을 특정 단위의 리스트로 분해한다. 예컨대 단어나 문자 등을 분할 단위로 사용. 유럽계 언어는 공백으로 구분해도 충분하지만 일본어, 중국어 등은 형태소 분석 처리가 필요하기도 함. 표기 차이도 통일해야 함. 예컨대 소문자로 전부 통일 / ~한다, ~하다 = ~하다로 통일. isn't 는 is not으로 통일하는 등.\n",
    "\n",
    "2. 모든 문장의 집합(Corpus)에 대한 토큰을 수집하고, 숫자 id를 부여하는 작업. 등장 순서대로 부여하거나 빈도수에 따라 부여하거나.\n",
    "\n",
    "3. 토큰의 리스트로 분할된 문장을 Dictionary를 사용해 id list로 변환한다.\n",
    "\n",
    "이 작업을 거치며 하나의 긴 문자열이었던 문장이 '수치 리스트'로 변한다. 이 리스트를 다시 집계하고 id의 등장횟수를 벡터로 표현한 것이 BoW.\n",
    "\n",
    "ex) (I, you, am, of ...) = (1,0,1,3 ..)\n",
    "\n",
    "BoW는 계산이 간단하고, 여러 문장을 모으면 희소행렬로 표현할 수 있어 효율은 좋다. 단 토큰 순서 정보를 잃는다.\n",
    "\n",
    "신경망에서는 Embedding이라는 기법으로 토큰을 벡터화하고, 벡터 데이터의 시계열로 문장을 처리하는 것이 주류임.\n",
    "\n",
    "Pytorch는 nn.Embedding으로 layer 생성이 가능하다.\n",
    "\n",
    "\n",
    "```python\n",
    "# 전체 10,000 종류의 토큰을 20차원 벡터로 표현하는 경우\n",
    "emb = nn.Embedding(10000, 20, padding_idx = 0)\n",
    "# Embedding layer input타입은 int64\n",
    "inp = torch.tensor([1,2,5,2,10], dtype = torch.int64)\n",
    "# 출력은 float32\n",
    "out = emb(inp)\n",
    "```\n",
    "\n",
    "padding_idx를 지정하므로, 이 경우 id가 0인 벡터로 바뀐다. 사전에 없는 토큰은 id가 0, 실제 id는 1부터 시작하도록 세팅한다.\n",
    "\n",
    "토큰 종류는 0을 포함한 수를 nn.Embedding의 첫 번째 인수로 지정해야 한다.\n",
    "\n",
    "\n",
    "(nn.Embedding 값도 미분 가능. 내부의 가중치 파라미터의 학습이 가능하다는 의미다. Neural Net 학습 시 여기도 최적화가 가능함. 사전에 학습된 nn.Embedding 값에 기반한 transfer Learning 가능)  -- ex) Word2Vec으로 유명한 Continuous-BOW나 Skip-Gram 등의 모델을 쓰는 경우"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN 문장분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-05-20 06:10:00--  http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
      "Resolving ai.stanford.edu (ai.stanford.edu)... 171.64.68.10\n",
      "Connecting to ai.stanford.edu (ai.stanford.edu)|171.64.68.10|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 84125825 (80M) [application/x-gzip]\n",
      "Saving to: 'aclImdb_v1.tar.gz'\n",
      "\n",
      "aclImdb_v1.tar.gz   100%[===================>]  80.23M  10.1MB/s    in 9.4s    \n",
      "\n",
      "2019-05-20 06:10:13 (8.54 MB/s) - 'aclImdb_v1.tar.gz' saved [84125825/84125825]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "!tar xf aclImdb_v1.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import pathlib\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_marks_regex = re.compile(\"[,\\.\\(\\)\\[\\]\\*:;]|<.*?>\")\n",
    "shift_marks_regex = re.compile(\"([?!])\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def text_to_ids(text, vocab_dict):\n",
    "    # ?! 이외의 기호 삭제\n",
    "    text = remove_marks_regex.sub(\"\",text)\n",
    "    # ?!와 단어 사이의 공백 삽입\n",
    "    text = shift_marks_regex.sub(r\" \\1 \",text)\n",
    "    tokens = text.split()\n",
    "    return [vocab_dict.get(token,0) for token in tokens]\n",
    "# 긴 문자열을 토큰 ID 리스트로 변환하는 함수. 정규식으로 문장부호나 괄호 제거, ?나 ! 사이에 공백을 넣어 단어와 별도의 토큰으로 분할한 것.\n",
    "# ?, !가 imdb.vocab이 포함되어 있기 때문. 용어집에 없는 토큰은 값을 0으로 할당한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def list_to_tensor(token_idxes, max_len=100, padding=True):\n",
    "    if len(token_idxes) > max_len:\n",
    "        token_idxes = token_idxes[:max_len]\n",
    "    n_tokens = len(token_idxes)\n",
    "    if padding:\n",
    "        token_idxes = token_idxes + [0] * (max_len - len(token_idxes))\n",
    "    return torch.tensor(token_idxes, dtype=torch.int64), n_tokens\n",
    "# id 리스트를 int64 텐서로 변환하는 함수. 변환할 때는 각 문장을 분할한 후 토큰 수를 제한, 그 수에 미치지 못하는 경우에는 id 0 할당한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset 클래스 작성\n",
    "\n",
    "생성자 내에서 텍스트 파일의 경로와 레이블을 모은 튜플 리스트 작성, __getitem__ 내에서 이 파일을 읽어 텐서로 변환한다.\n",
    "\n",
    "텐서는 max_len으로 지정한 길이로 통일. (padding) -> 나중에 처리하기 용이하다. + 0으로 채우기 전 원래 길이와 n_tokens도 이후에 필요하므로 함께 반환한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tqdm'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-ff3c08c6cdec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTensorDataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tqdm'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import (Dataset, DataLoader, TensorDataset)\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install -c conda-forge tqdm\n",
    "!y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
