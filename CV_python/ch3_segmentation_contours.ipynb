{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Segmentation\n",
    "\n",
    "partition images into diff regions.\n",
    "\n",
    "Contours = image segmentation tech. (important).\n",
    "\n",
    "\"continuous lines or curves that bound or cover the full boundary of an object in an image.\"를 말함. 윤곽이나 경계선이라고 이해하면 될 듯. 보통 Object detection이나 shape analysis에서 중요하게 쓰임.\n",
    "\n",
    "\n",
    "<hr>\n",
    "\n",
    "함수가 반환하는 Contour는 list of coordinates(좌표값의 리스트).\n",
    "\n",
    "\n",
    "**cv2.findContours(image, Retrieval Mode, Approximation Method)**\n",
    "\n",
    "Returns -> contours, hierarchy\n",
    "\n",
    "**NOTE** In OpenCV 3.X, findContours returns a 3rd argument which is ret (or a boolean indicating if the function was successfully run). \n",
    "\n",
    "If you're using OpenCV 3.X replace line 12 with:\n",
    "\n",
    "_, contours, hierarchy = cv2.findContours(image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
    "\n",
    "The variable 'contours' are stored as a numpy array of (x,y) points that form the contour\n",
    "\n",
    "While, 'hierarchy' describes the child-parent relationships between contours (i.e. contours within contours)\n",
    "\n",
    "\n",
    "\n",
    "#### Approximation Methods\n",
    "\n",
    "Using cv2.CHAIN_APPROX_NONE stores all the boundary points. But we don't necessarily need all bounding points. If the points form a straight line, we only need the start and ending points of that line.\n",
    "\n",
    "approx none는 윤곽의 모든 값들을 다 리턴한다. 리턴값이 상당히 많은 편. 반면 approx simple의 경우 bounding endpoint만 제공하기에 efficient한 편이다. 이미지가 크면 approx simple도 고려해볼만 하다.\n",
    "\n",
    "Using cv2.CHAIN_APPROX_SIMPLE instead only provides these start and end points of bounding contours, thus resulting in much more efficent storage of contour information..\n",
    "\n",
    "\n",
    "##### Retrieval Mode\n",
    "\n",
    "contour의 계층구조를 정의하는 걸 말한다. subcontours가 필요한지, externel만 필요한지, 전부 필요한지 등등.\n",
    "\n",
    "타입은 4개.\n",
    "\n",
    "* cv2.RETR_LIST : retrieve all contours\n",
    "* cv2.RETR_EXTERNAL : external / outer contours만 반환. 일반적으로 많이 쓰는 편\n",
    "* cv2.RETR_COMP : 2 level 계층구조로 전부 반환\n",
    "* cv2.RETR_TREE : 모든 contour를 full hierarchy로 반환. 유용하다. \n",
    "\n",
    "계층구조가 반환되는 방식 = ``[next layer, prev layer , first child, parent]`` 형태\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Contours found = 8\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Let's load a simple image with 3 black squares\n",
    "# 하얀 배경에 검은 사각형 세 개\n",
    "image = cv2.imread('./MasteringComputerVision-V1.03/Master OpenCV/images/shapes_donut.jpg')\n",
    "cv2.imshow('Input Image', image)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "# Grayscale. 윤곽석 확인에서 key step 역할을 한다. 아래 findContours함수가 grayscale 이미지만 취급하므로.\n",
    "gray = cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Find Canny edges\n",
    "# 필수적으로 해야 하는 건 아니지만, 노이즈를 상당히 줄여준다고. unnecessary contours 줄이는 역할.\n",
    "edged = cv2.Canny(gray, 30, 200)\n",
    "cv2.imshow('Canny Edges', edged)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "# Finding Contours\n",
    "# Use a copy of your image e.g. edged.copy(), since findContours alters the image\n",
    "# edged로 이미지를 받으면, edged 변수값 자체가 바뀐다. 만약 원본을 그대로 두고 싶으면 edged.copy() 함수를 써야 함\n",
    "contours, hierarchy = cv2.findContours(edged, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)\n",
    "cv2.imshow('Canny Edges After Contouring', edged)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Number of Contours found = \" + str(len(contours)))\n",
    "\n",
    "# Draw all contours\n",
    "# Use '-1' as the 3rd parameter to draw all. 모든 contours를 다 그리게 할 거면 -1.\n",
    "# 세 자리 튜플은 rgb값을 말함. (b,g,r). 마지막 변수는 link thickness.\n",
    "cv2.drawContours(image, contours, -1, (0,255,0), 3)\n",
    "\n",
    "cv2.imshow('Contours', image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sorting Contours\n",
    "\n",
    "applying object recognition이나 ML classification 알고리즘에서 많이 적용되는 방법.\n",
    "\n",
    "smallest Contours를 노이즈로 인식하거나, Largest Contour를 가장 중요한 파트로 인식한다거나 할 때. 손글씨 같은 걸 갖고 글자정렬을 해야 할 때 왼쪽 -> 오른쪽으로 정렬을 시킨다던가 하는 것들.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of contours found =  4\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load our image\n",
    "image = cv2.imread('./MasteringComputerVision-V1.03/Master OpenCV/images/bunchofshapes.jpg')\n",
    "cv2.imshow('0 - Original Image', image)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "# Create a black image with same dimensions as our loaded image\n",
    "blank_image = np.zeros((image.shape[0], image.shape[1], 3))\n",
    "\n",
    "# Create a copy of our original image\n",
    "orginal_image = image\n",
    "\n",
    "# Grayscale our image\n",
    "gray = cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Find Canny edges\n",
    "edged = cv2.Canny(gray, 50, 200)\n",
    "cv2.imshow('1 - Canny Edges', edged)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "# Find contours and print how many were found\n",
    "contours, hierarchy = cv2.findContours(edged.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
    "print (\"Number of contours found = \", len(contours))\n",
    "\n",
    "#Draw all contours\n",
    "cv2.drawContours(blank_image, contours, -1, (0,255,0), 3)\n",
    "cv2.imshow('2 - All Contours over blank image', blank_image)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "# Draw all contours over blank image\n",
    "cv2.drawContours(image, contours, -1, (0,255,0), 3)\n",
    "cv2.imshow('3 - All Contours', image)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contor Areas before sorting [20587.5, 22900.5, 66581.5, 90222.0]\n",
      "Contor Areas after sorting [90222.0, 66581.5, 22900.5, 20587.5]\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Function we'll use to display contour area\n",
    "\n",
    "def get_contour_areas(contours):\n",
    "    # returns the areas of all contours as list\n",
    "    all_areas = []\n",
    "    for cnt in contours:\n",
    "        area = cv2.contourArea(cnt) # 면적을 계산하는 함수\n",
    "        all_areas.append(area)\n",
    "    return all_areas\n",
    "\n",
    "# Load our image\n",
    "image = cv2.imread('./MasteringComputerVision-V1.03/Master OpenCV/images/bunchofshapes.jpg')\n",
    "orginal_image = image\n",
    "\n",
    "# Let's print the areas of the contours before sorting\n",
    "print(\"Contor Areas before sorting\", get_contour_areas(contours))\n",
    "\n",
    "# Sort contours large to small\n",
    "sorted_contours = sorted(contours, key=cv2.contourArea, reverse=True)\n",
    "#sorted_contours = sorted(contours, key=cv2.contourArea, reverse=True)[:3]\n",
    "\n",
    "print(\"Contor Areas after sorting\", get_contour_areas(sorted_contours))\n",
    "\n",
    "# Iterate over our contours and draw one at a time\n",
    "for c in sorted_contours:\n",
    "    cv2.drawContours(orginal_image, [c], -1, (255,0,0), 3)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.imshow('Contours by area', orginal_image)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'m00': 20587.5, 'm10': 18250924.833333332, 'm01': 9099782.333333332, 'm20': 16214606883.25, 'm11': 8067011518.125, 'm02': 4057711167.4166665, 'm30': 14436508422455.451, 'm21': 7166954589089.566, 'm12': 3597188928206.5664, 'm03': 1824964836980.2002, 'mu20': 35067975.07837677, 'mu11': 7590.238600730896, 'mu02': 35559934.190416336, 'mu30': -11432.6015625, 'mu21': 353952.1429042816, 'mu12': 359855.0387878418, 'mu03': -6583.537353515625, 'nu20': 0.08273770399700484, 'nu11': 1.7908046107890125e-05, 'nu02': 0.0838984088081491, 'nu30': -1.8799055832427544e-07, 'nu21': 5.820167929485615e-06, 'nu12': 5.9172314619470645e-06, 'nu03': -1.0825557560719243e-07}\n",
      "{'m00': 22900.5, 'm10': 13584963.333333332, 'm01': 10867154.833333332, 'm20': 8110772483.25, 'm11': 6445051111.708333, 'm02': 5205929715.75, 'm30': 4872947003709.101, 'm21': 3848412179322.7666, 'm12': 3086834689542.3667, 'm03': 2515682881991.0503, 'mu20': 51942817.172356606, 'mu11': -1528655.0430221558, 'mu02': 49053046.173353195, 'mu30': -126917095.82421875, 'mu21': 1357008125.645172, 'mu12': 40076224.0719223, 'mu03': -1283051858.9174805, 'nu20': 0.09904568753713808, 'nu11': -0.0029148725076047623, 'nu02': 0.09353540967770965, 'nu30': -0.0015992181651651063, 'nu21': 0.01709897339452276, 'nu12': 0.0005049802401388959, 'nu03': -0.01616708786396722}\n",
      "{'m00': 66581.5, 'm10': 85031433.66666666, 'm01': 26846617.666666664, 'm20': 108946563007.41666, 'm11': 34285897766.625, 'm02': 11177836618.416666, 'm30': 140036770320690.81, 'm21': 43928817353149.18, 'm12': 14275238215140.516, 'm03': 4791655567853.8, 'mu20': 352663629.83081055, 'mu11': 1694.9304962158203, 'mu02': 352894552.8251934, 'mu30': 11856707.90625, 'mu21': -6890856.511627197, 'mu12': -11876281.82006836, 'mu03': 6892053.5703125, 'nu20': 0.07955244337466987, 'nu11': 3.8233560514560937e-07, 'nu02': 0.07960453405508214, 'nu30': 1.0365259595666372e-05, 'nu21': -6.0240597258749655e-06, 'nu12': -1.0382371318383564e-05, 'nu03': 6.025106207252639e-06}\n",
      "{'m00': 90222.0, 'm10': 25262160.0, 'm01': 36043689.0, 'm20': 7714377697.0, 'm11': 10092232920.0, 'm02': 15117233610.0, 'm30': 2518970577480.0, 'm21': 3081893889951.5, 'm12': 4232825410800.0, 'm03': 6612840930940.5, 'mu20': 640972897.0, 'mu11': 0.0, 'mu02': 717779854.5, 'mu30': 0.0, 'mu21': 0.0, 'mu12': 0.0, 'mu03': 0.0, 'nu20': 0.07874350932988444, 'nu11': 0.0, 'nu02': 0.08817924273266713, 'nu30': 0.0, 'nu21': 0.0, 'nu12': 0.0, 'nu03': 0.0}\n",
      "output_shape_number_1.jpg\n",
      "output_shape_number_2.jpg\n",
      "output_shape_number_3.jpg\n",
      "output_shape_number_4.jpg\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Functions we'll use for sorting by position\n",
    "\n",
    "def x_cord_contour(contours):\n",
    "    #Returns the X cordinate for the contour centroid\n",
    "    if cv2.contourArea(contours) > 10:\n",
    "        M = cv2.moments(contours) # contour의 중심을 반환하는 함수.\n",
    "        return (int(M['m10']/M['m00']))\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    \n",
    "def label_contour_center(image, c):\n",
    "    # Places a red circle on the centers of contours\n",
    "    M = cv2.moments(c)\n",
    "    print(M)\n",
    "    cx = int(M['m10'] / M['m00'])\n",
    "    cy = int(M['m01'] / M['m00'])\n",
    " \n",
    "    # Draw the countour number on the image\n",
    "    cv2.circle(image,(cx,cy), 10, (0,0,255), -1)\n",
    "    return image\n",
    "\n",
    "\n",
    "# Load our image\n",
    "image = cv2.imread('./MasteringComputerVision-V1.03/Master OpenCV/images/bunchofshapes.jpg')\n",
    "orginal_image = image.copy()\n",
    "\n",
    "\n",
    "# Computer Center of Mass or centroids and draw them on our image\n",
    "for (i, c) in enumerate(contours):\n",
    "    orig = label_contour_center(image, c)\n",
    "    \n",
    "cv2.imshow(\"4 - Contour Centers \", image)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "# Sort by left to right using our x_cord_contour function\n",
    "# 왼쪽 상단부터 0 기준으로 출발하니까, 작은 것부터 계산하려면 reverse = False가 맞다.\n",
    "contours_left_to_right = sorted(contours, key = x_cord_contour, reverse = False)\n",
    "\n",
    "\n",
    "# Labeling Contours left to right\n",
    "for (i,c)  in enumerate(contours_left_to_right):\n",
    "    cv2.drawContours(orginal_image, [c], -1, (0,0,255), 3)  \n",
    "    M = cv2.moments(c)\n",
    "    cx = int(M['m10'] / M['m00'])\n",
    "    cy = int(M['m01'] / M['m00'])\n",
    "    cv2.putText(orginal_image, str(i+1), (cx, cy), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "    cv2.imshow('6 - Left to Right Contour', orginal_image)\n",
    "    cv2.waitKey(0)\n",
    "\n",
    "    # rectangle overlay of contour를 제공하는 함수\n",
    "    (x, y, w, h) = cv2.boundingRect(c)  \n",
    "    \n",
    "    # Let's now crop each contour and save these images\n",
    "    cropped_contour = orginal_image[y:y + h, x:x + w]\n",
    "    image_name = \"output_shape_number_\" + str(i+1) + \".jpg\"\n",
    "    print(image_name)\n",
    "    cv2.imwrite(image_name, cropped_contour)\n",
    "    \n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approximating Contours\n",
    "\n",
    "cv2.approxPolyDP(contour, approximation accuracy, closed)\n",
    "\n",
    "* contour: approximate할 contour\n",
    "* approximate accuracy: 정확도. 값이 작을수록 precise, 클수록 generic. rule of thumb: 5% 내외.\n",
    "* closed: approximate contour가 open / closed 여부. 보통 closed를 쓴다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# Load image and keep a copy\n",
    "image = cv2.imread('./MasteringComputerVision-V1.03/Master OpenCV/images/house.jpg')\n",
    "orig_image = image.copy()\n",
    "cv2.imshow('Original Image', orig_image)\n",
    "cv2.waitKey(0) \n",
    "\n",
    "# Grayscale and binarize\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "ret, thresh = cv2.threshold(gray, 127, 255, cv2.THRESH_BINARY_INV)\n",
    "\n",
    "# Find contours \n",
    "contours, hierarchy = cv2.findContours(thresh.copy(), cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)\n",
    "\n",
    "# Iterate through each contour and compute the bounding rectangle\n",
    "for c in contours:\n",
    "    x,y,w,h = cv2.boundingRect(c)\n",
    "    cv2.rectangle(orig_image,(x,y),(x+w,y+h),(0,0,255),2)    \n",
    "    cv2.imshow('Bounding Rectangle', orig_image)\n",
    "\n",
    "cv2.waitKey(0) \n",
    "    \n",
    "# Iterate through each contour and compute the approx contour\n",
    "for c in contours:\n",
    "    # Calculate accuracy as a percent of the contour perimeter\n",
    "    accuracy = 0.03 * cv2.arcLength(c, True)\n",
    "    approx = cv2.approxPolyDP(c, accuracy, True)\n",
    "    cv2.drawContours(image, [approx], 0, (0, 255, 0), 2)\n",
    "    cv2.imshow('Approx Poly DP', image)\n",
    "    # 손으로 막 그린 그림이라 해도, 꽤 비슷하게 contour가 만들어진다.\n",
    "    \n",
    "cv2.waitKey(0)   \n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convex Hull\n",
    "\n",
    "outer edge를 갖고 drawing Line을 한 결과를 말함. 보통 smallest polygon that can fit the object itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "image = cv2.imread('./MasteringComputerVision-V1.03/ingComputerVision-V1.03/steringComputerVision-V1.03/steringComputerVision-V1.03/steringComputerVision-V1.03/steringComputerVision-V1.03/Master OpenCV/images/hand.jpg')\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "cv2.imshow('Original Image', image)\n",
    "cv2.waitKey(0) \n",
    "\n",
    "# Threshold the image\n",
    "ret, thresh = cv2.threshold(gray, 176, 255, 0)\n",
    "\n",
    "# Find contours \n",
    "contours, hierarchy = cv2.findContours(thresh.copy(), cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)\n",
    "    \n",
    "# Sort Contors by area and then remove the largest frame contour\n",
    "# 이걸 쓰는 이유? 배경이 하얀 이미지를 갖고 작업하면, 이미지 전체(배경 포함한 사진 전체의 외곽선)가 하나의 contour로 잡힌다. 그래서 그걸 빼주는 개념\n",
    "# 배경이 까맣다면 이 문제는 생기지 않는다.\n",
    "n = len(contours) - 1\n",
    "contours = sorted(contours, key=cv2.contourArea, reverse=False)[:n]\n",
    "\n",
    "# Iterate through contours and draw the convex hull\n",
    "for c in contours:\n",
    "    hull = cv2.convexHull(c)\n",
    "    cv2.drawContours(image, [hull], 0, (0, 255, 0), 2)\n",
    "    cv2.imshow('Convex Hull', image)\n",
    "\n",
    "cv2.waitKey(0)    \n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shape Matching\n",
    "\n",
    "cv2.matchShapes(contour template, contour, method, method parameter)\n",
    "\n",
    "it returns 'match value' (lower value means a closer match)\n",
    "\n",
    "\n",
    "* contour template: reference contour that we're trying to find in the new image.\n",
    "* contour: individual contour we are checking\n",
    "* method: type of contour matching (1,2,3). 1,2,3 각각 다른 방식의 match 판별 method임. opencv markdown에서 확인 가능하다.\n",
    "* method parameter: 0.0으로 내버려둬라. 이 예시에서는 안 쓰임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.13081816783853514\n",
      "0.1590200533978871\n",
      "0.1498791568252558\n",
      "0.07094034474475601\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load the shape template or reference image\n",
    "template = cv2.imread('./MasteringComputerVision-V1.03/Master OpenCV/images/4star.jpg',0)\n",
    "cv2.imshow('Template', template)\n",
    "cv2.waitKey()\n",
    "\n",
    "# Load the target image with the shapes we're trying to match\n",
    "target = cv2.imread('./MasteringComputerVision-V1.03/Master OpenCV/images/shapestomatch.jpg')\n",
    "target_gray = cv2.cvtColor(target,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Threshold both images first before using cv2.findContours\n",
    "ret, thresh1 = cv2.threshold(template, 127, 255, 0)\n",
    "ret, thresh2 = cv2.threshold(target_gray, 127, 255, 0)\n",
    "\n",
    "# Find contours in template\n",
    "contours, hierarchy = cv2.findContours(thresh1, cv2.RETR_CCOMP, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "# We need to sort the contours by area so that we can remove the largest\n",
    "# contour which is the image outline\n",
    "sorted_contours = sorted(contours, key=cv2.contourArea, reverse=True)\n",
    "\n",
    "# We extract the second largest contour which will be our template contour\n",
    "template_contour = contours[1]\n",
    "\n",
    "# Extract contours from second target image\n",
    "contours, hierarchy = cv2.findContours(thresh2, cv2.RETR_CCOMP, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "for c in contours:\n",
    "    # Iterate through each contour in the target image and \n",
    "    # use cv2.matchShapes to compare contour shapes\n",
    "    match = cv2.matchShapes(template_contour, c, 3, 0.0)\n",
    "    print(match)\n",
    "    # If the match value is less than 0.15 we\n",
    "    if match < 0.15:\n",
    "        closest_contour = c\n",
    "    else:\n",
    "        closest_contour = [] \n",
    "                \n",
    "cv2.drawContours(target, [closest_contour], -1, (0,255,0), 3)\n",
    "cv2.imshow('Output', target)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini project _ 2 : identifying Contours by Shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# Load and then gray scale image\n",
    "\n",
    "image = cv2.imread('./MasteringComputerVision-V1.03/Master OpenCV/images/someshapes.jpg')\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "cv2.imshow('Identifying Shapes',image)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "ret, thresh = cv2.threshold(gray, 127, 255, 1)\n",
    "\n",
    "# Extract Contours\n",
    "contours, hierarchy = cv2.findContours(thresh.copy(), cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)\n",
    "\n",
    "for cnt in contours:\n",
    "    \n",
    "    # Get approximate polygons\n",
    "    approx = cv2.approxPolyDP(cnt, 0.01*cv2.arcLength(cnt,True),True)\n",
    "    \n",
    "    if len(approx) == 3:\n",
    "        shape_name = \"Triangle\"\n",
    "        cv2.drawContours(image,[cnt],0,(0,255,0),-1)\n",
    "        \n",
    "        # Find contour center to place text at the center\n",
    "        M = cv2.moments(cnt)\n",
    "        cx = int(M['m10'] / M['m00'])\n",
    "        cy = int(M['m01'] / M['m00'])\n",
    "        # putText 뒷부분의 input은 각각 font, ?, color, thickness라고 함.\n",
    "        cv2.putText(image, shape_name, (cx-50, cy), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 0), 1)\n",
    "    \n",
    "    elif len(approx) == 4:\n",
    "        x,y,w,h = cv2.boundingRect(cnt)\n",
    "        M = cv2.moments(cnt)\n",
    "        cx = int(M['m10'] / M['m00'])\n",
    "        cy = int(M['m01'] / M['m00'])\n",
    "        \n",
    "        # Check to see if 4-side polygon is square or rectangle\n",
    "        # cv2.boundingRect returns the top left and then width and \n",
    "        if abs(w-h) <= 3:\n",
    "            shape_name = \"Square\"\n",
    "            \n",
    "            # Find contour center to place text at the center\n",
    "            cv2.drawContours(image, [cnt], 0, (0, 125 ,255), -1)\n",
    "            cv2.putText(image, shape_name, (cx-50, cy), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 0), 1)\n",
    "        else:\n",
    "            shape_name = \"Rectangle\"\n",
    "            \n",
    "            # Find contour center to place text at the center\n",
    "            cv2.drawContours(image, [cnt], 0, (0, 0, 255), -1)\n",
    "            M = cv2.moments(cnt)\n",
    "            cx = int(M['m10'] / M['m00'])\n",
    "            cy = int(M['m01'] / M['m00'])\n",
    "            cv2.putText(image, shape_name, (cx-50, cy), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 0), 1)\n",
    "            \n",
    "    elif len(approx) == 10:\n",
    "        shape_name = \"Star\"\n",
    "        cv2.drawContours(image, [cnt], 0, (255, 255, 0), -1)\n",
    "        M = cv2.moments(cnt)\n",
    "        cx = int(M['m10'] / M['m00'])\n",
    "        cy = int(M['m01'] / M['m00'])\n",
    "        cv2.putText(image, shape_name, (cx-50, cy), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 0), 1)\n",
    "        \n",
    "        \n",
    "        \n",
    "    elif len(approx) >= 15:\n",
    "        shape_name = \"Circle\"\n",
    "        cv2.drawContours(image, [cnt], 0, (0, 255, 255), -1)\n",
    "        M = cv2.moments(cnt)\n",
    "        cx = int(M['m10'] / M['m00'])\n",
    "        cy = int(M['m01'] / M['m00'])\n",
    "        cv2.putText(image, shape_name, (cx-50, cy), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 0), 1)\n",
    "\n",
    "    cv2.imshow('Identifying Shapes',image)\n",
    "    cv2.waitKey(0)\n",
    "    \n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Line Detection\n",
    "\n",
    "self-driving car의 lead detection이나 chessboard의 line detection 등등.\n",
    "\n",
    "opencv에서 제공하는 함수는 두 개로, ``cv2.HoughLines / cv2.HoughLinesP (probabilistic Hough Lines)``\n",
    "\n",
    "\n",
    "왼쪽 상단을 좌표평면의 기준으로 생각하고, 왼쪽 상단의 점에서부터 직선까지의 거리를 P, 왼쪽 상단의 기준 x선과 P선 사이의 각도를 theta라고 하자. (radian)\n",
    "\n",
    "P = x * cos(theta) + y * sin(theta).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "image = cv2.imread('./MasteringComputerVision-V1.03/Master OpenCV/images/soduku.jpg')\n",
    "# 이미지로 line을 찾다 보면, 어떤 라인은 누락되고 어떤 라인은 여러 번 인식되는 등의 문제가 생길 수 있다.\n",
    "# 하나의 line을 여러 번 인식한 경우는, 인접한 라인 간 평균거리를 바탕으로 한 개의 line으로 통합해 인식하는 등의 방법을 사용한다.\n",
    "\n",
    "\n",
    "# Grayscale and Canny Edges extracted\n",
    "# Canny를 많이 사용한다고 함.\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "edges = cv2.Canny(gray, 100, 170, apertureSize = 3)\n",
    "\n",
    "# Run HoughLines using a rho accuracy of 1 pixel. 더 큰 값으로 바꾸어도 무방함.\n",
    "# theta accuracy of np.pi / 180 which is 1 degree\n",
    "# Our line threshold is set to 240 (number of points on line). line으로 인식할 수 있는 threshold의 기준을 말함.\n",
    "# 이 값이 너무 작으면 detects too much line. 크면 few lines. 255 넘어도 상관없음.\n",
    "lines = cv2.HoughLines(edges, 1, np.pi / 180, 240)\n",
    "\n",
    "# We iterate through each line and convert it to the format\n",
    "# required by cv.lines (i.e. requiring end points)\n",
    "# lines 안에는 rho, theta 값들이 있다.\n",
    "for line in lines:\n",
    "    line = line[0]\n",
    "    rho, theta = line[0], line[1]\n",
    "    a = np.cos(theta)\n",
    "    b = np.sin(theta)\n",
    "    x0 = a * rho\n",
    "    y0 = b * rho\n",
    "    x1 = int(x0 + 1000 * (-b))\n",
    "    y1 = int(y0 + 1000 * (a))\n",
    "    x2 = int(x0 - 1000 * (-b))\n",
    "    y2 = int(y0 - 1000 * (a))\n",
    "    # 두 점을 계산한 다음 cv2.line으로 그려낸 것.\n",
    "#     print(x1, x2, y1, y2)\n",
    "    cv2.line(image, (x1, y1), (x2, y2), (255, 0, 0), 2)\n",
    "\n",
    "cv2.imshow('Hough Lines', image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probabilistic Hough Lines\n",
    "\n",
    "일반적으로는 HoughLines 함수를 더 많이 쓴다고 하지만, 이 함수도 꽤 유용하다/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(158, 1, 4)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Grayscale and Canny Edges extracted\n",
    "image = cv2.imread('./MasteringComputerVision-V1.03/Master OpenCV/images/soduku.jpg')\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "edges = cv2.Canny(gray, 100, 170, apertureSize = 3)\n",
    "\n",
    "# Again we use the same rho and theta accuracies\n",
    "# However, we specific a minimum vote (pts along line) of 100\n",
    "# and Min line length of 5 pixels and max gap between lines of 10 pixels\n",
    "\n",
    "# 여기서 5는 min line length, 10은 min line gap을 말한다.\n",
    "# min line gap은 하나의 라인을 여러 번 인식하는 문제를 해결하는 파라미터.\n",
    "# min line length로 5는 5 pixel을 말함. 작은 크기지만, 처음에 line을 파악하기 위해서는 작은 것부터 출발하는 것도 괜찮다\n",
    "\n",
    "lines = cv2.HoughLinesP(edges, 1, np.pi / 180, 200, 5, 10)\n",
    "# 이 함수는 아예 직선을 만들 수 있는 좌표계의 두 점 자체를 반환하는 것.\n",
    "print(lines.shape)\n",
    "\n",
    "for line in lines:\n",
    "    line = line[0]\n",
    "    x1, y1, x2, y2 = line[0], line[1], line[2], line[3]\n",
    "    cv2.line(image, (x1, y1), (x2, y2),(0, 255, 0), 3)\n",
    "\n",
    "cv2.imshow('Probabilistic Hough Lines', image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Circle Detection - Hough Cirlces\n",
    "\n",
    "**cv2.HoughCircles**(image, method, dp, MinDist, param1, param2, minRadius, MaxRadius)\n",
    "\n",
    "\n",
    "- Method - currently only cv2.HOUGH_GRADIENT available\n",
    "- dp - Inverse ratio of accumulator resolution\n",
    "- MinDist - the minimum distance between the center of detected circles\n",
    "- param1 - Gradient value used in the edge detection\n",
    "- param2 - Accumulator threshold for the HOUGH_GRADIENT method (lower allows more circles to be detected (false positives))\n",
    "- minRadius - limits the smallest circle to this size (via radius)\n",
    "- MaxRadius - similarly sets the limit for the largest circles\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "# import cv2.cv as cv\n",
    "\n",
    "image = cv2.imread('./MasteringComputerVision-V1.03/Master OpenCV/images/bottlecaps.jpg')\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "cv2.imshow('circles', image)\n",
    "cv2.waitKey(0)\n",
    "blur = cv2.medianBlur(gray, 5)\n",
    "# cv2.CV_HOUGH_GRADIENT\n",
    "circles = cv2.HoughCircles(blur, cv2.HOUGH_GRADIENT, 1.9, 50, param1 = 100, param2 = 100, minRadius = 20, maxRadius = 50)\n",
    "# circles = cv2.HoughCircles(gray, cv2.HOUGH_GRADIENT, 1, 10)\n",
    "\n",
    "circles = np.uint16(np.around(circles))\n",
    "\n",
    "for i in circles[0,:]:\n",
    "    # draw the outer circle\n",
    "#     cv2.circle(image,(i[0], i[1]), i[2], (255, 0, 0), 2)\n",
    "    \n",
    "    # draw the center of the circle\n",
    "    cv2.circle(image, (i[0], i[1]), 2, (0, 255, 0), 5)\n",
    "\n",
    "cv2.imshow('detected circles', image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Blob Detection\n",
    "\n",
    "blob: as groups of connected pixels that all share a common property.\n",
    "\n",
    "Blob Detector를 만들기 위한 과정:\n",
    "\n",
    "1. create detector\n",
    "2. input img into detector.\n",
    "3. obtain key points\n",
    "4. draw key points.\n",
    "\n",
    "The function **cv2.drawKeypoints** takes the following arguments:\n",
    "\n",
    "**cv2.drawKeypoints**(input image, keypoints, blank_output_array, color, flags)\n",
    "\n",
    "flags:\n",
    "- cv2.DRAW_MATCHES_FLAGS_DEFAULT\n",
    "- cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS\n",
    "- cv2.DRAW_MATCHES_FLAGS_DRAW_OVER_OUTIMG\n",
    "- cv2.DRAW_MATCHES_FLAGS_NOT_DRAW_SINGLE_POINTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71\n"
     ]
    }
   ],
   "source": [
    "# Standard imports\n",
    "import cv2\n",
    "import numpy as np;\n",
    " \n",
    "# Read image\n",
    "image = cv2.imread(\"./MasteringComputerVision-V1.03/Master OpenCV/images/Sunflowers.jpg\",cv2.IMREAD_GRAYSCALE)\n",
    " \n",
    "# Set up the detector with default parameters.\n",
    "detector = cv2.SimpleBlobDetector_create()\n",
    "\n",
    "# cv2.imshow(\"img\", image)\n",
    "# Detect blobs.\n",
    "keypoints = detector.detect(image)\n",
    "print(len(keypoints))\n",
    "# Draw detected blobs as red circles.\n",
    "# cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS ensures the size of\n",
    "# the circle corresponds to the size of blob\n",
    "blank = np.zeros((1,1)) \n",
    "blobs = cv2.drawKeypoints(image, keypoints, blank, (0,255,255),\n",
    "                                      cv2.DRAW_MATCHES_FLAGS_DEFAULT)\n",
    "# print(blobs)\n",
    "# # Show keypoints\n",
    "cv2.imshow(\"Blobs\", blobs)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini project _ 3: Counting Circle and Ellipses\n",
    "\n",
    "by filtering blobs.\n",
    "\n",
    "\n",
    "여기서 원과 타원을 구분한 핵심: cv2.SimpleBlobDetector_Params() 함수.\n",
    "\n",
    "* Area\n",
    "    - params.filterByArea: boolean값. Area 기준으로 filter할 건지 결정\n",
    "    - params.minArea / params.maxArea : 값 설정. 픽셀값\n",
    "    \n",
    "    True로 설정한 다음 min max 값 지정하면, 해당 area 값 사이에 있는 blob만 가져온다.\n",
    "    \n",
    "    \n",
    "* Circularity\n",
    "    - params.filterByCircularity: bool. \n",
    "    - params.minCircularity: 1이면 prefect circle, 0이면 opposite\n",
    "  \n",
    "* Convexity\n",
    "    - params.filterByConvexity: bool\n",
    "    - params.minConvexity\n",
    "    \n",
    "    오목한 정도를 말함. convex하다는 건 어디 파인 부분 없이 원에 가깝다는 거고, concave하다는 건 어디가 파여 있다는 의미로 이해해도 무방\n",
    "    \n",
    "    \n",
    "* Inertia : 타원의 정도를 결정. low value = more elliptical, high being more circular\n",
    "    - params.filterByInertia : bool\n",
    "    - params.minInertiaRatio : 0.01 등등"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load image\n",
    "image = cv2.imread(\"./MasteringComputerVision-V1.03/Master OpenCV/images/blobs.jpg\", 0)\n",
    "cv2.imshow('Original Image',image)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "# Intialize the detector using the default parameters\n",
    "detector = cv2.SimpleBlobDetector_create()\n",
    " \n",
    "# Detect blobs\n",
    "keypoints = detector.detect(image)\n",
    " \n",
    "# Draw blobs on our image as red circles\n",
    "blank = np.zeros((1,1)) \n",
    "blobs = cv2.drawKeypoints(image, keypoints, blank, (0,0,255),\n",
    "                                      cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "\n",
    "number_of_blobs = len(keypoints)\n",
    "text = \"Total Number of Blobs: \" + str(len(keypoints))\n",
    "cv2.putText(blobs, text, (20, 550), cv2.FONT_HERSHEY_SIMPLEX, 1, (100, 0, 255), 2)\n",
    "\n",
    "# Display image with blob keypoints\n",
    "cv2.imshow(\"Blobs using default parameters\", blobs)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "\n",
    "# Set our filtering parameters\n",
    "# Initialize parameter settiing using cv2.SimpleBlobDetector\n",
    "params = cv2.SimpleBlobDetector_Params()\n",
    "\n",
    "# Set Area filtering parameters\n",
    "params.filterByArea = True\n",
    "params.minArea = 100\n",
    "\n",
    "# Set Circularity filtering parameters\n",
    "params.filterByCircularity = True \n",
    "params.minCircularity = 0.9\n",
    "\n",
    "# Set Convexity filtering parameters\n",
    "params.filterByConvexity = False\n",
    "params.minConvexity = 0.2\n",
    "    \n",
    "# Set inertia filtering parameters\n",
    "params.filterByInertia = True\n",
    "params.minInertiaRatio = 0.01\n",
    "\n",
    "# Create a detector with the parameters\n",
    "detector = cv2.SimpleBlobDetector_create(params)\n",
    "    \n",
    "# Detect blobs\n",
    "keypoints = detector.detect(image)\n",
    "\n",
    "# Draw blobs on our image as red circles\n",
    "blank = np.zeros((1,1)) \n",
    "blobs = cv2.drawKeypoints(image, keypoints, blank, (0,255,0),\n",
    "                                      cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "\n",
    "number_of_blobs = len(keypoints)\n",
    "text = \"Number of Circular Blobs: \" + str(len(keypoints))\n",
    "cv2.putText(blobs, text, (20, 550), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 100, 255), 2)\n",
    "\n",
    "# Show blobs\n",
    "cv2.imshow(\"Filtering Circular Blobs Only\", blobs)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
