{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Object detection using SIFT (Not available in openCV 3.0+)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def sift_detector(new_image, image_template):\n",
    "    # Function that compares input image to template\n",
    "    # It then returns the number of SIFT matches between them\n",
    "    \n",
    "    image1 = cv2.cvtColor(new_image, cv2.COLOR_BGR2GRAY)\n",
    "    image2 = image_template\n",
    "    \n",
    "    # Create SIFT detector object\n",
    "    sift = cv2.SIFT()\n",
    "\n",
    "    # Obtain the keypoints and descriptors using SIFT\n",
    "    # descriptor는, 이전에 언급했듯, vectors that store information about the key points. match의 대상이 되는 애들임.\n",
    "    keypoints_1, descriptors_1 = sift.detectAndCompute(image1, None)\n",
    "    keypoints_2, descriptors_2 = sift.detectAndCompute(image2, None)\n",
    "\n",
    "    ### Flann_Matcher 사용.\n",
    "    # Define parameters for our Flann Matcher\n",
    "    FLANN_INDEX_KDTREE = 0 # 초기값 0으로 설정\n",
    "    index_params = dict(algorithm = FLANN_INDEX_KDTREE, trees = 3) # tree가 많을수록 complicated, slower\n",
    "    search_params = dict(checks = 100) # how many matches are going to try to compute\n",
    "\n",
    "    # Create the Flann Matcher object\n",
    "    flann = cv2.FlannBasedMatcher(index_params, search_params)\n",
    "\n",
    "    # Obtain matches using K-Nearest Neighbor Method\n",
    "    # the result 'matchs' is the number of similar matches found in both images\n",
    "    # descriptor 1과 2가 가장 가까운 것들. (similar matches found in both img.)\n",
    "    matches = flann.knnMatch(descriptors_1, descriptors_2, k=2)\n",
    "\n",
    "    # Store good matches using Lowe's ratio test\n",
    "    # flannbasedmatcher는 정확도가 높지 않은 대신 속도가 빠르다. 정확성을 높이기 위해 사용하는 게 lowe's ratio test.\n",
    "    # distance는 numpy func이고, m.distance가 기준을 통과하면 good_match에 append한다.\n",
    "    good_matches = []\n",
    "    for m,n in matches:\n",
    "        if m.distance < 0.7 * n.distance:\n",
    "            good_matches.append(m) \n",
    "\n",
    "    # 결과적으로 중요한 건 'how many matches are in'이기 때문에 len() 결과값을 리턴한다.\n",
    "    return len(good_matches)\n",
    "\n",
    "# 1. 웹캠 Stream을 실행한다.\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Load our image template, this is our reference image\n",
    "# 2. template를 불러온다. \n",
    "image_template = cv2.imread('images/box_in_scene.png', 0) \n",
    "\n",
    "while True:\n",
    "\n",
    "    # 3. Get webcam images\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # 4. Get height and width of webcam frame\n",
    "    height, width = frame.shape[:2]\n",
    "\n",
    "    # 4.1 Define ROI Box Dimensions. \n",
    "    top_left_x = width / 3\n",
    "    top_left_y = (height / 2) + (height / 4)\n",
    "    bottom_right_x = (width / 3) * 2\n",
    "    bottom_right_y = (height / 2) - (height / 4)\n",
    "    \n",
    "    # 4.2 Draw rectangular window for our region of interest   \n",
    "    cv2.rectangle(frame, (top_left_x,top_left_y), (bottom_right_x,bottom_right_y), 255, 3)\n",
    "    \n",
    "    # 5. Crop window of observation we defined above. 이 위치를 가져와서 template과 비교할 예정.\n",
    "    cropped = frame[bottom_right_y:top_left_y , top_left_x:bottom_right_x]\n",
    "    \n",
    "    # 6. Flip frame orientation horizontally. 거울모드로 돌려놓은 거라고 보면 된다. 이게 more natural하다고.\n",
    "    frame = cv2.flip(frame,1)\n",
    "    \n",
    "    # 7. Get number of SIFT matches. cropped 이미지와 image template를 받아서 match 작업을 진행함.\n",
    "    matches = sift_detector(cropped, image_template)\n",
    "\n",
    "    # 8. Display status string showing the current no. of matches. 얼마나 많은 match / key points가 detected되는지.\n",
    "    # image와 target image 사이의.\n",
    "    cv2.putText(frame,str(matches),(450,450), cv2.FONT_HERSHEY_COMPLEX, 2,(0,255,0),1)\n",
    "    \n",
    "    # 9. Our threshold to indicate object deteciton\n",
    "    # We use 10 since the SIFT detector returns little false positves. \n",
    "    # 10으로 설정했다는 건, 10개 이상 match가 감지될 시 'object detected'로 인식하는 것.\n",
    "    # 그게 바로 아래 if문이다.\n",
    "    threshold = 10\n",
    "    \n",
    "    # If matches exceed our threshold then object has been detected\n",
    "    if matches > threshold:\n",
    "        cv2.rectangle(frame, (top_left_x,top_left_y), (bottom_right_x,bottom_right_y), (0,255,0), 3)\n",
    "        cv2.putText(frame,'Object Found',(50,50), cv2.FONT_HERSHEY_COMPLEX, 2 ,(0,255,0), 2)\n",
    "    \n",
    "    cv2.imshow('Object Detector using SIFT', frame)\n",
    "    if cv2.waitKey(1) == 13: #13 is the Enter Key\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Flannbased matching is quite fast, but not the most accurate. Other matching methods include:\n",
    "\n",
    "BruteForce... Flannbased보다는 accurate하다고.\n",
    "\n",
    "- BruteForce\n",
    "- BruteForce-SL2 (not in the documentation, BUT this is the one that skeeps the squared root !)\n",
    "- BruteForce-L1\n",
    "- BruteForce-Hamming\n",
    "- BruteForce-Hamming(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ORB 사용하기.\n",
    "\n",
    "그런데, ORB를 써서 결과를 내보면 위의 SIFT보다는 결과가 좋지 않다. 잘못된 이미지를 detected표시할 수 있음.\n",
    "\n",
    "& 단순 이미지를 떠나서, 만약 rotation of image같은 걸 detect하고 싶다면 CNN 형태의 solution이 필요할 수 있다. ORB 대신.\n",
    "\n",
    "https://www.cs.toronto.edu/~guerzhoy/oriviz/crv17.pdf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def ORB_detector(new_image, image_template):\n",
    "    # Function that compares input image to template\n",
    "    # It then returns the number of ORB matches between them\n",
    "    \n",
    "    image1 = cv2.cvtColor(new_image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Create ORB detector with 1000 keypoints with a scaling pyramid factor of 1.2\n",
    "    orb = cv2.ORB_create(1000)\n",
    "\n",
    "    # Detect keypoints of original image\n",
    "    # None는 image mask를 쓸지 말지 결정하는 파라미터 부분. 안 쓸 거라 None.\n",
    "    (kp1, des1) = orb.detectAndCompute(image1, None)\n",
    "\n",
    "    # Detect keypoints of template image\n",
    "    (kp2, des2) = orb.detectAndCompute(image_template, None)\n",
    "\n",
    "    # Create matcher \n",
    "    # Note we're no longer using Flannbased matching\n",
    "    # Norm_hamming은 distance matric이며, crossCheck은 True로 설정.\n",
    "    bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "\n",
    "    # Do matching\n",
    "    matches = bf.match(des1,des2)\n",
    "\n",
    "    # Sort the matches based on distance.  Least distance\n",
    "    # is better\n",
    "    # 정렬할 수 있음을 보여주려고 sorted를 쓴 거고, 실제로는 large distance를 버리면 성능이 올라간다고 함.\n",
    "    matches = sorted(matches, key=lambda val: val.distance)\n",
    "\n",
    "    return len(matches)\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Load our image template, this is our reference image\n",
    "image_template = cv2.imread('./MasteringComputerVision-V1.03/Master OpenCV/images/box_in_scene.png', 0) \n",
    "# image_template = cv2.imread('images/kitkat.jpg', 0) \n",
    "\n",
    "while True:\n",
    "\n",
    "    # Get webcam images\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    # Get height and width of webcam frame\n",
    "    height, width = frame.shape[:2]\n",
    "\n",
    "    # Define ROI Box Dimensions (Note some of these things should be outside the loop)\n",
    "    top_left_x = int(width / 3)\n",
    "    top_left_y = int((height / 2) + (height / 4))\n",
    "    bottom_right_x = int((width / 3) * 2)\n",
    "    bottom_right_y = int((height / 2) - (height / 4))\n",
    "    \n",
    "    top = (top_left_x, top_left_y)\n",
    "    bottom = (bottom_right_x, bottom_right_y)\n",
    "    # Draw rectangular window for our region of interest\n",
    "    cv2.rectangle(frame, top, bottom, 255, 3)\n",
    "    \n",
    "    # Crop window of observation we defined above\n",
    "    cropped = frame[bottom_right_y:top_left_y , top_left_x:bottom_right_x]\n",
    "\n",
    "    # Flip frame orientation horizontally\n",
    "    frame = cv2.flip(frame,1)\n",
    "    \n",
    "    # Get number of ORB matches \n",
    "    matches = ORB_detector(cropped, image_template)\n",
    "    \n",
    "    # Display status string showing the current no. of matches \n",
    "    output_string = \"Matches = \" + str(matches)\n",
    "    cv2.putText(frame, output_string, (50,450), cv2.FONT_HERSHEY_COMPLEX, 2, (250,0,150), 2)\n",
    "    \n",
    "    # Our threshold to indicate object deteciton\n",
    "    # For new images or lightening conditions you may need to experiment a bit \n",
    "    # Note: The ORB detector to get the top 1000 matches, 350 is essentially a min 35% match\n",
    "    threshold = 350\n",
    "    \n",
    "    # If matches exceed our threshold then object has been detected\n",
    "    if matches > threshold:\n",
    "        cv2.rectangle(frame, (top_left_x,top_left_y), (bottom_right_x,bottom_right_y), (0,255,0), 3)\n",
    "        cv2.putText(frame,'Object Found',(50,50), cv2.FONT_HERSHEY_COMPLEX, 2 ,(0,255,0), 2)\n",
    "    \n",
    "    cv2.imshow('Object Detector using ORB', frame)\n",
    "    \n",
    "    if cv2.waitKey(1) == 13: #13 is the Enter Key\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
