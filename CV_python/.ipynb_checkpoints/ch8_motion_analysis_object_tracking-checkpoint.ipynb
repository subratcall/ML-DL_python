{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motion Analysis & Object Tracking\n",
    "\n",
    "여러 가지 방법이 있다.\n",
    "* filtering colors\n",
    "* background substraction\n",
    "\n",
    "Object tracking 알고리즘\n",
    "\n",
    "* Meanshift\n",
    "* Camshift\n",
    "* Optical Flow\n",
    "\n",
    "1. Color filtering.\n",
    "\n",
    "Object tracking을 할 때 꽤나 유용한 요소. 추적할 대상의 색깔을 알면, easily separated from the rest of the scene. \n",
    "\n",
    "Color 대상으로는 HSV를 선호한다. color가 hue 형태로 저장되고, cylindrical representation을 띠고 있음. filtering by color하기가 쉽다. (range로 구분하기 쉽다는 것) \n",
    "\n",
    "Color Range Filter\n",
    "- red : 165 to 15\n",
    "- green : 45 to 75\n",
    "- blue : 90 to 120\n",
    "\n",
    "즉 upper / lower bound 설정을 해놓으면 이 범위에 있는 색깔 탐지가 가능하다는 것\n",
    "\n",
    "saturation은 0~60. 값이 작아질수록 whiter.\n",
    "\n",
    "brightness (value) 도 0~60. 값이 작아질수록 darker.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Initialize webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# define range of PURPLE color in HSV. 여기 값들은 HSV를 말함. color 범주는 125~175, saturation과 value는 full range.\n",
    "lower_purple = np.array([125,0,0])\n",
    "upper_purple = np.array([175,255,255])\n",
    "\n",
    "# loop until break statement is exectured\n",
    "while True:\n",
    "    \n",
    "    # Read webcam image\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    # Convert image from RBG/BGR to HSV so we easily filter\n",
    "    hsv_img = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "\n",
    "    # Use inRange to capture only the values between lower & upper_blue\n",
    "    mask = cv2.inRange(hsv_img, lower_purple, upper_purple)\n",
    "    # black / white로 binarized된 이미지 반환.\n",
    "\n",
    "    # Perform Bitwise AND on mask and our original frame\n",
    "    # original image와 mask 이미지를 bitwise_and 함수롤 써서 작업한다고 함. 이 result가 'color 기반 추출'이미지가 됨.\n",
    "    res = cv2.bitwise_and(frame, frame, mask=mask)\n",
    "\n",
    "    cv2.imshow('Original', frame)  \n",
    "    cv2.imshow('mask', mask)\n",
    "    cv2.imshow('Filtered Color Only', res)\n",
    "    if cv2.waitKey(1) == 13: #13 is the Enter Key\n",
    "        break\n",
    "        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background substraction\n",
    "\n",
    "useful CV technique. separate foreground / background.\n",
    "\n",
    "비디오 프레임을 받아서 foreground와 background를 구분하도록 Learn. 기준은 detecting movement in a scene.\n",
    "\n",
    "OpenCV의 backgroudn substraction Algorithm은 3개.\n",
    "\n",
    "- BackgroundSubstractorMOG\n",
    "- BackgroundSubstractorMOG2 : can see the shadows.\n",
    "- Geometric Multigrid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Gaussian Mixture-based Background/Foreground Segmentation Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# background 이미지의 제거\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "cap = cv2.VideoCapture('./MasteringComputerVision-V1.03/Master OpenCV/images/walking.avi')\n",
    "\n",
    "# Initlaize background subtractor\n",
    "foreground_background = cv2.bgsegm.createBackgroundSubtractorMOG()\n",
    "\n",
    "while True:\n",
    "    \n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # Apply background subtractor to get our foreground mask\n",
    "    foreground_mask = foreground_background.apply(frame)\n",
    "\n",
    "    cv2.imshow('Output', foreground_mask)\n",
    "    if cv2.waitKey(1) == 13: \n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 움직임을 감지하지 않으면 배경처럼 인식한다.\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# Intialize Webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Initlaize background subtractor\n",
    "foreground_background = cv2.bgsegm.createBackgroundSubtractorMOG()\n",
    "\n",
    "while True:\n",
    "    \n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # Apply background subtractor to get our foreground mask\n",
    "    foreground_mask = foreground_background.apply(frame)\n",
    "\n",
    "    cv2.imshow('Output', foreground_mask)\n",
    "    if cv2.waitKey(1) == 13: \n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's try the Improved adaptive Gausian mixture model for background subtraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shadow도 잡아내는 걸 확인할 수 있다.\n",
    "# 상용화하고 싶다면 C++ 레벨까지는 들여다봐야 한다.\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "cap = cv2.VideoCapture('./MasteringComputerVision-V1.03/Master OpenCV/images/walking.avi')\n",
    "\n",
    "# Initlaize background subtractor\n",
    "foreground_background = cv2.createBackgroundSubtractorMOG2()\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # Apply background subtractor to get our foreground mask\n",
    "    foreground_mask = foreground_background.apply(frame)\n",
    "\n",
    "    cv2.imshow('Output', foreground_mask)\n",
    "    if cv2.waitKey(1) == 13: \n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenCV 2.4.13\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# Intialize Webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Initlaize background subtractor\n",
    "foreground_background = cv2.createBackgroundSubtractorMOG2()\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    # Apply background subtractor to get our foreground mask\n",
    "    foreground_mask = foreground_background.apply(frame)\n",
    "\n",
    "    cv2.imshow('Output', foreground_mask)\n",
    "    if cv2.waitKey(1) == 13: \n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Foreground substraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Initalize webacam and store first frame\n",
    "cap = cv2.VideoCapture(0)\n",
    "ret, frame = cap.read()\n",
    "\n",
    "# Create a flaot numpy array with frame values\n",
    "average = np.float32(frame)\n",
    "\n",
    "while True:\n",
    "    # Get webcam frmae\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    # 0.01 is the weight of image, play around to see how it changes\n",
    "    # 이 알고리즘: looks the movement in the image, 움직이지 않는 것에 weight 높게 준다.\n",
    "    cv2.accumulateWeighted(frame, average, 0.01)\n",
    "    \n",
    "    # Scales, calculates absolute values, and converts the result to 8-bit\n",
    "    # 이미지 형태로 반환하기 위해서 scale Abs 함수를 써줘야 함\n",
    "    background = cv2.convertScaleAbs(average)\n",
    "\n",
    "    cv2.imshow('Input', frame)\n",
    "    cv2.imshow('Disapearing Background', background)\n",
    "    \n",
    "    if cv2.waitKey(1) == 13: #13 is the Enter Key\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Background Substraction KKN\n",
    "#### OpenCV 3.X only!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenCV 3.1.0\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(3,3))\n",
    "fgbg = cv2.createBackgroundSubtractorKNN()\n",
    "\n",
    "while(1):\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    fgmask = fgbg.apply(frame)\n",
    "    fgmask = cv2.morphologyEx(fgmask, cv2.MORPH_OPEN, kernel)\n",
    "\n",
    "    cv2.imshow('frame',fgmask)\n",
    "    \n",
    "    if cv2.waitKey(1) == 13: \n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MeanShift for Object Tracking\n",
    "\n",
    "Premise is simple.\n",
    "\n",
    "it tracks objects by finding the maximun density of a discrete sample of points -> recalculates it at the next frame. This effectively moves our observation window in the direction the object has moved.\n",
    "\n",
    "<hr>\n",
    "\n",
    "**cv2.calcHist** is simply a function calculates the color histograms for an array of images.\n",
    "\n",
    "**calcBackProject** is a somewhat more complicated function, that calculates the histogram back projection.\n",
    "\n",
    "In this case, the histogram back projection gives a probability estimate an image is equal to the image the original histogram was generated. \n",
    "\n",
    "Confused yet?\n",
    "\n",
    "calcBackProject takes the histogram generated by calcHist and projects it back onto an image. The result is the probability that each pixel belongs to the image that originally generated the histogram.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# Initialize webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# take first frame of the video\n",
    "ret, frame = cap.read()\n",
    "print(type(frame))\n",
    "\n",
    "# setup default location of window\n",
    "r, h, c, w = 240, 100, 400, 160 \n",
    "track_window = (c, r, w, h)\n",
    "\n",
    "# Crop region of interest for tracking\n",
    "roi = frame[r:r+h, c:c+w]\n",
    "\n",
    "# Convert cropped window to HSV color space\n",
    "hsv_roi =  cv2.cvtColor(roi, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "# Create a mask between the HSV bounds\n",
    "lower_purple = np.array([125,0,0])\n",
    "upper_purple = np.array([175,255,255])\n",
    "mask = cv2.inRange(hsv_roi, lower_purple, upper_purple)\n",
    "\n",
    "# Obtain the color histogram of the ROI\n",
    "roi_hist = cv2.calcHist([hsv_roi], [0], mask, [180], [0,180])\n",
    "\n",
    "# Normalize values to lie between the range 0, 255\n",
    "cv2.normalize(roi_hist, roi_hist, 0, 255, cv2.NORM_MINMAX)\n",
    "\n",
    "# Setup the termination criteria\n",
    "# We stop calculating the centroid shift after ten iterations \n",
    "# or if the centroid has moved at least 1 pixel\n",
    "term_crit = ( cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 1 )\n",
    "\n",
    "while True:\n",
    "    \n",
    "    # Read webcam frame\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if ret == True:\n",
    "        \n",
    "        # Convert to HSV\n",
    "        hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "        \n",
    "        # Calculate the histogram back projection \n",
    "        # Each pixel's value is it's probability\n",
    "        # pixel의 probability value를 말한다.\n",
    "        dst = cv2.calcBackProject([hsv],[0],roi_hist,[0,180],1)\n",
    "\n",
    "        # apply meanshift to get the new location\n",
    "        # meanshift의 작동을 위해 사용하는 value라고 이해하자..\n",
    "        ret, track_window = cv2.meanShift(dst, track_window, term_crit)\n",
    "\n",
    "        # Draw it on image\n",
    "        x, y, w, h = track_window\n",
    "        img2 = cv2.rectangle(frame, (x,y), (x+w, y+h), 255, 2)    \n",
    "\n",
    "        cv2.imshow('Meansift Tracking', img2)\n",
    "        \n",
    "        if cv2.waitKey(1) == 13: #13 is the Enter Key\n",
    "            break\n",
    "\n",
    "    else:\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Camshift Object Tracking\n",
    "\n",
    "meanshift와 거의 유사한 알고리즘이지만, meanshift의 단점인 'fixed size of tracking window' 보완.\n",
    "\n",
    "Continously adaptive Meanshift라는 의미이며, adaptive window를 제공한다.\n",
    "\n",
    "1. apply MeanShift till it converges\n",
    "2. calculate the size of window\n",
    "3. calculate the orientation by using the best fitting elipse\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 작동하지 않는다.\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# Initialize webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# take first frame of the video\n",
    "ret, frame = cap.read()\n",
    "\n",
    "# setup default location of window\n",
    "r, h, c, w = 240, 100, 400, 160 \n",
    "track_window = (c, r, w, h)\n",
    "\n",
    "# Crop region of interest for tracking\n",
    "roi = frame[r:r+h, c:c+w]\n",
    "\n",
    "# Convert cropped window to HSV color space\n",
    "hsv_roi =  cv2.cvtColor(roi, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "# Create a mask between the HSV bounds\n",
    "lower_purple = np.array([130,60,60])\n",
    "upper_purple = np.array([175,255,255])\n",
    "mask = cv2.inRange(hsv_roi, lower_purple, upper_purple)\n",
    "\n",
    "# Obtain the color histogram of the ROI\n",
    "roi_hist = cv2.calcHist([hsv_roi], [0], mask, [180], [0,180])\n",
    "\n",
    "# Normalize values to lie between the range 0, 255\n",
    "cv2.normalize(roi_hist, roi_hist, 0, 255, cv2.NORM_MINMAX)\n",
    "\n",
    "# Setup the termination criteria\n",
    "# We stop calculating the centroid shift after ten iterations \n",
    "# or if the centroid has moved at least 1 pixel\n",
    "term_crit = ( cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 1 )\n",
    "\n",
    "while True:\n",
    "    \n",
    "    # Read webcam frame\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if ret == True:\n",
    "        # Convert to HSV\n",
    "        hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "        \n",
    "        # Calculate the histogram back projection \n",
    "        # Each pixel's value is it's probability\n",
    "        dst = cv2.calcBackProject([hsv],[0],roi_hist,[0,180],1)\n",
    "\n",
    "        # apply Camshift to get the new location\n",
    "        ret, track_window = cv2.CamShift(dst, track_window, term_crit)\n",
    "\n",
    "        # Draw it on image \n",
    "        # We use polylines to represent Adaptive box \n",
    "        pts = cv2.boxPoints(ret)\n",
    "        pts = np.int0(pts)\n",
    "        img2 = cv2.polylines(frame,[pts],True, 255,2)\n",
    "        \n",
    "        cv2.imshow('Camshift Tracking', img2)\n",
    "        \n",
    "        if cv2.waitKey(1) == 13: #13 is the Enter Key\n",
    "            break\n",
    "\n",
    "    else:\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When / How to use MeanShift or CamShift?\n",
    "\n",
    "카메라에 나타나는 이미지 크기가 거의 고정되어 있으면 MeanShift. 크기 변화가 있으면 CamShift.\n",
    "\n",
    "+ starting Location에 유의하길. local minima에 stuck될 가능성은 항상 있다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optical Flow\n",
    "\n",
    "used to show the pattern of apparent motion of objects in an image btwn 2 consecutive frames.\n",
    "\n",
    "shows the distribution of the apparent velocities of objects in an image.\n",
    "\n",
    "cf. 카메라 구도 특성상, 카메라에 가까이 있는 물체가 이동하는 건 멀리 있는 물체가 이동하는 것보다 '빠른 움직임'으로 인식될 수 있다. \n",
    "\n",
    "Optical Flow의 assumption: objects maintain consistent intensity, which means that a neighboring pixels exhibit similar motions.\n",
    "\n",
    "openCV에서 제공하는 Optical Flow\n",
    "\n",
    "1. Lucas-kanade differential method. top-down perspective에서 드론으로 자동차의 움직임을 추적하거나 할 때유용한 방식. Track some keypoints in the video, good for corner-like features.\n",
    "\n",
    "\n",
    "2. Dense Optical Flow. 1번 방식보다 slower, corner feature만 사용하는 게 아니라 optical flow의 all point를 사용하기 때문. Colors are used to reflect movement with Hue being directionn and value (brightness / intensity) being speed. \n",
    "\n",
    "(색깔이 indicate direction of object 또는 type of object that are moving in this image. brightness of image가 speed를 의미함)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trail을 만든다.\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# Load video stream\n",
    "cap = cv2.VideoCapture('./MasteringComputerVision-V1.03/Master OpenCV/images/walking.avi')\n",
    "\n",
    "# Set parameters for ShiTomasi corner detection\n",
    "feature_params = dict( maxCorners = 100,\n",
    "                       qualityLevel = 0.3,\n",
    "                       minDistance = 7,\n",
    "                       blockSize = 7 )\n",
    "\n",
    "# Set parameters for lucas kanade optical flow\n",
    "lucas_kanade_params = dict( winSize  = (15,15),\n",
    "                  maxLevel = 2,\n",
    "                  criteria = (cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03))\n",
    "\n",
    "# Create some random colors. trail 생성을 위한 랜덤 색깔\n",
    "# Used to create our trails for object movement in the image \n",
    "color = np.random.randint(0,255,(100,3))\n",
    "\n",
    "# Take first frame and find corners in it\n",
    "ret, prev_frame = cap.read()\n",
    "prev_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Find inital corner locations\n",
    "prev_corners = cv2.goodFeaturesToTrack(prev_gray, mask = None, **feature_params)\n",
    "\n",
    "# Create a mask image for drawing purposes\n",
    "# empty mask임.\n",
    "mask = np.zeros_like(prev_frame)\n",
    "\n",
    "while(1):\n",
    "    ret, frame = cap.read()\n",
    "    frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # calculate optical flow\n",
    "    new_corners, status, errors = cv2.calcOpticalFlowPyrLK(prev_gray, \n",
    "                                                           frame_gray, \n",
    "                                                           prev_corners, \n",
    "                                                           None, \n",
    "                                                           **lucas_kanade_params)\n",
    "\n",
    "    # Select and store good points\n",
    "    good_new = new_corners[status==1]\n",
    "    good_old = prev_corners[status==1]\n",
    "\n",
    "    # Draw the tracks\n",
    "    for i,(new,old) in enumerate(zip(good_new, good_old)):\n",
    "        a, b = new.ravel()\n",
    "        c, d = old.ravel()\n",
    "        mask = cv2.line(mask, (a,b),(c,d), color[i].tolist(), 2)\n",
    "        frame = cv2.circle(frame, (a,b), 5, color[i].tolist(),-1)\n",
    "        \n",
    "    img = cv2.add(frame,mask)\n",
    "\n",
    "    # Show Optical Flow\n",
    "    cv2.imshow('Optical Flow - Lucas-Kanade',img)\n",
    "    if cv2.waitKey(1) == 13: #13 is the Enter Key\n",
    "        break\n",
    "\n",
    "    # Now update the previous frame and previous points\n",
    "    prev_gray = frame_gray.copy()\n",
    "    prev_corners = good_new.reshape(-1,1,2)\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dense Optical Flow\n",
    "Requires OpenCV 3.1\n",
    "\n",
    "색깔이 강해질수록(brighter) 속도가 빠르다는 걸 의미한다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load video stream\n",
    "cap = cv2.VideoCapture(\"./MasteringComputerVision-V1.03/Master OpenCV/images/walking.avi\")\n",
    "\n",
    "# Get first frame\n",
    "ret, first_frame = cap.read()\n",
    "previous_gray = cv2.cvtColor(first_frame, cv2.COLOR_BGR2GRAY)\n",
    "hsv = np.zeros_like(first_frame)\n",
    "hsv[...,1] = 255\n",
    "\n",
    "while True:\n",
    "    \n",
    "    # Read of video file\n",
    "    ret, frame2 = cap.read()\n",
    "    next = cv2.cvtColor(frame2,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Computes the dense optical flow using the Gunnar Farneback’s algorithm\n",
    "    flow = cv2.calcOpticalFlowFarneback(previous_gray, next, \n",
    "                                        None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "\n",
    "    # use flow to calculate the magnitude (speed) and angle of motion\n",
    "    # use these values to calculate the color to reflect speed and angle\n",
    "    magnitude, angle = cv2.cartToPolar(flow[...,0], flow[...,1])\n",
    "    hsv[...,0] = angle * (180 / (np.pi/2))\n",
    "    hsv[...,2] = cv2.normalize(magnitude, None, 0, 255, cv2.NORM_MINMAX)\n",
    "    final = cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)\n",
    "\n",
    "    # Show our demo of Dense Optical Flow\n",
    "    cv2.imshow('Dense Optical Flow', final)\n",
    "    if cv2.waitKey(1) == 13: #13 is the Enter Key\n",
    "        break\n",
    "    \n",
    "    # Store current image as previous image\n",
    "    previous_gray = next\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
