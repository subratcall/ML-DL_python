{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Face, People & Car Detection\n",
    "\n",
    "* Understanding HAAR Cascade Classifiers\n",
    "* Face & Eye detection\n",
    "* Mini project _ car detection & pedestrian (body) detection\n",
    "\n",
    "### HAAR Cascade Classifiers\n",
    "\n",
    "앞의 사례에서 extracting features가 가능하고, 해당 features를 활용해 classify object가 가능하다는 걸 보였다.\n",
    "\n",
    "HAAR Cascade Classifier도 위 원칙을 그대로 따르는데, different way of storing the features. \n",
    "\n",
    "이 features의 이름을 Haar Feature라고 한다. 얘네를 series of classifiers(cascade)에 넣어서 identify objects in an image.\n",
    "\n",
    "Detecting one particular object에 매우 효율적이며, parallel하게 여러 개를 학습시켜서 다른 것들을 detecting하는 것도 가능하다. \n",
    "\n",
    "ex) detecting eyes and face together.\n",
    "\n",
    "#### Haar classifier explained.\n",
    "\n",
    "1. HAAR classifiers are trained using lots of positive images(with the objects present) and negative images(without the object present)\n",
    "    - 예컨대 얼굴 인식이라면 얼굴이 있는 사진은 Positive, 얼굴 없는 사진은 negative.\n",
    "\n",
    "2. 이제 이미지에서 HAAR Features를 뽑아내야 한다. \n",
    "    - HAAR Feature는 간단히 말하면, sum of pixel intensities under rectangular windows. 여기서 windows로 다양한 형태의 rectangular를 사용한다. edge feature, line feature 등등... \n",
    "    - 다만, 이렇게 다양한 window로 feature를 뽑아내면 너무 많은 양의 feature가 만들어진다. 그래서 Integral images라는 방식을 사용하는데, 얘는 computed with 4 array references.\n",
    "    - feature 수를 줄여도, 대다수의 features는 별로 쓸모가 없다. 얼굴 인식이라면, 얼굴을 정확히 가리키는 feature는 극히 일부일 거니까. 그래서 Boosting 기법이 들어왔고, 여기서는 AdaBoost를 사용한다. 간단히 말해 incorrect classification에는 penalty를 부과해서 classifier의 성능을 높이는 것. 이렇게 무의미한 feature를 상당부분 줄여냈다.\n",
    "    - 줄여냈다고 해도, 여전히 no real value인 feature가 많다. 정확히는 information의 양 차이가 나는 feature는 존재할 것. 그래서 사용한 방법으로 \"check whether the region can potentially have a face\". 먼저 이것부터 확인하면, 모든 feature를 일일이 연산해서 대조할 필요는 없어진다.\n",
    "    - 이런 방식이 cascade of classifier이며, Viola Jones method의 경우 38 stage에 걸쳐 진행된다고 함.\n",
    "\n",
    "\n",
    "### Cascade classifier를 사용해 face / eye detection.\n",
    "\n",
    "pretrained된 OpenCV Classifier를 사용할 예정. 깃허브에도 있고, 강의자료에도 제공돼 있다.\n",
    "\n",
    "haarcascade_eye.xml 과 haarcascade_frontalface_default.xml 사용.\n",
    "\n",
    "#### cascade classifier flow 사용법\n",
    "\n",
    "1. load classifier\n",
    "2. pass img to classifier / detector\n",
    "3. get location / ROI(region of interest) for detected objects\n",
    "4. draw rectangle over detected objects.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Face Detection\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# We point OpenCV's CascadeClassifier function to where our \n",
    "# classifier (XML file format) is stored\n",
    "# classifier 생성.\n",
    "face_classifier = cv2.CascadeClassifier('./MasteringComputerVision-V1.03/Master OpenCV/Haarcascades/haarcascade_frontalface_default.xml')\n",
    "\n",
    "# Load our image then convert it to grayscale\n",
    "image = cv2.imread('./MasteringComputerVision-V1.03/Master OpenCV/images/Trump.jpg')\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Our classifier returns the ROI of the detected face as a tuple\n",
    "# It stores the top left coordinate and the bottom right coordiantes\n",
    "\n",
    "# 리턴값은 array. multiple location of face 반환 (얼굴이 여러 개일 경우)\n",
    "faces = face_classifier.detectMultiScale(gray, 1.3, 5)\n",
    "\n",
    "# When no faces detected, face_classifier returns and empty tuple\n",
    "if faces is ():\n",
    "    print(\"No faces found\")\n",
    "\n",
    "# We iterate through our faces array and draw a rectangle\n",
    "# over each face in faces\n",
    "for (x,y,w,h) in faces:\n",
    "    cv2.rectangle(image, (x,y), (x+w,y+h), (127,0,255), 2)\n",
    "    cv2.imshow('Face Detection', image)\n",
    "    cv2.waitKey(0)\n",
    "    \n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Face + Eye Detector\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    " \n",
    "face_classifier = cv2.CascadeClassifier('./MasteringComputerVision-V1.03/Master OpenCV/Haarcascades/haarcascade_frontalface_default.xml')\n",
    "eye_classifier = cv2.CascadeClassifier('./MasteringComputerVision-V1.03/Master OpenCV/Haarcascades/haarcascade_eye.xml')\n",
    " \n",
    "img = cv2.imread('./MasteringComputerVision-V1.03/Master OpenCV/images/Trump.jpg')\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "faces = face_classifier.detectMultiScale(gray, 1.3, 5)\n",
    "\n",
    "# When no faces detected, face_classifier returns and empty tuple\n",
    "if faces is ():\n",
    "    print(\"No Face Found\")\n",
    "\n",
    "for (x,y,w,h) in faces:\n",
    "    cv2.rectangle(img,(x,y),(x+w,y+h),(127,0,255),2)\n",
    "    cv2.imshow('img',img)\n",
    "    cv2.waitKey(0)\n",
    "    roi_gray = gray[y:y+h, x:x+w]\n",
    "    roi_color = img[y:y+h, x:x+w]\n",
    "    eyes = eye_classifier.detectMultiScale(roi_gray)\n",
    "    for (ex,ey,ew,eh) in eyes:\n",
    "        cv2.rectangle(roi_color,(ex,ey),(ex+ew,ey+eh),(255,255,0),2)\n",
    "        cv2.imshow('img',img)\n",
    "        cv2.waitKey(0)\n",
    "    \n",
    "cv2.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Live face / eye detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "face_classifier = cv2.CascadeClassifier('./MasteringComputerVision-V1.03/Master OpenCV/Haarcascades/haarcascade_frontalface_default.xml')\n",
    "eye_classifier = cv2.CascadeClassifier('./MasteringComputerVision-V1.03/Master OpenCV/Haarcascades/haarcascade_eye.xml')\n",
    "\n",
    "def face_detector(img, size=0.5):\n",
    "    # Convert image to grayscale\n",
    "    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_classifier.detectMultiScale(gray, 1.3, 5)\n",
    "    if faces is ():\n",
    "        return img\n",
    "    \n",
    "    for (x,y,w,h) in faces:\n",
    "        x = x - 50\n",
    "        w = w + 50\n",
    "        y = y - 50\n",
    "        h = h + 50\n",
    "        cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "#         roi_gray = gray[y:y+h, x:x+w]\n",
    "#         roi_color = img[y:y+h, x:x+w]\n",
    "        eyes = eye_classifier.detectMultiScale(gray)\n",
    "        \n",
    "        for (ex,ey,ew,eh) in eyes:\n",
    "            cv2.rectangle(img,(ex,ey),(ex+ew,ey+eh),(0,0,255),2) \n",
    "\n",
    "# 좌우반전을 줬는데, cam 원본이 좌우반전이 아니라서 오히려 화면이 어지러움\n",
    "#     img = cv2.flip(img,1)\n",
    "# 안경을 쓰면 제대로 인식을 못한다. 보완할 수 있을까?\n",
    "    return img\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "\n",
    "    ret, frame = cap.read()\n",
    "    cv2.imshow('Our Face Extractor', face_detector(frame))\n",
    "    if cv2.waitKey(1) == 13: #13 is the Enter Key\n",
    "        break\n",
    "        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()      \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "detectMultiScale의 파라미터 (직전 예시의 경우 1.3 과 5) 가 갖는 의미\n",
    "\n",
    "### Tuning Cascade Classifiers\n",
    "\n",
    "*ourClassifier*.**detectMultiScale**(input image, **Scale Factor** , **Min Neighbors**)\n",
    "\n",
    "- **Scale Factor**\n",
    "Specifies how much we reduce the image size each time we scale. E.g. in face detection we typically use 1.3. This means we reduce the image by 30% each time it’s scaled. Smaller values, like 1.05 will take longer to compute, but will increase the rate of detection.\n",
    "\n",
    "단순히 생각하면, 숫자가 작을수록 more precise한 경향을 보인다. face bounding box도 얼굴에 훨씬 근접해지고, eye box도 더 많이 나타난다. 실제 detection한 bounding box의 30% 정도 크기를 더 키운다고 보면 될 듯. 값을 1로 주면 효과가 없다.\n",
    "\n",
    "\n",
    "\n",
    "- **Min Neighbors**\n",
    "Specifies the number of neighbors each potential window should have in order to consider it a positive detection. \n",
    "\n",
    "다시말해, potential window가 window로 인정받기 위해서 각 window간 distance의 최소치. same region에 bounding box가 최소 몇 개는 그려져야 해당 region을 인식할 것인지. 따라서 low value = detect multiple pieces of a single face. high value = reduce the sensitivity of detection units / algorithm.\n",
    "\n",
    "Typically set between 3-6. \n",
    "It acts as sensitivity setting, low values will sometimes detect multiples faces over a single face. High values will ensure less false positives, but you may miss some faces.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Car & Pedistrian Detection\n",
    "\n",
    "### Pedistrian Detection\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Create our body classifier\n",
    "body_classifier = cv2.CascadeClassifier('./MasteringComputerVision-V1.03/Master OpenCV/Haarcascades/haarcascade_fullbody.xml')\n",
    "\n",
    "# Initiate video capture for video file\n",
    "cap = cv2.VideoCapture('./MasteringComputerVision-V1.03/Master OpenCV/images/walking.avi')\n",
    "\n",
    "# Loop once video is successfully loaded\n",
    "while cap.isOpened():\n",
    "    \n",
    "    # Read first frame\n",
    "    ret, frame = cap.read()\n",
    "    # resize를 하는 이유는 speed up classification. resolution을 절반으로 줄이고 interpolation 방법으로 Inter linear 사용.\n",
    "    frame = cv2.resize(frame, None,fx=0.5, fy=0.5, interpolation = cv2.INTER_LINEAR)\n",
    "\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    # Pass frame to our body classifier\n",
    "    bodies = body_classifier.detectMultiScale(gray, 1.2, 3)\n",
    "    \n",
    "    # Extract bounding boxes for any bodies identified\n",
    "    for (x,y,w,h) in bodies:\n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 255), 2)\n",
    "        cv2.imshow('Pedestrians', frame)\n",
    "\n",
    "    if cv2.waitKey(1) == 13: #13 is the Enter Key\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "# Create our body classifier\n",
    "car_classifier = cv2.CascadeClassifier('./MasteringComputerVision-V1.03/Master OpenCV/Haarcascades/haarcascade_car.xml')\n",
    "\n",
    "# Initiate video capture for video file\n",
    "cap = cv2.VideoCapture('./MasteringComputerVision-V1.03/Master OpenCV/images/cars.avi')\n",
    "\n",
    "\n",
    "# Loop once video is successfully loaded\n",
    "while cap.isOpened():\n",
    "    \n",
    "    time.sleep(.05)\n",
    "    # Read first frame\n",
    "    ret, frame = cap.read()\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "   \n",
    "    # Pass frame to our car classifier\n",
    "    cars = car_classifier.detectMultiScale(gray, 1.3, 2)\n",
    "    \n",
    "    # Extract bounding boxes for any bodies identified\n",
    "    for (x,y,w,h) in cars:\n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 255), 2)\n",
    "        cv2.imshow('Cars', frame)\n",
    "\n",
    "    if cv2.waitKey(1) == 13: #13 is the Enter Key\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
