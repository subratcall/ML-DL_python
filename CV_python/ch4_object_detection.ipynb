{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Object detection.\n",
    "\n",
    "CV에서 가장 유용한 경우. Labeling Scenes에서부터 다방면에 걸쳐 쓰인다.\n",
    "\n",
    "recognition은 second level of Object detection. multiple object within img 확인. \n",
    "\n",
    "Template Matching: object detection의 기본. 비유하자면 사막에서 바늘 찾기."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding Waldo (Quickly find a specific pattern within image)\n",
    "\n",
    "input image를 받은 다음 grayscale로 변환. template에 해당하는 작은 이미지도 받은 다음 gray 변환.\n",
    "\n",
    "matchtemplate 함수를 이용함. 아래 예시의 경우 match method는 cv2.TM_CCOEFF를 사용. correlation coefficient를 말함\n",
    "\n",
    "* 링크에 올라간 함수들을 보면, 물론 특정 함수가 다른 함수보다 잘 match하는 경우는 있어도 전체적으로는 유사한 결과를 내는 편\n",
    "\n",
    "return값은 array이고, 이 값의 bounding box를 반환하는 함수가 cv2.minMaxLoc 함수.\n",
    "\n",
    "bounding box를 찾은 뒤, 원본그림에 box를 실제로 그린 것.\n",
    "\n",
    "### Notes on Template Matching\n",
    "\n",
    "There are a variety of methods to perform template matching, but in this case we are using the correlation coefficient which is specified by the flag **cv2.TM_CCOEFF.**\n",
    "\n",
    "So what exactly is the cv2.matchTemplate function doing?\n",
    "Essentially, this function takes a “sliding window” of our waldo query image and slides it across our puzzle image from left to right and top to bottom, one pixel at a time. Then, for each of these locations, we compute the correlation coefficient to determine how “good” or “bad” the match is. \n",
    "\n",
    "Regions with sufficiently high correlation can be considered “matches” for our waldo template.\n",
    "From there, all we need is a call to cv2.minMaxLoc on Line 22 to find where our “good” matches are.\n",
    "That’s really all there is to template matching!\n",
    "\n",
    "http://docs.opencv.org/2.4/modules/imgproc/doc/object_detection.html      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load input image and convert to grayscale\n",
    "image = cv2.imread('./MasteringComputerVision-V1.03/Master OpenCV/images/WaldoBeach.jpg')\n",
    "cv2.imshow('Where is Waldo?', image)\n",
    "cv2.waitKey(0)\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Load Template image\n",
    "template = cv2.imread('./MasteringComputerVision-V1.03/Master OpenCV/images/waldo.jpg',0)\n",
    "\n",
    "result = cv2.matchTemplate(gray, template, cv2.TM_CCOEFF)\n",
    "min_val, max_val, min_loc, max_loc = cv2.minMaxLoc(result)\n",
    "\n",
    "#Create Bounding Box\n",
    "top_left = max_loc\n",
    "bottom_right = (top_left[0] + 50, top_left[1] + 50)\n",
    "cv2.rectangle(image, top_left, bottom_right, (0,0,255), 5)\n",
    "\n",
    "cv2.imshow('Where is Waldo?', image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그런데, 원본 이미지를 잘라낸 특정 이미지를 사용하는 경우 몇 가지 limitation이 있다.\n",
    "\n",
    "* scale, rotation, another angle(perspective), brightness / contrast / hue 등에 전부 무력함. distortion에도 취약\n",
    "\n",
    "이런 문제를 해결하려는 노력으로 \"image Features\"가 있다.\n",
    "\n",
    "image features = 'summaries of pictures'. \n",
    "\n",
    "\"interesting areas that are somewhat unique to the specific image.\" \n",
    "\n",
    "보통 key point features / interest points라고도 불린다.\n",
    "\n",
    "#### 중요한 이유??\n",
    "\n",
    "analyse, describe, match image에서 상당히 중요한 기능을 하기 때문. \n",
    "\n",
    "image alignment (파노라마 switching = 이미지 매칭하기 등) / 3D reconstruction, robot navigation, object recognition, motion tracking 등등 다양한 분야에 응용된다.\n",
    "\n",
    "**\"interesting\"의 정의??**\n",
    "\n",
    "= distinct, unique한 정보를 담고 있는 부분이라고 이해하면 쉽다. High change of intensities, Corners of edges 등등.\n",
    "\n",
    "주의할 점: 노이즈도 appear 'informative' when it's not. 그래서 cleaning 작업이 필요.\n",
    "\n",
    "\n",
    "#### interesting features의 특징\n",
    "\n",
    "* repeatable : 사진 내 여러 곳에서 비슷하게 발견됨\n",
    "* distinctive : 각각의 feature는 unique, different to other features of same scene\n",
    "* compactness / efficiency : 해당 이미지의 특징을 잡아낼 수 있는 최소한의 feature를 말함. \n",
    "* locality : features occupies a small area of image, and is robust to clutter & occlusion.\n",
    "\n",
    "\n",
    "\n",
    "1. Corners as Features.\n",
    "\n",
    "window shift시 intensity 차이가 크다. \n",
    " - edge : change in one direction (상하 / 좌우)\n",
    " - corner : change in all direction (상하좌우 전부 다)\n",
    "\n",
    "\n",
    "### Harris Corner Detection\n",
    "\n",
    "#### Harris Corner Detection is an algorithm developed in 1998 for corner detection  (http://www.bmva.org/bmvc/1988/avc-88-023.pdf) and works fairly well.\n",
    "\n",
    "**cv2.cornerHarris**(input image, block size, ksize, k)\n",
    "- Input image - should be grayscale and float32 type.\n",
    "- blockSize - the size of neighborhood considered for corner detection\n",
    "- ksize - aperture parameter of Sobel derivative used.\n",
    "- k - harris detector free parameter in the equation\n",
    "- **Output** – array of corner locations (x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load image then grayscale\n",
    "image = cv2.imread('./MasteringComputerVision-V1.03/Master OpenCV/images/chess.jpg')\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# The cornerHarris function requires the array datatype to be float32\n",
    "gray = np.float32(gray)\n",
    "\n",
    "harris_corners = cv2.cornerHarris(gray, 3, 3, 0.05)\n",
    "\n",
    "#We use dilation of the corner points to enlarge them\\\n",
    "kernel = np.ones((7,7),np.uint8)\n",
    "harris_corners = cv2.dilate(harris_corners, kernel, iterations = 2)\n",
    "\n",
    "# Threshold for an optimal value, it may vary depending on the image.\n",
    "image[harris_corners > 0.025 * harris_corners.max() ] = [255, 127, 127]\n",
    "\n",
    "cv2.imshow('Harris Corners', image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improved Corner Detection using - Good Features to Track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "img = cv2.imread('./MasteringComputerVision-V1.03/Master OpenCV/images/chess.jpg')\n",
    "gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# We specific the top 50 corners\n",
    "corners = cv2.goodFeaturesToTrack(gray, 100, 0.01, 15)\n",
    "# 마지막 값은 설명하기 복잡하지만 'corner 사이의 최소 거리' 같은 개념으로 이해하면 쉽다.\n",
    "for corner in corners:\n",
    "    x, y = corner[0]\n",
    "    x = int(x)\n",
    "    y = int(y)\n",
    "    cv2.rectangle(img,(x-10,y-10),(x+10,y+10),(0,255,0), 2)\n",
    "    \n",
    "cv2.imshow(\"Corners Found\", img)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**cv2.goodFeaturesToTrack**(input image, maxCorners, qualityLevel, minDistance)\n",
    "\n",
    "- Input Image - 8-bit or floating-point 32-bit, single-channel image.\n",
    "- maxCorners – Maximum number of corners to return. If there are more corners than are found, the strongest of them is returned.\n",
    "- qualityLevel – Parameter characterizing the minimal accepted quality of image corners. The parameter value is multiplied by the best corner quality measure (smallest eigenvalue). The corners with the quality measure less than the product are rejected. For example, if the best corner has the quality measure = 1500, and the  qualityLevel=0.01 , then all the corners with the quality - - measure less than 15 are rejected.\n",
    "- minDistance – Minimum possible Euclidean distance between the returned corners.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 여전히 몇 가지 문제점을 안고 있음\n",
    "\n",
    "장점: tolerent of \"rotation, translation (image shift), slight brightness / affine intensity change에 강하다\n",
    "\n",
    "단점: large change in intensity or photometric change(brightness, contrast, affine intensity), scaling (enlargen, shrinking)에 약함\n",
    "\n",
    "이미지 크기가 작아지면 corner를 제대로 찾지 못하고, 이미지 크기가 커지면 corner 수가 지나치게 많아진다. (detect edges로 선회하는 문제점)\n",
    "\n",
    "\n",
    "## 문제점을 해결하는 몇 가지 알고리즘들\n",
    "\n",
    "**SIFT** (scale invariant feature transform)\n",
    "\n",
    "scaling tolerent한 알고리즘이며, CV에서 보편적으로 쓰이는 알고리즘이자 scale invariance issue에서 디폴트로 사용되는 알고리즘. 단, no longer freely available with openCV 3.0+. (patent issue) academic / research purpose로 사용될 경우는 무료이지만 openCV 2.143을 써야 한다고 함.\n",
    "\n",
    "\n",
    "간단설명.\n",
    "\n",
    "1. interesting key point를 Difference of Gaussian method라는 방법으로 찾아낸다. 이 방법은 variation이 특정 threshold를 초과하는 경우를 탐지하며, edge descriptor보다 성능이 좋다고 함\n",
    "2. create vector descriptor for these interesting areas.\n",
    "    - 이 과정에서 several different scale로 scan을 진행한다. 이걸 하는 이유는 specific stability criteria를 찾아내기 위해서고, 이렇게 찾아낸 criteria를 vector descriptor로 인코딩함. 따라서, 크기에 상관없이 가장 stable scale을 사용하기에 scale invariant 특성을 갖게 됨\n",
    "    \n",
    "3. rotation invariance는 Orientation Assignment of key point를 찾아서 해결. image gradient magnitudes라는 걸 사용한다고 하며, 2D direction만 알게 되면 normalize direction이 가능하다는 논리라고 하는데 뭔소린지 잘 모르겠음\n",
    "\n",
    "\n",
    "\n",
    "**SURF**\n",
    "\n",
    "speed version of SIFT. Difference of Gaussian의 연산량이 많은 편이라 시간이 오래 걸림. 이 알고리즘은 Hessian Matrix를 사용해 시간단축에 성공했다고 함. 또한 sum of Haar wavelet response for orientation assignment.\n",
    "\n",
    "\n",
    "SIFT, SURF는 patented이기에 돈이 든다. 그래서 사용할 수 있는 대안들이 몇 있음\n",
    "\n",
    "**Features from Accelerated Segment Test (FAST) **\n",
    "\n",
    "key point detection only.(No descriptor. descriptor 계산하려면 유료 알고리즘 필요) Real time application에서 사용한다고 함\n",
    "\n",
    "**Binary Robust Independent Elementary Features (BRIEF)**\n",
    "\n",
    "Computers descriptors quickly (유료 알고리즘보다 빠름).\n",
    "\n",
    "**Oriented FAST and Rotated BRIEF(ORB)**\n",
    "\n",
    "FAST와 BRIEF를 합친 알고리즘이며 openCV에서 무료로 제공한다.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "프로세스 절차를 요약하자면\n",
    "1. Create Detector\n",
    "2. Input image into detector\n",
    "3. obtain key points\n",
    "4. Draw key points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'cv2' has no attribute 'SIFT'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-c3546aafe398>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#Create SIFT Feature Detector object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0msift\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSIFT\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m#Detect key points\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'cv2' has no attribute 'SIFT'"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "image = cv2.imread('./MasteringComputerVision-V1.03/Master OpenCV/images/input.jpg')\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "#Create SIFT Feature Detector object\n",
    "sift = cv2.SIFT()\n",
    "\n",
    "#Detect key points\n",
    "keypoints = sift.detect(gray, None)\n",
    "print(\"Number of keypoints Detected: \", len(keypoints))\n",
    "\n",
    "# Draw rich key points on input image\n",
    "image = cv2.drawKeypoints(image, keypoints, flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "\n",
    "cv2.imshow('Feature Method - SIFT', image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'cv2' has no attribute 'SURF_create'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-eebfbc41bdc1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#Create SURF Feature Detector object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0msurf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSURF_create\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Only features, whose hessian is larger than hessianThreshold are retained by the detector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'cv2' has no attribute 'SURF_create'"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "image = cv2.imread('./MasteringComputerVision-V1.03/Master OpenCV/images/input.jpg')\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "#Create SURF Feature Detector object\n",
    "surf = cv2.SURF()\n",
    "\n",
    "# Only features, whose hessian is larger than hessianThreshold are retained by the detector\n",
    "surf.hessianThreshold = 500\n",
    "keypoints, descriptors = surf.detectAndCompute(gray, None)\n",
    "print(\"Number of keypoints Detected: \", len(keypoints))\n",
    "\n",
    "# Draw rich key points on input image\n",
    "image = cv2.drawKeypoints(image, keypoints, flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "\n",
    "cv2.imshow('Feature Method - SURF', image)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of keypoints Detected:  8960\n"
     ]
    }
   ],
   "source": [
    "# FAST\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "image = cv2.imread('./MasteringComputerVision-V1.03/Master OpenCV/images/input.jpg')\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Create FAST Detector object\n",
    "fast = cv2.FastFeatureDetector_create()\n",
    "\n",
    "# Obtain Key points, by default non max suppression is On\n",
    "# to turn off set fast.setBool('nonmaxSuppression', False)\n",
    "keypoints = fast.detect(gray, None)\n",
    "print(\"Number of keypoints Detected: \", len(keypoints))\n",
    "\n",
    "# Draw rich keypoints on input image\n",
    "image = cv2.drawKeypoints(image, keypoints, None, flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "\n",
    "cv2.imshow('Feature Method - FAST', image)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of keypoints Detected:  8735\n"
     ]
    }
   ],
   "source": [
    "# BRIEF\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "image = cv2.imread('./MasteringComputerVision-V1.03/Master OpenCV/images/input.jpg')\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Create FAST detector object\n",
    "fast = cv2.FastFeatureDetector_create()\n",
    "\n",
    "# Create BRIEF extractor object\n",
    "# brief = cv2.DescriptorExtractor_create(\"BRIEF\")\n",
    "brief = cv2.xfeatures2d.BriefDescriptorExtractor_create()\n",
    "# Determine key points\n",
    "keypoints = fast.detect(gray, None)\n",
    "\n",
    "# Obtain descriptors and new final keypoints using BRIEF\n",
    "# 여기서 descriptor를 만들 수 있다. brief.compute 명령어로.\n",
    "keypoints, descriptors = brief.compute(gray, keypoints)\n",
    "print(\"Number of keypoints Detected: \", len(keypoints))\n",
    "\n",
    "# Draw rich keypoints on input image\n",
    "image = cv2.drawKeypoints(image, keypoints, None, flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "                                    \n",
    "cv2.imshow('Feature Method - BRIEF', image)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of keypoints Detected:  4993\n"
     ]
    }
   ],
   "source": [
    "# ORB\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "image = cv2.imread('./MasteringComputerVision-V1.03/Master OpenCV/images/input.jpg')\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Create ORB object, we can specify the number of key points we desire\n",
    "# key point의 최대값을 객체 생성 시 부여할 수 있다. 부여하지 않을 경우 디폴트는 500으로 설정. 500개의 best key point를 찾는 것.\n",
    "orb = cv2.ORB_create(5000)\n",
    "\n",
    "# Determine key points\n",
    "keypoints = orb.detect(gray, None)\n",
    "\n",
    "# Obtain the descriptors\n",
    "keypoints, descriptors = orb.compute(gray, keypoints)\n",
    "print(\"Number of keypoints Detected: \", len(keypoints))\n",
    "\n",
    "# Draw rich keypoints on input image\n",
    "image = cv2.drawKeypoints(image, keypoints, None, flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "\n",
    "cv2.imshow('Feature Method - ORB', image)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
